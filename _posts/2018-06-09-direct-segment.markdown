---
layout: paper-summary
title:  "Efficient virtual memory for big memory servers"
date:   2018-06-09 00:34:00 -0500
categories: paper
paper_title: "Efficient virtual memory for big memory servers"
paper_link: https://dl.acm.org/citation.cfm?doid=2485922.2485943
paper_keyword: Direct Segment; Segmentation
paper_year: ISCA 2013
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---   

This paper proposes direct segment, a segmentation based approach to virtual address
translation. The motivation of direct segment is high overhead of Translation Lookaside Buffer (TLB)
lookup on modern big-data machines. On one hand, big-data applications do not require complicated memory 
mapping. On the other hand, existing paging systems manage memory mapping for each 4 KB page separately, 
relying on a TLB to accelerate translation in most cases. The classical paging-based address mapping in this 
regard is inefficient by having both page walk overhead and redundency of memory protection bits. Based on these 
observations, direct segment is designed to eliminate paging overhead with simple hardware changes. We 
elaborate the design in the next few sections.

Long running big data applications, such as memcached or MySQL, demonstrate several memory usage patterns that 
distinguish them from short running interactive programs. First, they normally do not rely on the OS to swap in 
and swap out physical pages to transparently overcommit. Instead, they treat the main memory as a buffer pool/object 
pool, and automatically adjusts its memory usage to the size of the physical memory available in the system. 
Swapping can do very little good here, because big-data applications are not willing to suffer from the extra I/O
overhead bound to swapping. Second, big-data applications typically allocate its workspace memory at startup, and 
then perform memory management by its own. Fine-grained memory management at page granularity is of little use
in this scenario, as external fragmentation is not observable by the OS. Third, the workspace memory of big-data
applications are almost always of readable and writable permission. Per-page protections bits are useful in cases such as 
protecting code segment from being maliciously altered, but there is no way of selectively turning them off for 
the workspace memory area. Overall, we conclude that the current hardware page-level fine grained mapping and protection 
machanism is sufficient, but not in its best shape to deliver high performance for big-data applications.

The paper also argues that the overhead of page level mapping and protection constitutes a non-negligible amount 
of cycles while running big-data applications. The evidence is presented by running several benchmarks, including 
graph500, GUPS, and NBP. The benchmarking result shows that for regular 4 KB pages, D-TLB misses can take as much as 
51% of execution time. Even with 2 MB and 1 GB super pages, D-TLB can still cause slow down, raning from zero to 10%
of execution time. 

A segmentation approach can help relieve the slow down caused by page level mapping for big-data applications.
In the direct segment design, three registers are added to the context of a process: BASE, LIMIT and OFFSET.
The BASE register holds the start address of segmentation in virtual address space, while LIMIT holds the 
address of the end of the segment. OFFSET stores the different between the physical and virtual segments. During 
an address translation, the MMU compares the virtual address with BASE and LIMIT. If the virtual address falls 
in-between, then segmentation is used, and TLB lookup is aborted. The physical address is generated by adding  
OFFSET onto the virtual address, which can be done with only few cycles. Page table and TLB is entirely avoided 
in the translation process.