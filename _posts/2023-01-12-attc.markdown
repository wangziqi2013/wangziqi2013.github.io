---
layout: paper-summary
title:  "ATTC (@C): Addressable-TLB based Translation Coherence"
date:   2023-01-12 05:00:00 -0500
categories: paper
paper_title: "ATTC (@C): Addressable-TLB based Translation Coherence"
paper_link: https://dl.acm.org/doi/10.1145/3410463.3414653
paper_keyword: ATTC; TLB Shootdown; Virtualization
paper_year: PACT 2020
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Highlight:**

1. TLB shootdown is a more complicated problem to address, because (1) the 2D nested page table walk design has more 
shootdown scenarios, e.g., both the host and the guest can update the mapping; (2) existing software solutions 
do not support fine-grained, precise tracking of the paging history; and (3) in hybrid NVM-DRAM systems, frequent
page migration between the two may aggravate the situation as every page migration operation requires a TLB shootdown.

2. We can address the TLB coherence problem on hardware by adding a level-3 TLB in the main memory. TLB coherence 
messages can then be sent as regular cache coherence messages using the memory address containing the entry's 
address. In other words, the in-memory TLB structure becomes the point of coherence, where every update to the 
translation entry (i.e., the page table) is also reflected on the in-memory TLB as a memory update. The 
memory update is then captured by regular cache coherence and sent to the sharer core that has the same 
translation entry, which triggers TLB invalidation.

**Comments:**

1. Modern TLB entries have two disambiguation fields. One is VMID which distinguishes between VMs and the 
VMM, which the paper discusses. The other, however, is PCID which distinguishes between different processes 
within the same VM instance or within the VMM/host OS, which the paper neglects. How does the PCID field
affect the design of ATTC?

2. The paper did not discuss how are the tables (ATLB and INVTBL) populated. Are they populated automatically by 
the MMU page walker on the completion of page walks? Are they populated by the guest or host OS (unlikely)?
It seems that the paper adds the ATLB as a level 3 in-memory TLB. If this assumption is true, then the ATLB
should be populated by the page walker.

3. If the cache block containing the ATLB entry is evicted from the private hierarchy (or from the LLC, depending
on whether the coherence protocol has silent L2 eviction), then cache coherence will no longer be able to reach the 
private hierarchy potentially holding the TLB entry. How did ATTC deal with this scenario? 
One possible solution is to mark cache blocks containing ATLB entries in the L2 cache and then invalidate 
TLN entries as well when a marked block is evicted. However, this approach would make the L2 essentially
inclusive of the TLB, which may incur unexpected performance issues. Besides, it introduces extra tagging per 
cache block.

4. If an entry is evicted from the ATLB as part of the MMU's page walk process, does the MMU also evict 
all locally cached TLB entries for correctness? The paper should have added a discussion of this scenario.

5. It seems that the design assumes that the ATLB index is generated the same way as the local TLB hierarchy.
Otherwise, it is impossible to use ATLB's entry index to perform invalidation on the local TLB.

This paper proposes ATTC, a hardware TLB coherence scheme that eliminates the overhead of software TLB shootdown
in a virtualized environment. ATTC is built upon cache coherence and it reuses the existing cache coherence network
to perform TLB coherence on hardware. Compared with prior schemes that also rely on cache coherence, ATTC handles 
virtualized environments better and incurs only minimum hardware additions on the TLB structure.

The paper is motivated by the inefficiency of existing software solutions to maintain TLB coherence. Conventionally, 
when the OS kernel updates a page table entry that mandates a TLB coherence action on a processor core, the kernel must 
explicitly notify the other cores that can potentially cache the same translation entry in their local TLB, and wait
for their response to complete the TLB shootdown process. This process is implemented in pure software using IPIs and 
is blocking, which incurs non-negligible overhead. The problem is further exacerbated with the introduction of 
nested page tables as part of the virtualization support. In particular, with the nested page table, the TLB shootdown 
mechanism must deal with two radically different scenarios. The first scenario is when the guest OS updates the 
guest page table. In this case, the guest OS tracks the virtual cores on which the translation entry can be potentially
cached, and issues IPIs to these cores. 
In the second scenario, the host OS (or hypervisor) updates the translation from guest physical addresses (gPA) to 
host physical addresses (hPA). In this case, the set of cores that can potentially cache the translation is 
not clear to the host OS, as the host OS only tracks the set of virtual cores allocated to a particular VM, but not 
the cores that each individual process running in the VM instance is scheduled on. Besides, the host OS cannot 
decide the gVA that corresponds 
to the gPA being updated, as it requires a reverse mapping of the inner (guest OS) page table. As a result, the 
host OS is unable to determine the virtual address to be invalidated (which must be the gVA because the TLB is designed
to be indexed by gVA). In the current OS implementation, this issue is addressed by flushing the entire TLB on all 
virtual cores that have been allocated to the VM instance. 

The paper also pointed out that while prior works seem to perfectly reduce the overhead of software TLB shootdowns, 
these proposed mechanisms have been largely rendered less effective by nested page table, since prior works mostly 
assume a single-level translation between VA and PA and are generally difficult to be extended to support 2D 
page tables.
Furthermore, prior works tend to extend TLB entries with more metadata bits, which results in higher hardware cost
and more design complications. 

Lastly, as byte-addressable NVM is gradually adopted as a slower but bigger replacement to DRAM to serve as the main 
memory, the paper observes that on future systems, pages need to be constantly migrated between NVM and DRAM.
Page migration would incur more frequent TLB shootdowns, which makes an efficient solution incredibly valuable.

Based on the above observations, the paper proposes ATTC which we describe as follows. In ATTC, a level-3 TLB,
called the ATLB, is added to the main memory. The ATLB is organized as a set-associative main memory cache, but 
it stores translation entries from gVA to hPA as a result of 2D page table walks. Although the paper did not specify
the optimal parameters for ATLB, it is suggested that ATLB should be at least a few MBs in size, and have more 
sets than the L2 TLB. Each entry of the ATLB stores the gVA to hPA mapping as a regular hardware TLB.
When TLB lookups miss the local hierarchy, the MMU will further perform lookups in the ATLB. The ATLB is indexed 
using both the VMID and the gVA, such that translation entries of different VM instances can co-exist peacefully 
in the ATLB.
