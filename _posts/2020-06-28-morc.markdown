---
layout: paper-summary
title:  "A Many-Core Oriented Compressed Cache"
date:   2020-06-28 04:46:00 -0500
categories: paper
paper_title: "A Many-Core Oriented Compressed Cache"
paper_link: https://dl.acm.org/doi/10.1145/2830772.2830828
paper_keyword: Cache; Compression; Log-structured Cache
paper_year: MICRO 2015
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Lowlight:**

1. If two addresses are mapped to the same log, and uses the same LMT entry, how do you know which address the status
   bits represent? A conservative approach is to always check tags as long as the LMT entry is valid, but in this case,
   storing MESI states is an overkill, as a single valid bit suffices.

2. The paper does not mention how conventional per-line status bits are stored. Are they compressed with tags using BDI
   (which I suspect is, but BDI will be less effective since there are not only address tags but also status bits).

This paper proposes MORC, a novel log-structured last-level cache (LLC) design optimized for high throughput but longer
access latency. The paper identifies that on modern multicore architectures, the performance of a single core is less
important than the overall system throughput. As in previous works, MORC takes advantage of cache compression to increase
overall system throughput by providing a larger effective cache, allowing more accesses to be fulfilled by the LLC,
rather than the more power-hungey and lower latency DRAM.
The paper then proceeds to identify three critical problems that are common with existing cache compression schemes.
First, most existing algorithms have very limited compression ratio, ranging between 2:1 and 8:1. The compression ratio
depends not only on the effectiveness of the algorithm itself, but also on the over-priovisioned tags. In practice, it is 
difficult to achieve maximum compression ratio due to the size of the block and/or the number of over-privisioned tags.
Second, most compression schemes adopt the segmented cache approach, in which data slots within a set are unified into 
a whole chunk, which is then divided into fixed size segments. Each segment can only be mapped by one tag and store 
part of all of its compressed data. If the size of the compressed block is not a multiple of segment size, the last segment
cannot be fully utilized, due to internal fragmentation. 
Lastly, to reduce circult complexity, most compressed cache designs only allocate segments using minimum algorithm, such
as random or FIFO. In addition, there are often other constraints on the placement of segments, e.g. all segments mapped
by a tag must store compressed data in ascending order. These constriants will prevent a free segment from being used,
called external fragmentation, which can only be resolved by compaction. The paper claims that cache compaction operation 
incurs major overhead for circuit area and power consumption, which is not advised.
Both internal and external fragmentation reduce effective cache size, but can be allievated by reducing the granularity 
of tracking at the cost of more on-chip metadata.

MORC avoids all above three problems using log-structured approach to organize compressed cache lines. 
First, by compressing multiple cache lines together (512KB as suggested in the paper), dictionary information can be 
better utilized to encode duplicated entries. An average of 24x and a maximum of 64x compression ratio are reported by 
the paper. Second, a compressed log is stored compactly, with blocks within the log aligned to byte boundaries, reducing
internal fragmentation to a minimum.
Lastly, no entries in the log will be overwritten by a line fill response or write back request. Entries of individual
blocks are always appended to the end of the log, eliminating the need of compaction. 
Furthermore, MORC allocates tag storage at the end of the log. Tags are also compressed to reduce the number of physical
over-provisioning storage for storing tags of compressed blocks.

We next describe the overall organization as follows. Compressed blocks are stored as logs, which are allocated to distinct
physical slots. The paper suggests that 512 byte logs be used. All blocks in a log are compressed together, and blocks 
can only be inserted into the log by first decompressing the log, append the block data, and then recompress it. 
The data array is implemented as an indexable array of logs.
Each log also has an associated tag array, which is stored in another physical bank. Tags in each individual tag slot
are compressed to leverage locality of tag addresses. Tags are stored in the same order as log data. 
Depending on the compression algorithm, appending a tag entry to the end of the tag store may or may not require 
decompression (in the latter case, some extra state data may be required to be associated with the tag array).
Status bits and coherence bits are also compressed with tags, although not mentioned by the paper.
To help locate the log object given a requested address, an extra Log Mapping Table (LMT) is used to map requested 
addresses to log objects. The LMT is implemented as a coventional direct-mapped tag array, except that the address
tag is not stored with the LMT entry. Address tags are verified after the compressed tag of the log object is located, 
decompressed, and checked in parallel. Due to the absence of address tags in LMT entries, it is possible that the LMT 
entry points to a log object, but tag check fails, which we call an "aliased miss". 
Two extra designs help optimize the lookup access. First, to allow fast detection of non-aliased misses, in which case
decompression of tag arrays are not needed, each LMT entry also stores the status bits of the cache line it maps. The 
LMT lookup logic could then determine the status of the requested cache line if it is not mapped at all and 
the LMT entry is not in-use by other addresses.
Second, the LMT entry lookup is performed as a column-associative cache with an effective associativity of two. Two different
hash functions are used to map an address to two possible locations. The LMT lookup logic should check both for possible 
hits, which potentially doubles cache lookup latency.