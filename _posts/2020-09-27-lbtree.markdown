---
layout: paper-summary
title:  "LB+-Trees: Optimizing Persistent Index Performance on 3DXPoint Memory"
date:   2020-09-27 03:50:00 -0500
categories: paper
paper_title: "LB+-Trees: Optimizing Persistent Index Performance on 3DXPoint Memory"
paper_link: https://dl.acm.org/doi/10.14778/3384345.3384355
paper_keyword: NVM; B+Tree; LB-Tree
paper_year: VLDB 2020
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper presents LB-Tree, or Line Write/Logless B+Tree, a NVM optimized B+Tree design for efficient updates.
Although not explicitly stated, the paper identifies a few inefficiencies in conventional in-memory B+Tree designs
when ported to the NVM, such as ordered nodes, the overhead of structural modifications, and maintaining consistent
node states.

The paper makes three critical observations that enable extra optimizations on NVM. First, it is observed that the 
performance of NVM writes of a cache line does not depend on the number of dirty bits in the cache line. Theoretically
speaking, NVM may adopt optimizations to minimize the number of flipping bits in each internal writes, to reduce the 
number of physical state changes. This optimization is not present in the current commercial product, most likely because
the NVM device encrypts data stored in the physical array, in which case a single bit change in the input line will
propagate to the remaining bits, offsetting the benefits of the optimization. This observation suggests that B+Tree
designs may perform extra writes in addition to the requested one if these extra writes lower the overhead of the following
updates. 
The second observation is that NVM internal units of reads and writes are 256 bytes, meaning that all reads and writes
are performed internally with 256-byte persistent buffers despite the 64-byte memory interface. 
As a result, reads to aligned 256-byte data can achieve higher bandwidth, since the internal buffer can sustain higher
bandwidth than accessing the physical array. Similarly, writes to aligned 256-byte data are also faster.
The last observation is that the performance of NVM device also depends on storage utilization. The performance of a
new device drops until the working set size grows exceed one eighth of the device capacity. Although this property
does not affect the design directly, it indicates that testing should use at least one eighth of device storage in order
to be cover all cases.

Overall, LB-Tree leverages the following techniques to make writes efficient. First, elements in leaf nodes are not sorted,
since maintaining the ordering property of leaf nodes will involve shifting existing elements in the node, causing massive,
multi-line updates. Instead, elements can be stored anywhere in the leaf node, even leaving gaps between two valid 
elements. A bitmap in the node header tracks which slots are occupied and which are empty. Insert operations, therefore,
can just select an empty slot using the bitmap, and fill key and value into the slot.
Search operations on leaf nodes, as a result, must scan the node linearly, skipping invalid slots using the bitmap.

The second technique is that inner nodes are maintained in the DRAM, and rebuilt at recovery time. This avoids complicated
race conditions and corner cases when updating inner nodes on leaf splits, since inner nodes will be wiped out on crashes.
The tree itself, however, can always be rebuilt, since B+Tree leaf nodes always maintain full knowledge of the state
of the tree. As long as leaf nodes are in a consistent state, inner nodes can be rebuilt in a reasonable amount of time.
It does not matter whether inner nodes are sorted or not, since they do not have atomicity requirements.

The third technique is read-only optimization using HTM support for thread synchronization. Reader threads do not acquire
locks, and always proceed optimistically, assuming a consistent node image. Writers, however, acquire per-node lock as 
it traverses down the B+Tree by setting a lock bit in the node header. Readers will be notified when read-write conflicts
occur via HTM, as we will see below. This allows fast and synchronization-free reads, compensating for the loss of efficiency
due to unordered leaf nodes.

The fourth technique is element moving for less persistent barriers. This technique leverages the observation that if 
the element to be updated is in the same cache line as the header, which will always be updated on insert operations,
the write operation that fills in the element does not need to be flushed and persisted before the header does, since
it is guaranteed that a 64-byte line will be persisted atomically. In this case, the write operation simply fills in the 
element slot, updates the bitmap, and then executes a persistence barrier, reducing the number of barriers to one,
instead of two. In addition, write operations to the remaining part of the node may also proactively move elements in the
first cache line of the same node to the cache line it updates. According to the observation that the number of dirty bits
does not affect NVM write performance, this essentially optimizes future write operations on the same node without incurring
any NVM write overhead.

The fifth technique is logless split operations. LB-Tree optimizes node split with shadowed copies of the sibling pointer
and a special "sense" bit indicating which sibling pointer is currently in-use.
Element copy and sibling pointer updates can hence be performed atomically with a single write to the node header as we
will see later. 

We next describe leaf node layouts. Inner nodes can use arbitraty layout as long as it contains a lock bit for writers 
to acquire. Leaf nodes are always aligned, whose size is a multiple of 256 bytes, which matches the internal buffer size 
of NVM for maximum throughput. In the following discussion we assume 256 byte nodes. 
The node consists of a header, an array of key-value pairts as elements, and two sibling pointers at the end. The header 
is 16 bytes, which consists of a 14-bit bitmap for each of the 14 elements in the node (assuming 8-byte keys and values), 
one "lock" bit for writer synchronization, one "sense" bit for shadowing the two sibling pointers (see below), and a 
14-byte fingerprint array. The fingerprint array has one byte hash value for each valid element in the node. Read operations
will first hash the key into a byte value, and then test whether it matches any of the 14 values in the fingerprint array
using SIMD comparison. A match indicates a possible hit, which should be confirmed with a full key comparison, and a miss 
indicates a definite miss.
