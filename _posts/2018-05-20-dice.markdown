---
layout: paper-summary
title:  "DICE: Compressing DRAM Caches for Bandwidth and Capacity"
date:   2018-05-20 21:16:00 -0500
categories: paper
paper_title: "DICE: Compressing DRAM Caches for Bandwidth and Capacity"
paper_link: https://dl.acm.org/citation.cfm?id=3080243
paper_keyword: DRAM Cache; Cache Compression
paper_year: 2017
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

Dynamic Indexing Cache Compression (DICE) is a technique designed to increase the bandwidth of the 
DRAM cache. Traditional algorithms of cache compression focus mainly on reducing the storage of 
a cache line by taking advantage of data patterns. This can indeed increase the cache hit rate for smaller
L1 and L2 caches, because more lines can be stored without the cost of larger SRAM. For DRAM caches, however,
compressing for capacity may not bring as much benefit as it would for smaller caches. First, DRAM caches 
are typically few hundreds megabytes or even gigabytes in size. Increasing the size of the DRAM cache by compressing 
lines has only marginal effect on the hit rate. Second, DRAM cache is more flexible than SRAM cache as it stores both 
tags and lines in DRAM array. It is hence easier for the cache controller to alter the storage format of compressed lines.
In SRAM this is impossible as extra tag arrays must be added. Furthermore, DRAM caches are usually direct-mapped,
because looking for multiple tags is costly. All of these subtlties suggest a different design perspective than 
what used to be for SRAM caches.

Instead of compressing for capacity, this paper claims that compressing for bandwidth in DRAM caches are 
beneficial to performance. If two cache lines can be delivered in one bus read transaction, and both lines are useful
for the computation, then the bandwidth of the DRAM cache is effectively doubled. Determining which two cache 
lines should be compressed into the same 64 byte storage, however, requires some careful thinking. The classical way of 
using lower bits of the line address maps spatially consecutive lines into sets that are far away from each other
and are likely not on the same row. Since the row buffer of a DRAM cache is typically several KBs (2KB in the paper), 
the spatial locality of the DRAM cache does not work well with the spatial locality of the access pattern. 

