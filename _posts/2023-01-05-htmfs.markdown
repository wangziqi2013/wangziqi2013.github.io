---
layout: paper-summary
title:  "HTMFS: Strong Consistency Comes for Free with Hardware Transactional Memory in Persistent Memory File Systems"
date:   2022-01-05 19:40:00 -0500
categories: paper
paper_title: "HTMFS: Strong Consistency Comes for Free with Hardware Transactional Memory in Persistent Memory File Systems"
paper_link: https://www.usenix.org/system/files/fast22-yi.pdf
paper_keyword: NVM; File System; HTM
paper_year: FAST 2022
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Comments:**

1. The paper seems to mix two different types of consistency: multi-threaded synchronization, i.e., consistency
at the memory ordering level, and durability, i.e., consistency for crash recovery. While Intel RTM guarantees both,
these two types of consistency requirements have different considerations. For example, durability problems can
occur even with a single thread. The paper started with consistency for crash recovery but focused more on
multi-threaded synchronization throughout the body.

This paper presents HTMFS, a file system that leverages existing Hardware Transactional Memory (HTM) support to
provide strong consistency guarantees while incurring low overhead. HTMFS achieves its design goals by wrapping 
multi-store operations within hardware transactions, whose atomicity is guaranteed by the hardware. As a result,
HTMFS, compared with similar designs that use journaling or shadow paging, achieves similar or better performance 
without weakening the consistency guarantees.

The paper was motivated by the high overhead for maintaining consistency in conventional file systems. The paper 
has noted that conventional file systems use either journaling or shadow paging to maintain crash consistency. 
Both approaches, however, involve a large overhead.
In the case of journaling, the same data is written twice, hence incurring 2x write amplification. 
In the case of shadow paging, even a small write to a data page can result in numerous writes to copy the page 
and update page pointers, causing an avalanche effect, as the paper authors have observed with NOVA.
Consequently, conventional file systems often chose to trade off consistency guarantees for better performance.

With the introduction of Non-Volatile Memory, file systems can be implemented as directly operating on the address 
space, thanks to a technique called DAX which maps the NVM storage to the virtual address space for random access.
As noted by the paper, this paradigm change has simplified the consistency challenge a little since processors can 
now perform atomic updates on a single cache block within the cache hierarchy. However, multi-update atomicity is 
still not guaranteed as the cache hierarchy may write back a dirty block halfway through the update, leaving the 
NVM image in an inconsistent state after a system crash. 

To address this problem, the paper noticed that HTM is a perfect solution as HTMs guarantee the atomicity of a 
code block (called a transaction) with regard to concurrently executing threads by hardware. On newer hardware 
models, HTM transactions are atomic with regard to failures as well, meaning that committed transactions are 
guaranteed to be persisted on the NVM even in the event of power failures (as with all dirty data in the 
cache hierarchy), while uncommitted transactions will simply be rolled back by the failure and none of its modified
data will be visible after the crash.

By leveraging such a novel and powerful hardware feature, NVM file system development is greatly simplified because 
developers can simply wrap all file system operations within a single transaction and let the hardware serialize 
them when multiple threads are accessing the same piece of data and on power failures. 
However, this ideal model is highly unrealistic for today's commercial HTM implementation (IntelÂ® RTM),
the reason being that today's RTM only supports limited transaction size regarding both read and write sets.
As a result, a transaction on RTM may never be able to commit due to the working set size exceeding a certain 
threshold, or due to the data access pattern.

The main contribution of this paper is to adapt HTMFS file system implementation to use RTM for its consistency
benefits while guaranteeing forward progress. From a high level, all file system operations are implemented as 
a three-stage process. In the first stage, the metadata required by the operation is read, without any write operation
on shared data (writes to private buffers that are only accessible to the thread itself and will not be persisted
after a power failure is fine). In HTMFS, every piece of metadata is accompanied by a sequence counter 
that stores the version of the metadata. The granularity that versions are tracked, though, is not revealed 
(and it can vary based on the logical meaning of the metadata).
In this stage, after a piece of metadata is read, the corresponding sequence counter is also accessed, whose value
will then be saved into the local buffer of the accessing thread.
Then in the second stage, the operation performs local computation to derive the new values that should be written 
to update shared metadata. 
In the last stage, an RTM transaction is started, and all writes are performed as an atomic unit within the transaction.
To verify that the metadata read during the first stage remains unchanged, at the beginning of the third stage, 
the operation will first validate the sequence counters stored in its local buffer against the current value of 
the counters, hence adding them to the transaction's read set. If any of the counters mismatches, indicating that 
some writer transaction must have already updated the metadata, the current operation is abandoned and then restarted.
Otherwise, the last stage performs the writes with values computed in the previous stage, and then increments 
the sequence counters for every piece of metadata it has modified. 
Any attempts to write the sequence counters after they have been verified will also cause the current RTM transaction
to abort, and the operation is restarted as if the validation had failed.
The operation logically commits when the RTM transaction from the last stage commits, after which point the metadata 
updates will be guaranteed to be atomic and persisted.

Data operations, however, cannot be performed as metadata updates as data operations can be unbounded in size. 
To deal with this challenge, the paper proposes several different techniques.
First, for read operations, instead of wrapping the entire operation in an RTM transaction, which will unlikely
to succeed if the read set is large, HTMFS simply records the page pointers and the sequence counter for those 
pointers, and validate them later during the last stage as if it were metadata accesses.

