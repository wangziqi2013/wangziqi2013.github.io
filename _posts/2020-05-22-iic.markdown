---
layout: paper-summary
title:  "A Fully-Associative Software-Managed Cache Design"
date:   2020-05-22 20:51:00 -0500
categories: paper
paper_title: "A Fully-Associative Software-Managed Cache Design"
paper_link: https://dl.acm.org/doi/10.1145/1028176.1006719
paper_keyword: Cache
paper_year: ISCA 2000
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Indirect Index Cache (IIC), a fully associative cache design featuring better hit rate and more flexible
replacement policies. The paper begins by identifying similarities between cache memory and virtual pages. Both are 
proposed and implemented as a measurement of dealing with the increasing speed gap between components in the memory hierarchy.
The difference is that on commercial produces, cache memories, at all levels, are typically implemented as a hardware-managed
array of tag and data. Tags are statically mapped to data slots. In order to access data using an address tag, tags
(or more likely, a subset of tags) need to be searched until a matching entry is found, after which the data slot is 
located at the same offset and accessed. Virtual memory, on the other hand, works differently. There is no fixed binding
between a virtual page frame and a physical page (although some implementations may enforce one to aid virtual address
caches or better performance). Any virtual page frame can hold any physical page. Hardware must consult a dedicated 
mapping structure to acquire the mapping instead. 

Being able to map cache lines in a fully associative manner is beneficial to cache memory design and application performance,
as pointed out by the paper. There are three reasons. First, a fully associative cache enables better chance of finding 
an ideal victim for replacement on capacity misses. On traditional caches, the victim must be found within the same set
as the address to be accessed. Fully associative cache treats all lines in the cache as candidates, allowing more 
complicated algorithm to be used for replacements. Second, some advanced designs can be implemented easily on fully 
associative cache, such as cache line pinning and hardware transactional memory. Cache line pinning "locks" a certain
cache block in the set, either to retain atomicity of data operations on that block or to protect the line from cache thrashing.
With a limited associativity per set, locking a line in the cache always means losing some of the flexibility of 
replacements. In the worst case, all cache lines in the set are locked, and the hardware will be trapped into a deadlock
when a new cache line is to be brought into the same set. The same applies to hardware transactional memory, such as 
Intel TSX, in which a set conflict misses (too many transactionally written lines in a set) will abort the transaction
and block progress. With a fully associative cache design, neither of the above two use cases can become a problem until
all lines in the cache are locked, which is an extremely rare corner case. The last reason is that cache partition can be  
implemented more efficiently on a fully associative cache. On regular caches, partitioning are often done on separate ways,
which has its limit.


