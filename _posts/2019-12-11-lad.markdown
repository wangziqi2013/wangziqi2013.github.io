---
layout: paper-summary
title:  "Distributed Logless Atomic Durability with Persistent Memory"
date:   2019-12-11 17:54:00 -0500
categories: paper
paper_title: "Distributed Logless Atomic Durability with Persistent Memory"
paper_link: https://dl.acm.org/citation.cfm?doid=3352460.3358321
paper_keyword: NVM; LAD; Memory Controller
paper_year: MICRO 2019
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Logless Atomic Durability (LAD), a hardware framework for supporting failure atomic transactions in
which no logging is involved for most of the time. The paper identified the problem of software logging as write amplification
and excessive write orderings which require persist barrier to be issued quite often, degrading performance. The paper also 
identifies that hardware schemes, such as Kiln, are making unrealistic assumptions which make the propal less attractive.
For example, Kiln assumes that the LLC is manufactured using STT-RAM and that the LLC can atomically commit a transaction
by performing a battery-backed cache tag walk. Neither of these two assumptions is realistic nowadays. Today's LLC is still
manufactured using SRAM, and is quite unlikely to be replaced by STT-RAM in the near future. Furthermore, Non-uniform
Cache Access (NUCA), which is common on server processors, partitions the LLC into several slices, each maintained by
a separate controller. Atomicity of operations, such as flash-clearing all "speculative" bits, as proposed by Kiln, is
not guaranteed. 

The paper makes the following observation about implementing failure atomicity. First, in a distributed environment 
such as NUCA and/or multi-core, atomicity of operation is not guaranteed, since devices act on their own, and only
communicate through pre-defined interfaces. In order to ensure that all of these devices make the same decision, Two-Phase
Commit (2PC) must be employed to determine the final state of the operation. Second, software logging is expensive, making
hardware assisted failure atomicity an attractive option. Among popular hardware logging designs, the paper points out 
that undo logging fits current NVDIMM the best for the following reasons: (1) For dynamic workloads (i.e. the write set
is only known at the end of the transaction, not before the transaction), undo logging supports in-place update, which 
eliminates the remapping table in order for a transaction to read its own dirty data; (2) Although both redo and undo
logging suffer from write amplification problem, undo logging has better access locality than redo logging. In undo
logging, we need two row activations to write the undo log and the data, while in redo logging, we need one row activation
to write redo data, and another two to read the log and to update data in-place after log commit; (3) Undo log entries 
are never read except for recovery. The log can be trivially removed after the transaction has committed, while in redo
logging the log has to be replayed for in-place updates. Based on the above reasons, LAD uses undo logging whenever the 
write set of a durable transaction exceeds what the hardware could support. The last observation is that the memory hierarchy
naturally serves as an intermediate buffer for pending updates that are supposed to be atomic. Part of the hierarchy is 
even made persistent using battery-backed SRAM to decrease latency of certain operations. The paper proposes that instead
of letting these buffers drain as quickly as possible, we only clear these buffers lazily, taking advantage of the non-volatility
of the buffers to implement a small "shadow page" for the write set. In this paper, the write pending queue (WPQ) is 
considered as non-volatile, which has been implemented on recent commercial products. 

The paper makes the following assumptions, First, programmers rely on a transactional software interface to declare failure
atomic sections in which either all stores are persisted, or none of them is persisted. The failure atomic section is denoted
using a "persistent{}" construct, which is translated into section begin and commit instructions. The processor will start 
and commit the failure atomic section accordingly when these instructions are seen, and treat all stores within the section
as failure atomic. Transactions in LAD are identified by a globally unique transaction identifier which consist of the 
ID of the thread and the thread-local serial number of the transaction instance. The thread-local serial number can be 
allocated by just incrementing a counter that is accessed exclusively by the thread. In the following discussion, we 
simply use the term "txn ID" to refer to the global identifier. Second, the paper assumes that the system consists of 
multiple memory controllers, which have non-volatile WPQ as discussed in the previous paragraph. The write set of a transaction 
can be scattered over all memory controllers, which require the processor to notify each of them about the transaction 
commit. In addition, this paper assumes an interconnection network in which packets to some controllers might be lost while 
some other controllers receive successfully. As we will see later, this assumption plays an important role in affecting 
the design of LAD, since a transaction can be only partially committed after the crash on some controllers, while uncommitted 
on the rest.

LAD extends both the L1 cache controller and the memory controllers by adding extra states. On the cache controller side,
we first add a bitmap in which each bit represents whether the corresponding block is dirty during the transaction. Dirty 
blocks will be handled differently when they are evicted, requested via coherence, or when the transaction commits. We also
add a thread-local counter which is part of the thread context for dispensing transaction ID. An ACK counter on L1 controller
records the number of pending memory flushes that have not been acknowledged by the memory controller. On the memory controller
side, we extend the WPQ by adding a few extra fields for each entry in the queue. These fields are: (1) A "speculative" bit
to indicate whether the memory write is uncommitted or not. The memory controller must not write a speculative entry
back to the NVM, unless there is no free space in the WPQ when a new entry arrives. In this case we spill the speculative
states to a logging area, as we will describe below; (2) A transaction ID field recording the ID of the transaction that
generates the store request. We also add a mapping structure (implemented as an array) to map thread IDs to the most recent
committed transaction ID of that thread. To limit the size of this mapping structure, LAD puts an upper bound of 256
as the maximum number of threads that are allowed to execute concurrently. This mapping structure is updated when a thread
commits and sends the global ID to the memory controller during the commit protocol. All newly added fields and structures 
on the memory controller are also battery backed, such that their contents can be restored after a crash.

We next describe the operation of LAD. When a new transaction is started, we assign a transaction ID by combining the current
thread ID with the thread-local instance counter. The instance counter is incremented every time after the assignment. All
store operations sent to the memory controller during the transaction are tagged with this transaction ID. Load operations
are not affected. Store operations will set the corresponding bit in the per-core bitmap to indicate that the block
has not been written back to the NVM for persistence. Note that although this bitmap logically belongs to the transaction,
the paper suggests that the bitmap be cleared when context switch happens instead of saving the bitmap into the thread
context. All dirty blocks indicated by the bitmap are written back to the NVM (via the memory controller). When a block
is evicted out of L1 or requested via coherence, the content of the block is written back through the memory hierarchy
to the memory controller, updating lower level cached blocks if any. The bit is cleared after a dirty block is written
back.
