---
layout: paper-summary
title:  "Hardware-Assisted Data Compression for Energy Minimization in Systems with Embedded Processors"
date:   2021-06-23 01:37:00 -0500
categories: paper
paper_title: "Hardware-Assisted Data Compression for Energy Minimization in Systems with Embedded Processors"
paper_link: https://dl.acm.org/doi/10.5555/882452.874530
paper_keyword: Memory Compression; Delta Encoding; diff123
paper_year: DATE 2002
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes a main memory compression technique for embedded systems. Embedded systems are often deployed in
environments where energy supply is limited and hence should be utilized wisely.
The paper is motived by previous proposals that perform instruction compression for reducing energy consumption,
and it extends compression to data memory.
The challenge, however, is that data memory must be optimized for both compression and decompression, as both will
be performed online and are performance critical. Instruction compression, on the other hand, only needs to be fast 
on decompression, as instructions can be compressed off-line, and will unlikely be modified in the runtime.

The paper identifies two reasons why data compression helps saving energy. First, less data bits are streamed from
the main memory on accesses, which can reduce power related to row buffer activities.
Second, since blocks are transferred over the memory bus in compressed form, the bus also enjoys the energy benefit
of compression, as less bits are being transferred.
Despite previous studies which show that the extra hardware added for compression will offset the energy benefit 
of reduced data activity, the paper argues that compression can still be beneficial if the compression circuit
is designed to be simple enough.

We next describe the overall architecture. The compression scheme works for data transferred between the LLC and the
main memory. The proposed logic is implemented as a Compression and Decompression Unit (CDU), which is inserted
on the datapath between the LLC and the main memory. Data written back from the LLC is first compressed before it is
sent to the memory bus. Access requests to the main memory are also intercepted by the CDU, and the CDU fetches
the requested block from the main memory in either compressed or uncompressed form. 
The CDU is put on the LLC side such that data transfers between the LLC and the main memory use compressed blocks
whenever possible.

Memory blocks are stored in the DRAM in compressed form. Not all blocks are compressed, though. The paper proposes
that part of the main memory be dedicated to storing compressed blocks (called a compressed region), while the 
rest still stores uncompressed data. The paper noted that this is equivalent to having a compressed DRAM buffer 
between the LLC and the uncompressed main memory.
Blocks that are stored in the compressed region need extra address translation when being accessed, as the cache
hierarchy only uses their canonical addresses in memory requests.
The address translation can be a major bottleneck if not handled properly, since in the most straightforward
implementation, we need a translation table in the CDU, which stores the addresses of all blocks in the 
compressed region, and their offsets, which incurs considerable metadata overhead.
The paper proposes a simple address translation scheme as follows: The compression region is divided into fixed size
slots, the size of which is less than a full cache line. Only blocks that can be compressed to below this size will
be stored in the compressed region, while others are just stored in their canonical addresses.
The address translation table, therefore, only stores a list of the addresses of blocks in the region, without 
maintaining the offsets for each block, as the offset can be computed by multiplying the position in the list with
slot size.
The translation table can be implemented as a CAM structure where each entry stores a block address, which outputs the
index that hits the CAM given the requested address. The index is then used to compute the offset in the compressed 
region.

Using CAM for address translation can still be too heavyweight, as CAMs are power hungry.
To entirely get rid of the translation table, the paper further proposes to co-locate address tags with compressed
block data in the compressed region, rather than using an on-chip CAM.
In addition, blocks and tags in the compressed region can be organized as either direct-mapped or ser-associative.
Memory requests are then directly used to probe the tags in the compressed region just like how a SRAM cache is
probed. If one of the tags hit, then the block is streamed out from the compressed region. 
Otherwise, it is accessed from the home location. 
This optimization, however, increases main memory activity, since all memory accesses need to check the 
compressed region first, largely offsetting the first benefit discussed above.
The bandwidth benefit is still valid, though.

The paper then proposes two types of compression algorithms.
The first type relies on offline value profiling to find the most frequent N values (N = 128 in this paper).
These values are then stored on a CAM structure for lookup during compression and decompression.
During compression, 32-bit words that are in the CAM will be represented with only log2(N) bits, while 
the rest are stored in uncompressed form. 
A bit vector header is attached at the beginning to indicate which word is compressed and which is not.
Decompression logic uses this bit vector to interpret the data code words that follow.
The paper argues that while value profiling may not be practical for general purpose computing, for embedded
systems it may actually work pretty well, since only a small number of programs are executed.

The paper further proposes three more compression algorithms using delta encoding. The paper observes that many values
in the same cache block are close to each other in numeric values. As a result, those values are likely to share the
same upper bits.
The first algorithm, diff1, takes advantage of this by comparing the 2nd, 3rd and 4th word in the block 
(the paper assumes 4-word block) with the first word, and computes the number of bits in the longest common prefix.
The length of the prefix is stored in a 5-bit field, and for the last three words, the prefix will not be stored
(the first one is still stored as 32-bit word).
On decompression, the prefix will be added back to the last three words.
