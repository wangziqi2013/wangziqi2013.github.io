---
layout: paper-summary
title:  "HOOP: Efficient Hardware-Assisted Out-of-Place Update for Non-Volatile Memory"
date:   2020-12-23 15:48:00 -0500
categories: paper
paper_title: "HOOP: Efficient Hardware-Assisted Out-of-Place Update for Non-Volatile Memory"
paper_link: https://dl.acm.org/doi/10.1109/ISCA45697.2020.00055
paper_keyword: NVM; Redo Logging; HOOP
paper_year: ISCA 2020
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Highlight:**

1. Using redo logging for failure atomicity. The read redirection problem is addressed by an on-memory mapping table,
   which indices the redo log entries for both committed and uncommitted data.
   Log replay is performed in the background by applying log entries back to the home addresses, and then remove the
   mapping table entry.
   Transactional dirty data lines are discarded on eviction to avoid polluting the home address image.

2. The cache hierarchy does not track the write set (read set is not tracked for failure atomicity) of a transaction.
   Instead, the write set is sent to the memory controller when they are first written. 
   This is different from a design where a subset of writes are cached in the hierarchy, which is only flushed back
   on transaction commit via tag walk.
   HOOP avoids the tag walk at the cost of longer first-time write latency.

3. GC interval is upper bounded by the controller's mapping table size

4. The paper also attempts to reduce write amplification by (1) Coalescing writes from multiple committed txns by
   a group replay and only selecting the most up-to-date wrote; (2) Byte-granularity logging instead of cache line
   granularity.

**Lowlight:**

1. This paper makes a fundamental mistake of claiming that the algorithm is shadow paging, while it is actually redo 
   logging with an auxiliary index. I do appreciate this combination, which has not been fully explored. I also quite 
   appreciate the idea of amortizing multiple transactions' updates to one log replay, and byte-granularity logging, 
   but by the end of the day, this is really not shadow paging.
   In shadow paging design, there is no fixed home location for data items to be written back, which is also the 
   biggest difference between shadow paging and redo logging.
   Also the paper separates shadow paging with log-structured design, but these two are in fact the same thing,
   i.e., log-structured NVM is just an aggressive case of shadowing.

2. If the on-controller mapping table is full, then GC must be invoked, and there is no way to avoid this.
   The problem is the size of a transaction is upper bounded by the mapping table size, since otherwise the
   table would overflow first, but no entry can be released, since no committed log entry is replayed.
   The paper failed even to mention this issue.

3. The maximum size of the log buffer area is also upper bounded by the mapping table size, but the paper seems
   to be suggesting that the log buffer can be arbitrarily large?

4. The paper indicates that a persistent bit is added to each line in the hierarchy, but does not clearify
   how this bit is used. Is it used for tracking the write set?

5. The paper does not tell what happens when a transactionally written dirty line is written back. Are they discarded,
   or are they treated like a normal write back? I would say the former is the correct answer, since in the latter
   case, the line will pollute the working image before the transaction commits or aborts. making it impossible to
   recover.
   The former option, however, requires the hardware to track transactionally written lines using the bit
   in point 4, which requires a tag walk on commit to clear the bit (but not performing data eviction).

6. The paper is also inconsistent about how log entries are generated.
   On page 5 it is claimed that "Whenever a cache line is evicted from the LLC within a
   transaction, the cache line is written into the OOP region.", indicating that dirty lines are redirected to the
   log buffer, while on page 8 under "Store Operation" subsection, it is stated that 
   "As a result, the cache controller will send the
    modified data and its home-region address to HOOP", indicating that the cache controller directly send updated
    data and the physical address to the memory controller on each update. In this case dirty lines are in fact
    not needed.

7. What if there are multiple memory controllers? Do they work together on the collective log buffer, or they have
   private log buffer? How does GC work? Do you elect a master controller to perform GC, or you let them work
   in parallel (which is difficult to do)?

8. I do not get why eviction buffer is required? The paper seems to be suggesting that GC will lock out the entire
   NVM device to avoid race conditions. 

This paper proposes HOOP, a hardware redo logging design with low write amplification and performance overhead for
achieving transactional failure atomicity.
The paper is motivated by the fact that most previous designs either use logging or shadow paging, which both have 
flaws. Logging, for example, requiring writing the same piece of data twice, first to the log buffer, and then to
the home location, which doubles the write bandwidth to the NVM device, harming device lifetime as well as available
bandwidth on the bus. In addition, both undo and redo logging approaches enforce write ordering between log entry
and dirty data, which is on the critical path of the execution, degrading performance as the pipeline gets frequently stalled.
Shadow paging, on the other hand, still incurs write amplification if implemented in page granularity. With cache
line granularity shadowing, writes no longer require duplicating a page, but previous hardware proposals introduce
other performance bottleneck such as TLB translation entries which brings TLB shootdown cost on each entry update. 
Furthermore, the paper points out that log-structured NVM is also infisible, despite good write performance and 
locality, due to the high read indirection cost, which can be as bad as O(log(N)) where N is the number of log 
entries on the device.

This paper addresses the above challenges with a combination of techniques. First, to reduce the write amplification
of logging, the paper proposes that log entries should be generated in byte granularity, since it is observed that
many cache lines are only updated sparsely.
In addition, the paper also proposes that writes in different transactions to the same data location can be coalesced 
into a single write, saving log replay bandwidth.
Second, redo log entries are not flushed in the foreground, stalling the processor as transaction commits. Instead,
log entries are directly generated into the log buffer on the memory controller, which are then written back to the 
NVM in the background without any cycle overhead.
Third, to avoid read redirection problem with redo logging, HOOP uses a mapping table on the memory controller serving 
as the index of redo log entries in the log buffer. Data requests are redirected to the log buffer, if the mapping
table indicates that a log entry exists, which contains the more up-to-date image.

We describe the hardware changes as follows. The main function of HOOP is implemented on the memory controller.
Two buffers are added to the controller: A data buffer for holding log entries that have not yet been written back
to the NVM, and an eviction buffer storing evicted lines while GC is being performed. 
