---
layout: paper-summary
title:  "A Frequent-Value Based PRAM Memory Architecture"
date:   2020-12-03 17:08:00 -0500
categories: paper
paper_title: "A Frequent-Value Based PRAM Memory Architecture"
paper_link: https://ieeexplore.ieee.org/document/5722186/
paper_keyword: Compression; NVM; Frequent Value Compression
paper_year: ASP-DAC 2011
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Lowlight:**

1. This paper assumes that NVM is used as a cache for disk. I think a more proper expresion would be that the 
   NVM device is used as main memory and that disk is used as swap storage. 

This paper proposes a compressed NVM architecture for better wear-leveling using frequent value compression.
The paper identifies one of the most important issue with NVM is that its lifetime is not infinite, which actually
has limited write-erase cycles. Without proper management, frequently written cells will wear out fast (in a matter
of days), rendering the entire device unusable.
Another minor issue is that NVM write energy is much larger than DRAM. Reducing the number of bits written into
the device, therefore, becomes critical for energy consumption.

The paper assumes the following architecture. NVM is deployed as a fast, byte-addressable cache between the cache
hierarchy and a backing store, such as hard disk. Cache lines can be brought into and written back from the NVM
device as they are accessed and evicted respectively. 
Compression is performed on a word-to-word basis, with the word size before and after compression being configurable 
parameters. The word size after compression also limites the size of NVM's internal code word mapping table, 
the size of which must not be larger than 2^K where K is the number of bits in compressed words. 
The mapping table is necessary for both compression and decompression, as we will see below.

This paper adopts frequent-value compression, which is commonly used in cache compression proposals, in NVM
architecture to preserve energy and protwct device, rather than saving storage. 
Each NVM row now has a different layout. Instead of being a non-structured, flat bit array, which
stores uncompressed words one next to another, the paper proposes that each row be divided into compressed words.
Each compressed word consists of a data region, and a bit indicating whether it stores a compressed word or not.
The data region takes the same number of bits as an uncompressed word.
If the bit is "1", then the data region stores an uncompressed word, and otherwise, it stores a compressed word 
(the exact bit offset where the word starts is non-trivial, as we will see later).
Each row also has one extra bit indicating whether it has been written into since brought into the NVM.
This bit is used to identify whether decompression is needed while it is accessed. The bit is cleared when a row
is populated with data just read from the backing store, and is set when a write back request with dirty
data is received from the LLC.
If the bit is clear, then a special data path will be activated to bypass the decompression circuit, which enables
faster reads of uncompressed data, and saves energy by clock-gating the uncompression circuit in this case.

Data is compressed when being inserted into the row. This happens when dirty data is written back from the LLC, and when
read from the disk. Data is decompressed when they are fetched by the LLC, and when they are written back to the disk, 
if dirty.
Compression works by replacing commonly occuring values with a translated code word, which uses less number of bits.
Decompression works by performing the reverse of compression, i.e., replacing compressed code words with their original
values. These two tasks utilize a hardware mapping table, which is implemented as a multi-ported SRAM and CAM, which 
stores uncompressed words identified as frequent values in different locations.
During compression, the CAM is used with the uncompressed word being the data to be searched, and the index returned 
is used as the compressed code word. During decompression, the SRAM is used with the compressed code word as index.
The content of the slot is retrieved and written to the input as the uncompressed word. 