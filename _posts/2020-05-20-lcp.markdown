---
layout: paper-summary
title:  "Linear Compressed Pages: A Low Complexity, Low-Latency Main Memory Compression Framework"
date:   2020-05-20 22:40:00 -0500
categories: paper
paper_title: "Linear Compressed Pages: A Low Complexity, Low-Latency Main Memory Compression Framework"
paper_link: https://ieeexplore.ieee.org/document/7847624/
paper_keyword: Memory Compression; LCP
paper_year: MICRO 2013
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Linearly Compressed Pages (LCP), a main memory compression framework featuring easy algorithm design,
low-cost deployment, and memory management. Broadly speaking, main memory compression schemes exist to satisfy one or both 
of the two goals.
The first goal is to reduce memory usage by allowing cache lines or pages to be packed in a more compact form after 
compression. This way less physical memory can be allocated to a full 4KB page frame, and hence more pages can be mapped
into virtual memory at the same time. The second goal is to save bandwidth when fetching cache lines into the cache 
hierarchy. Memory designs either employ compression as a prefetching scheme, allowing adjacent cache lines to be brought 
into the LLC in the same transaction to fulfill a cache miss, or redesign memory bus protocol such that less data can be 
fetched per-transaction, increasing bus transaction throughput. 

The paper points out that both commercial memory compression schemes or proposals face several challenges that make
the above two goals difficult to achieve. First, memory compression changes one of the most fundamental abstraction
that the physical address space is linearly mapped to the virtual address space. The linear map not only
simplifies physical address computation, but also simplifies memory management, since the OS can conveniently divide the 
physical address space into uniformly sized pages, and manage it as a fix-sized page buffer pool. 
With compression, pages can be variably sized, which introduces external fragmentation as in malloc().
Second, by compressing cache lines, we store them in a different hardware address as the translated physical address.
This complicates the semantics of cache tags, as the cache tag used to indicate both the physical address and hardware 
address. By introducing compression, there are now three address spaces: Virtual address space, physical address,
and compressed address space. Some designs, such as IBM MXT, explicitly define three address spaces, and require that
the physical address be translated when DRAM is accessed. This translation typically lies on the critical path of 
DRAM operations, which can degrade performance, unless speculation is employed to overlap access with address mapping a 
bit. Speculation itself, however, may introduce extra complexity and verification cost. 