---
layout: paper-summary
title:  "Linear Compressed Pages: A Low Complexity, Low-Latency Main Memory Compression Framework"
date:   2020-05-20 22:40:00 -0500
categories: paper
paper_title: "Linear Compressed Pages: A Low Complexity, Low-Latency Main Memory Compression Framework"
paper_link: https://ieeexplore.ieee.org/document/7847624/
paper_keyword: Memory Compression; LCP
paper_year: MICRO 2013
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Linearly Compressed Pages (LCP), a main memory compression framework featuring easy algorithm design,
low-cost deployment, and memory management. Broadly speaking, main memory compression schemes exist to satisfy one or both 
of the two goals.
The first goal is to reduce memory usage by allowing cache lines or pages to be packed in a more compact form after 
compression. This way less physical memory can be allocated to a full 4KB page frame, and hence more pages can be mapped
into virtual memory at the same time. The second goal is to save bandwidth when fetching cache lines into the cache 
hierarchy. Memory designs either employ compression as a prefetching scheme, allowing adjacent cache lines to be brought 
into the LLC in the same transaction to fulfill a cache miss, or redesign memory bus protocol such that less data can be 
fetched per-transaction, increasing bus transaction throughput. 

The paper points out that both commercial memory compression schemes or proposals face several challenges that make
the above two goals difficult to achieve. First, memory compression changes one of the most fundamental abstraction
that the physical address space is linearly mapped to the virtual address space. The linear map not only
simplifies physical address computation, but also simplifies memory management, since the OS can conveniently divide the 
physical address space into uniformly sized pages, and manage it as a fix-sized page buffer pool. 
With compression, pages can be variably sized, which introduces external fragmentation as in malloc().
Second, by compressing cache lines, we store them in a different hardware address as the translated physical address.
This complicates the semantics of cache tags, as the cache tag used to indicate both the physical address and hardware 
address. By introducing compression, there are now three address spaces: Virtual address space, physical address,
and compressed address space. Some designs, such as IBM MXT, explicitly define three address spaces, and require that
the physical address be translated when DRAM is accessed. This translation typically lies on the critical path of 
DRAM operations, which can degrade performance, unless speculation is employed to overlap access with address mapping a 
bit. Speculation itself, however, may introduce extra complexity and verification cost. 
The last challenge is that even compression is conducted on a coarser granularity, such as pages, there is still 
the problem of locating a compressed line within the page. Due to the fact that compressed lines may not be of 
unified sizes, in order to access a line in the middle of a page, the DRAM controller should either compute the prefix
sum of all previous cache line sizes, or per-compute and cache such information in a dedicated cache. This also complicates
the design, and increases DRAM's access latency.

LCP, on the other hand, avoids the above complexities by taking advantage of the following observations. First, cache 
lines on the same page typically have similar compressability, since they often store data of the same kind from the 
same process. It is, in most cases, beneficial to assume that these cache lines can be compressed into similar sizes.
The second observation is that both address mapping and tag address mapping can be performed with trivial hardware 
if the mapping is linear, i.e. the hardware address of compressed lines can be decomposed into a base address plus 
the index multiplied by a constant. Both observations imply that if we sacrifice some efficiency of compression in 
exchange for uniformity of cache line sizes, the complexity of the resulting compression can be significantly lowered.

Compressed pages in LCP are sorted into a few per-defined sizes. The paper suggests using 512B, 1KB, 2KB and 4KB size
classes, which correspond to different compression ratios (there is no fixed ratio as goals). 
The simplcity of page size classes also simplify OS memory management, since the OS can still manage the physical address 
space as a page buffer pool, expect that there are multiple size classes just like how malloc() works.

We next introduce the page layout as follows. Each compressed page, no matter the size, consists of three parts. The first
part is compressed cache lines, linearly mapped into the page. The size of each compressed line is determined by the 
compression algorithm, and can hence be known when the algorithm is selected. Note every cache line in the page is stored
in this part. For those who cannot be compressed to fit into a slot, they are classified as "exceptions", and will be 
stored in the overflow area, as we will see below. 