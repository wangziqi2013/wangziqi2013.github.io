---
layout: paper-summary
title:  "COSPlay: Leveraging Task-Level Parallelism for High-Throughput Synchronous Persistence"
date:   2021-10-28 21:10:00 -0500
categories: paper
paper_title: "COSPlay: Leveraging Task-Level Parallelism for High-Throughput Synchronous Persistence"
paper_link: https://dl.acm.org/doi/10.1145/3466752.3480075
paper_keyword: NVM; Persist Barrier; COSPlay
paper_year: MICRO 2021
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Comments:**

1. My biggest concern is that this mechanism require a few co-routine tasks to be assigned to the same core for fast
   switch (b/c otherwise they will not share the same store buffer anyway). This would have serious performance
   implications because of resource contention, locality problems, etc. Servers typically only schedule one or two
   threads per core for this exact reason. Although the paper uses a different switching mechanism (coroutines),
   the resource contention problem is not changed from the conventional thread model.

2. This is like hyper-threading, why not just extend hyper-threading such that threads will switch when an sfence is 
   active? How many coroutines can fully saturate the core pipeline to avoid stalls? Surely this number will not be
   very large, due to the short latency of NoC NVM?

3. It makes perfect sense to call it COSP rather than COSPlay.

This paper introduces Coroutines for Synchronous Persistence (COSPlay), a hardware-software co-design that exploits 
task-level parallelism from the software side, and virtualized write buffer on the hardware side, for improved 
persistence throughput. The paper is motivated by the fact that current support for persistence ordering on commercial 
processors is limited to per-core barriers that block all future store and flush operations while existing 
outstanding flushes are being served. This coarse-grained mechanism guarantees safety, but causes unnecessary 
pipeline stalls, since not all store and flush operations waiting to be executed are dependent on the current
persistent operation.
This paper proposes that: (1) Stores and flushes should be made context-tagged, and the store buffer holding committed
store operations should be virtualized such that entries from different contexts can be distinguished and processed
independently, which maximizes the throughput; (2) Application threads should be switched out while a persist
barrier is in effect, overlapping the persistence overhead with the execution of another thread.

The paper first briefly discusses the x86 persistency model. Due to the existence of the write-back cache hierarchy, 
the ordering of NVM writes on x86 platform for cache-able memory is decoupled from the regular consistency order. 
Programmers should manually issue cache line flush or write back instructions (without losing generality, we use
clwb to represent both in the rest of this article) to force the cached content to be written back to the NVM,
and the order of these write backs that is actually observed on the NVM side is not guaranteed to be consistent
with the order of flush instructions in program order for two reasons. 
First, the processor does not enforce ordering between 
flush instructions on different addresses, meaning that they could potentially be reordered when deemed necessary
by the processor. Secondly, even if flush instructions are strongly ordered, the NoC network connecting the 
store queue and the NVM controller may still deliver messages out-of-order, making the task of inferring write 
ordering impossible without proper fencing.

Luckily, x86 provides a store fence instruction, namely sfence, to order flushes and stores. The sfence instruction, 
when decoded in the frontend, will block future stores and flushes to be allocated in the store queue, until
the queue has been drained. This conservative implementation essentially stalls the pipeline while outstanding
clflush operations and store instructions are being serviced, the performance impact of which is dependent on the 
aggregated latency of these operations. 
The paper conducted studies on the slowdown caused by the store fence, and unsurprisingly, as the memory access 
intensity of application increases, the performance becomes worse than non-fenced executions.
In addition, the paper also points out that Backend Memory Operations (BMOs) will further aggravate this problem, 
since BMOs are on the critical path of writes, and will make the latency longer. Evaluation shows that as the latency
of BMOs becomes longer, the overall performance also degrades proportionally. 

The paper observes two types of inefficiencies in the above persistence model. First, the processor pipeline is stalled
while waiting for the store buffer to drain out, while there may be some other logical tasks
that are ready to be executed. This is analogous to the classic I/O latency-hiding problem, which is solved in modern
systems by switching to a different thread and overlaps the execution of that thread with I/O.
Performing thread switches at long latency sfence instructions, however, are unrealistic,
both because there is no notification mechanism on sfences (unlike blocking on I/O operations, in which case the OS
issues the I/O and switches out the thread), and because of the switching overhead, which is a few orders of magnitude
larger than the NVM latency.
To enable fast task switches without incurring significant overhead, the paper proposes using a lightweight 
task-based execution model within a single thread, implemented as software coroutines.
In the proposed execution model, tasks are self-contained, logical executions of a single operation, which may 
perform NVM writes and use persist barriers to commit the write.
Different tasks do not communicate with each other to avoid complication of synchronization, and each task can be 
scheduled and executed till the end.
Three utility functions control the lifetime and scheduling of the tasks: TASK\_START(), TASK\_END(), and YIELD(),
with the first two creating and terminating tasks, respectively, and the last one relinquishing control over to the 
scheduler, which is called right after the flush instructions in the persist barrier, but before the sfence.
On a YIELD() call, control is transferred to the scheduler, which selects another waiting coroutine in a round-robin
manner to execute.
Note that the sfence instruction is still necessary, even if it is only executed after the coroutine is scheduled 
again after issuing flushes and calling YIELD(), because at that moment, it is not still unknown whether the previous 
flushes have completed, and the sfence is just a conservative measure to ensure safety.

The second type of inefficiency is the fact that the effect of sfence is global, meaning that it blocks the execution
of the pipeline until all operations in the store buffer have been drained, regardless of whether these operations are
actually related to the flushes that the sfence is intended for, or just some innocent clflush from another 
logical task that has no dependency on the current one.
