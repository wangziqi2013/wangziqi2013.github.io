---
layout: paper-summary
title:  "COSPlay: Leveraging Task-Level Parallelism for High-Throughput Synchronous Persistence"
date:   2021-10-28 21:10:00 -0500
categories: paper
paper_title: "COSPlay: Leveraging Task-Level Parallelism for High-Throughput Synchronous Persistence"
paper_link: https://dl.acm.org/doi/10.1145/3466752.3480075
paper_keyword: NVM; Persist Barrier; COSPlay
paper_year: MICRO 2021
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Comments:**

1. My biggest concern is that this mechanism require a few co-routine tasks to be assigned to the same core for fast
   switch (b/c otherwise they will not share the same store buffer anyway). This would have serious performance
   implications because of resource contention, locality problems, etc. Servers typically only schedule one or two
   threads per core for this exact reason. Although the paper uses a different switching mechanism (coroutines),
   the resource contention problem is not changed from the conventional thread model.

2. It makes perfect sense to call it COSP rather than COSPlay

This paper introduces Coroutines for Synchronous Persistence (COSPlay), a hardware-software co-design that exploits 
task-level parallelism from the software side, and virtualized write buffer on the hardware side, for improved 
persistence throughput. The paper is motivated by the fact that current support for persistence ordering on commercial 
processors is limited to per-core barriers that block all future store and flush operations while existing 
outstanding flushes are being served. This coarse-grained mechanism guarantees safety, but causes unnecessary 
pipeline stalls, since not all store and flush operations waiting to be executed are dependent on the current
persistent operation.
This paper proposes that: (1) Stores and flushes should be made context-tagged, and the store buffer holding committed
store operations should be virtualized such that entries from different contexts can be distinguished and processed
independently, which maximizes the throughput; (2) Application threads should be switched out while a persist
barrier is in effect, overlapping the persistence overhead with the execution of another thread.

The paper briefly discusses the x86 persistency model. Due to the existence of the write-back cache hierarchy, 
the ordering of writes on x86 platform for cache-able memory is decoupled from the regular consistency order. 
Programmers should manually issue cache line flush or write back instructions (without losing generality, we use
clwb to represent both in the rest of this article) to force the cached content to be written back to the NVM,
and the order of these write backs that is actually observed on the NVM side is not guaranteed to be consistent
with the order of flush instructions for two reasons. First, the processor does not enforce ordering between 
flush instructions on different addresses, meaning that they could potentially be reordered when deemed to be 
convenient by the processor. Secondly, even if flush instructions are strongly ordered, the NoC network connecting the 
store queue and the NVM controller may still deliver messages out-of-order, making the task of inferring write 
ordering impossible without proper fencing.

Luckily, x86 provides a store fence instruction, namely sfence, to order flushes and stores. The sfence instruction, 
when decoded in the frontend, will block future stores and flushes to be allocated in the store queue, until
the queue has been drained. This conservative implementation essentially stalls the pipeline while outstanding
clflush operations and store instructions are being serviced, the performance impact of which is dependent on the 
aggregated latency of these operations. 
The paper conducted studies on the slowdown caused by the store fence, and unsurprisingly, as the memory access 
intensity of application increases, the performance becomes worse than non-fenced executions.
In addition, the paper also points out that Backend Memory Operations (BMOs) will further aggravate this problem, 
since BMOs are on the critical path of writes, and will make the latency longer. Evaluation shows that as the latency
of BMOs becomes longer, the overall performance also degrades proportionally. 


