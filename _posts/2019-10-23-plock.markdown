---
layout: paper-summary
title:  "pLock: A Fast Lock for Architectures with Explicit Inter-Core Message Passing"
date:   2019-10-23 15:17:00 -0500
categories: paper
paper_title: "pLock: A Fast Lock for Architectures with Explicit Inter-Core Message Passing"
paper_link: https://dl.acm.org/citation.cfm?doid=3297858.3304030
paper_keyword: SW26010; Synchronization; Lock; Message Passing
paper_year: ASPLOS 2019
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper presents pLock, a distributed locking scheme for high performance computing (HPC) implemented on SW26010 architecture.
Typical HPC architectures use Explicit Message Passing (EMP) to implement inter-core communication. Shared data between
cores must be explicitly sent and received by primitives rather than implicitly acquired via cache coherence protocol.
In fact, most HPC does not implememt cache coherence and shared address space for efficiency reasons. As a result, these
HPC platforms support the abstraction of critical sections using one or more dedicated lock servers. Before entering the
critical section, a core must send a request to the lock server, which is then granted if the lock is currently unheld,
or queued into the requestor list by the lock server if otherwise. When a thread finishes the critical section, it
sends an unlock message to the lock server, which causes the latter to either release the lock, or grant the lock
to the next requestor in the requestor list. For N cores and S critical sections on each core, this will incur 3NS
messages (1 for request, 1 for grant, and 1 for release).

This paper observes that this classical EMP scheme is suboptimal for two reasons. First, the 3NS message overhead can
be further reduced by use of clever lock granting scheme. Second, on an architecture with non-uniform communication latency,
lock acquisition and release may take longer for some cores for every request, which further degrades performance. This
paper solves the first problem by allowing locks to be passed between peers to reduce lock granting message overhead. The 
second problem is solved by adding local lock servers.

The paper is based on SW26010 platform, which consists of "core groups". A core group is formed by connecting 64 cores
in a 8 * 8 mesh, in which each non-boundary core is directly connected to its neighbors in the grid. Although each processor 
has its own private and shared cache, no cache coherence is implemented in the core group. Processors explicitly send and 
receive messages using hardware primitives built into the ISA.