---
layout: paper-summary
title:  "Optimizing Hash-Array Mapped Tries for Fast and Lean Immutable JVM Collections"
date:   2020-08-28 17:47:00 -0500
categories: paper
paper_title: "Optimizing Hash-Array Mapped Tries for Fast and Lean Immutable JVM Collections"
paper_link: https://dl.acm.org/doi/10.1145/2814270.2814312
paper_keyword: HAMP; CHAMP; Persistent Data Structure
paper_year: OOPSLA 2015
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Lowlights:**

1. This paper is written badly, with lots of jargons and concepts unexplained (what are "elements"? It took me a while to 
   figure out that the authors are referring to terminal level values, which is to differentiate it from next-level child
   nodes). I inferred most parts by guessing instead 
   of reading the actual text. Some terminologies are not used properly, such as "memorization", which is often used to
   refer to a programming technique that is used in top-down recursion.
   One example of bad grammar is on page 8, right below listing 6:
   "With MEMCHAMP we will refer throughout the text to the
    variant of CHAMP that adds memoized element hash codes,
    but drops incremental collection hash codes."
    Can you be more obsecure with this sentence structure?

2. I did not see how memory footprint is reduced. In the HAMT design, each node must contain 32 slots (actually, 64, since 
   full keys are also stored). In CHAMP, although empty slots are removed using `nodeMap` and `dataMap`, in the worst
   case where the node is full with all terminal values, 64 slots are still needed, unless the array can be extended
   dynamically, which is not mentioned at all in the paper.
   Cache locality is improved, though, since useful values are likely closer to each other in a sparse node.

This paper introduces Compressed Hash-Array Mapped Prefix-Trees (CHAMP), which is an improvement over the existing Hash-Array 
Mapped Tries (HAMT). The paper identifies four problems with a naive HAMT. The first problem is that tries (radix trees)
consume too much memory by maintaining a full sized node when most of the slots are empty. This both causes, excessive 
memory to be allocated for storing non-meaningful NULL values, and hurts cache performance, since the memory footprint 
of nodes become larger. The chance that a node access will hit the hardware cached copy decreases compared with a 
more compact representation.
The second problem is deletion. The original HAMT design lacks a proper deletion algorithm, such that the radix tree cannot
be restored to the canonical shape, resulting in sparse nodes and singleton paths, wasting both memory and cycle.
The third problem is bad cache locality of iteration, due to the fact that child node pointers and terminal values can be 
stored in an interleaved manner in a direct mapped node. The iterator will have to traverse to childen nodes recursively 
before returning to the current node and consinuing iterating on the current node, resulting in poor cache locality.


