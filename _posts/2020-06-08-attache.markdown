---
layout: paper-summary
title:  "Attache: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads"
date:   2020-06-07 19:55:00 -0500
categories: paper
paper_title: "Attache: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads"
paper_link: https://ieeexplore.ieee.org/document/8574551/
paper_keyword: Attache; Memory Compression
paper_year: MICRO 2018
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Attache, a main memory compression framework for reducing bandwidth using special signatures.
Main memory compression is proposed in prior publications to serve two main purposes. First, memory compression
reduces bus bandwidth consumption by transferring compressed blocks, which are variably sized and are smaller than
uncompressed block, instead of fix sized cache blocks. Second, more blocks can fit into the same amount of physical
storage, if compressed blocks are stored in a way that the extra space can be collected and taken advantage of.
Conventional main memory scheme faces two major challenges. First, the compressed block needs to be relocated in the 
main memory in order for the space advantage to be leveraged. The memory controller should maintain a mapping table
for locating compressed lines given their physical addresses. The second challenge is that compressed lines need to
identify themselves such that the decompression engine could recognize the compression algorithm and process the 
line properly.

Attache focus on solving the bandwidth reduction problem without increasing effective memory size, and therefore the address
mapping scheme is a rather simple one. Still, compressed lines should identify themselves to the uncompression engine,
since some lines are simply stored uncompressed, while others have different compressibility. 
Conventional memory compression schemes often use a metadata cache to enable fast retrieval of metadata for recently 
accessed blocks. Metadata caches, however, have several problems.
First, the cache cannot be very large, due the physical and budget constraint of SRAM caches. If the size of the working
set exceeds cache size, then each memory access will incur two DRAM references: one for the metadata of the requested
address, and the other for the actual data. The benefit of bandwidth reduction diminishes, as the number of DRAM operations
is doubled. Even worse, in such a scenario, it is likely that the application is bandwidth hungry. Bandwidth reduction
is the most critical feature in this case, but unfortunately the benefits are only marginal.
