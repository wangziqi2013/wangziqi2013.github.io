---
layout: paper-summary
title:  "Attache: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads"
date:   2020-06-07 19:55:00 -0500
categories: paper
paper_title: "Attache: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads"
paper_link: https://ieeexplore.ieee.org/document/8574551/
paper_keyword: Attache; Memory Compression
paper_year: MICRO 2018
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Attache, a main memory compression framework for reducing bandwidth using special signatures.
Main memory compression is proposed in prior publications to serve two main purposes. First, memory compression
reduces bus bandwidth consumption by transferring compressed blocks, which are variably sized and are smaller than
uncompressed block, instead of fix sized cache blocks. Second, more blocks can fit into the same amount of physical
storage, if compressed blocks are stored in a way that the extra space can be collected and taken advantage of.
Conventional main memory scheme faces two major challenges. First, the compressed block needs to be relocated in the 
main memory in order for the space advantage to be leveraged. The memory controller should maintain a mapping table
for locating compressed lines given their physical addresses. The second challenge is that compressed lines need to
identify themselves such that the decompression engine could recognize the compression algorithm and process the 
line properly.

Attache focus on solving the bandwidth reduction problem without increasing effective memory size, and therefore the address
mapping scheme is a rather simple one. Still, compressed lines should identify themselves to the uncompression engine,
since some lines are simply stored uncompressed, while others have different compressibility. 
Conventional memory compression schemes often use a metadata cache to enable fast retrieval of metadata for recently 
accessed blocks. Metadata caches, however, have several problems.
First, the cache cannot be very large, due the physical and budget constraint of SRAM caches. If the size of the working
set exceeds cache size, then each memory access will incur two DRAM references: one for the metadata of the requested
address, and the other for the actual data. The benefit of bandwidth reduction diminishes, as the number of DRAM operations
is doubled. Even worse, in such a scenario, it is likely that the application is bandwidth hungry. Bandwidth reduction
is the most critical feature in this case, but unfortunately the benefits are only marginal.
The second reason is that a cache will incur load and eviction traffic when a miss occurs. These traffic will further 
consume bandwidth on the memory bus, since the final destination of metadata is DRAM.
Lastly, with a metadata cache, metadata access and DRAM data access are serialized, since the controller does not know 
the exact location and/or the compression scheme of the requested address. The latency of DRAM accesses is therefore longer,
which is also on the critical path of DRAM accesses.

Attache optimizes metadata usage in main memory compression schemes by getting rid of a dedicated metadata area and 
storing a special marker and a bit indicating compression status at the beginning of a compressed DRAM block. 
The memory controller will check for the marker after a block is read. If the mark is present, and the compression status
bit indicates that the line is indeed compressed, the line is sent to the decompression engine for further 
processing. Otherwise, if the marker is not found, or if the bit indicates that the line is not compressed, the block
is considered as uncompressed, and will be returned directly.
