---
layout: paper-summary
title:  "Efficient Persist Barriers for Multicores"
date:   2019-03-24 21:00:00 -0500
categories: paper
paper_title: "Efficient Persist Barriers for Multicores"
paper_link: http://homepages.inf.ed.ac.uk/vnagaraj/papers/micro2015.pdf
paper_keyword: NVM; persist barrier; persistency model
paper_year: MICRO 2015
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper proposes a new implementation of persist barrier, a common programming language construct for NVM programming. 
Prior works have proposed four persistency models of different semantics and constraints. The first and the most straightfirward
persitency model is strict persistency, in which the order that memory operations persist must follow the order that they
become visible. In other words, the persistency model follows the consistency model. To achieve this, processors must
not make store operations visible to other processors via coherence before these operations are persisted to the NVM, which 
essentially make the cache write-through. This model suffers from performance issues for three reasons. The first reason is that
writes must propagate through the entire cache hierarchy which usually consists of several levels, since the processor 
bypasses the cache hierarchy and directly writes into the NVM. The second reason is that NVM writes are typically much slower
than a local cache write. The latency of store operations is longer even compared with uncached memory writes. The last reason
is that since the pipeline must not reorder persistent stores with other stores, there is little chance that NVM writes 
can be coalesced or batched in order to take advantage of the internal scheduling of the memory controller. The second model is 
epoch persistency, which relaxes strict persistency by allowing store operations to be persisted in a different order than
the one they become visible. A special memory ordering primitive called the epoch barriers are inserted into the instruction 
stream. It is required that store operations before the barrier must be persisted before any store operations after the 
barrier. The easiest implementation of epoch persistency is to track dirty cache lines accessed within an epoch. At the end 
of the epoch, these dirty cache lines are forced to be written back to the NVM using special instructions such as clwb.
Regular store barriers may also need to be inserted to enforce correct ordering between epoches. The processor in the meantime
must stall and wait for the persistence to complete before continue executing the next barrier. Epoch persistency overcomes
some of the undesirable properties of strict persistency, such as long latency on the store critical path. It is, however,
still inefficient, since the processor remains idle while an epoch has finished and is being persisted. The third type 
of persistency model is buffered epoch persistency (BEP), which is essentially the same as epoch persistency, except that 
processors can continue executing the next epoch while the previous epoch is being written back to the NVM. BEP completely
decouples memory persistence from write visibility: A memory operation can become visible to other processors arbitrarily
earlier than the operation becomes persistent. In non-buffered epoch persistency model, this is impossible, because a memory
operation is guaranteed to be persisted when the processor executes the next epoch barrier. Although epoches are not immediately
persisted when the corresponding epcoch barriers are executed, the correct ordering must be maintained. This paper identifies two
sources of epoch orderings, which will be discussed later. The last type of persistence model is buffered strict persistency (BSP).
BSP is different from the above all persistency models in that it requires atomic execution of epoches. Executions are divided
into epoches, which is the atomic unit for memory instructions to become both visible and persistent. Within an epoch, no 
coherence message is sent on writes, implying that their visibility as well as persistence are postponed to the end of the 
epoch.

This paper focuses on providing an efficient hardware implementation of buffered epoch persistency. Traditionally, BEP
is implemented using the lazy scheme: Dirty cache lines are not evicted from the cache when an epoch completes. These 
dirty lines only leave the LLC for two reasons: Either the LLC chooses the line for eviction as per the eviction policy,
or the line observes an epoch conflict and must be forced back to NVM. The paper further identifies two cases for epoch
conflicts: Intra-thread conflict and Inter-thread conflict. Intra-thread conflicts occur when a later epoch in a thread
writes to a dirty cache line generated by a previous epoch for the first time. If epoch B overwrites epoch A's dirty data
which has not been persisted yet, the store instruction cannot proceed before all cache lines in epoch A are persisted.
To see the reason, assume that the store operation in epoch B overwrites the line, and then the system crashes before epoch
A is fully persisted. In the post-crash image, a store operation from epoch B can be observed, which violates the ordering
constraint posed by the persist barrier: Stores from epoch B must not be persisted until all cache lines in epoch A have
been persisted. Similarly, an inter-thread conflict is observed if an epoch B on thread Y reads from or writes to a
dirty cache line generated by epoch A on thread X. The semantics of persist barrier require that epoch B must be persisted
after epoch A. Imagine the case where epoch B persists before epoch A and the system crashes before A is persisted. In 
the post-crash image, epoch B contains a value generated or derived from epoch A, but the corresponding line in epoch A
has not been persisted, and is lost during the crash. This outcome is inconsistent with normal execution, because epoch
B contains a value from nowhere.

In order to enforce epoch dependencies caused by inter- or intra-thread conflicts, traditional implementations stall
the destination processor when such dependency is about to form, and initiate an epoch flush on the source processor.
This adds a significant number of cycles to the critical path of the destination processor if the source epoch has a large
working set. This paper addresses the problem using a tachnique called proactive flush: Instead of flushing epoches 
only on-demand, the epoch flushing process is initiated as soon as an epoch completes. To support this, every cache line
in the hierarchy is extended with 