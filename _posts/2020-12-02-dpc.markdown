---
layout: paper-summary
title:  "Designing Hybrid DRAM/PCM Main Memory Systems Utilizing Dual-Phase Compression"
date:   2020-12-02 11:36:00 -0500
categories: paper
paper_title: "Designing Hybrid DRAM/PCM Main Memory Systems Utilizing Dual-Phase Compression"
paper_link: https://dl.acm.org/doi/10.1145/2658989
paper_keyword: Compression; NVM; DRAM Cache
paper_year: ACM Transactions 2014
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Dual-Phase Compression (DPC), a novel NVM-based memory architecture featuring memory compression
with lower access latency and better NVM lifetime.
The paper points out that previous proposals using memory compression face a few issues. 
First, these proposals usually radically modify existing cache hierarchy and memory hierarchy with specialized hardware,
which brings compatibility and hardware cost challenges. 
Second, memory compression increases access latency, sometimes even significantly, to the main memory, which can become
problematic when the compression algorithm is a general-purpose one. 
Third, most previous proposals only optimize for either bandwidth or endurance of NVM device, but not both. 
Lastly, these proposals may also introduce undisirable components, such as mapping tables for address translation,
which can also potentially degrade performance, and complicate the design.

DPC addresses the above issues with the following design features. 
First, DPC is purely a modularized design, with the only component addition being the DRAM controller and NVM 
controller, which is transparent to the processor and cache hierarchy.
In addition, it relies on general-purpose NVM architecture, without any specialized hardware.
Second, DPC adopts the so-called "Dual-Phase Compression", which uses two different compression algorithms for 
data of different characteristics. For frequently accessed data, DPC assumes they will be buffered by an L4 DRAM
cache, and only uses a fast, lightweight compression algorithm, such that decompression latency is negligible
compared with access latency.
In addition, the algorithm itself is designed such that critical words can be delivered first to the processor
pipeline, further reducing the length of the critical path.
For data that is rarely accessed, they are further compressed using a second-stage algorithm, which results in
both higher compression ratio, hence better bandwidth saving, and higher access delay. The increased access delay, 
however, is not a concern either, since NVM accesses also takes more cycles.
Third, DPC combines compression with both local and global wear-leveling techniques to reduce repeated writes 
on the same NVM cells, achiving bandwidth saving and longer NVM lifetime at the same time.
Lastly, DPC does not introduce extra levels of indirection, notably mapping tables, on the access critical path.
Compression is performed not to reduce NVM storage, but to increase NVM endurance.
This way, it is sufficient to delegate address remapping to the OS using the existing virtual memory system.

DPC assumes a system architecture as follows. The system is assumed to be equipped with both NVM as the main memory
device, and a L4 DRAM cache for fast access and less NVM write traffic. The DRAM cache might be implemented with
on-chip DRAM modules for even better performance. The paper, however, also noted that the design works equally well
for systems without a DRAM buffer with only minor changes.
