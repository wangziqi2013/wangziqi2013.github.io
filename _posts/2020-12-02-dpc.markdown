---
layout: paper-summary
title:  "Designing Hybrid DRAM/PCM Main Memory Systems Utilizing Dual-Phase Compression"
date:   2020-12-02 11:36:00 -0500
categories: paper
paper_title: "Designing Hybrid DRAM/PCM Main Memory Systems Utilizing Dual-Phase Compression"
paper_link: https://dl.acm.org/doi/10.1145/2658989
paper_keyword: Compression; NVM; DRAM Cache
paper_year: ACM Transactions 2014
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Lowlight:**

1. What I cannot get is that, if smaller lines are favored for eviction, then multiple lines will be potentially
   evicted for an insertion, which affects effective capacity, as claimed by the paper. This, however, also does 
   no good to the NVM, since more data will be written back also. Why the paper claims that this is good for NVM?

This paper proposes Dual-Phase Compression (DPC), a novel NVM-based memory architecture featuring memory compression
with lower access latency and better NVM lifetime.
The paper points out that previous proposals using memory compression face a few issues. 
First, these proposals usually radically modify existing cache hierarchy and memory hierarchy with specialized hardware,
which brings compatibility and hardware cost challenges. 
Second, memory compression increases access latency, sometimes even significantly, to the main memory, which can become
problematic when the compression algorithm is a general-purpose one. 
Third, most previous proposals only optimize for either bandwidth or endurance of NVM device, but not both. 
Lastly, these proposals may also introduce undisirable components, such as mapping tables for address translation,
which can also potentially degrade performance, and complicate the design.

DPC addresses the above issues with the following design features. 
First, DPC is purely a modularized design, with the only component addition being the DRAM controller and NVM 
controller, which is transparent to the processor and cache hierarchy.
In addition, it relies on general-purpose NVM architecture, without any specialized hardware.
Second, DPC adopts the so-called "Dual-Phase Compression", which uses two different compression algorithms for 
data of different characteristics. For frequently accessed data, DPC assumes they will be buffered by an L4 DRAM
cache, and only uses a fast, lightweight compression algorithm, such that decompression latency is negligible
compared with access latency.
In addition, the algorithm itself is designed such that critical words can be delivered first to the processor
pipeline, further reducing the length of the critical path.
For data that is rarely accessed, they are further compressed using a second-stage algorithm, which results in
both higher compression ratio, hence better bandwidth saving, and higher access delay. The increased access delay, 
however, is not a concern either, since NVM accesses also takes more cycles.
Third, DPC combines compression with both local and global wear-leveling techniques to reduce repeated writes 
on the same NVM cells, achiving bandwidth saving and longer NVM lifetime at the same time.
Lastly, DPC does not introduce extra levels of indirection, notably mapping tables, on the access critical path.
Compression is performed not to reduce NVM storage, but to increase NVM endurance.
This way, it is sufficient to delegate address remapping to the OS using the existing virtual memory system.

DPC assumes a system architecture as follows. The system is assumed to be equipped with both NVM as the main memory
device, and a L4 DRAM cache for fast access and less NVM write traffic. The DRAM cache might be implemented with
on-chip DRAM modules for even better performance. The paper, however, also noted that the design works equally well
for systems without a DRAM buffer with only minor changes.
Frequently accessed cache lines are brought into the DRAM cache.
Any DRAM cache design would work with DPC, but the paper assumes a simple design where DRAM rows are used as sets,
and all tags are stored in the same row as data.
The DRAM cache can store up to 4 uncompressed lines per row.

We next describe the first-stage compression algorithm as follows. First-stage compression is performed when a cache
line is evicted from the LLC to the L4 DRAM cache. This algorithm, as discussed above, should have low decompression
latency, and support critical word delivery. The algorithm uses simple word-level matching between adjacent 32-bit 
words. During compression, a word on offset i is compared with word on offset (i - 1). If their values match, a mask
bit "0" is output, and the word on offset i is removed from the output stream. Otherwise, a "1" bit appears in the bit
mask, and word on offset i is copied to the output stream.
The output of the algorithm consists of a 16-bit mask (where the first bit is always "1" since that word does not need
comparison and is always stored), and 1 to 15 words. 
If the cache line after compression is larger than or equal to the original line, the uncompressed line is used.
Decompression is just the reverse of compression. In order to decompress the word on offset i, the decompressor 
simply computes the prefix sum of the first i bits in the mask, and use that as the offset to access compressed
words. This way, the critical word accessed by the load operation can be decompressed first, and bypass the cache
hierarchy to the LSU. The rest is performed in the background, which will then be inserted into the LLC.

The L4 DRAM cache benefits from compression by storing more lines compactly in each set, which results in potentially
larger effective size. The uncompressed DRAM cache design includes four logical lines per DRAM row, with four tags and
the corresponding status bits. On each access, the DRAM controller reads the entire row in one access into the row 
buffer, after which the tags are checked.
In the compressed design, however, since more logical lines are potentially stored, and the size of each line is 
not static, the paper proposes that: (1) The number of tags is extended to 16, enabling a maximum compression ratio
of 4; (2) Compressed lines stored in the data region is no longer statically bound to tags. Instead, the data region
is divided into 16-bit segments. A compressed line can start from any segment boundary, and each tag also contains a 
segment pointer and a size field for locating compressed data.
Compressed lines are always stored compactly in the DRAM cache, which implies that if fragmentation occurs after a
write or eviction operation, the DRAM controller should also compact the row and rearrange valid lines, before the 
row is closed.

The paper also proposes eviction algorithms for the DRAM buffer. Due to the fact that cache lines of variably sized,
the classical LRU may not work the best for a compressed cache. The paper, for example, discusses two extremes.
In the first extreme, larger lines (uncompressed lines) is preferred over smaller lines. This favors effective
size of the cache, since more lines can potentially fit into the empty slot after eviction, at the cost of extra 
write traffic on the NVM, since evicted lines will be written back to the NVM.
In the second extreme, smaller lines are favored more over larger lines. In this case, write traffic to the NVM can
be reduced, but effective cache size might be impaired, since more than one smaller lines might be evicted.

As a balance, the paper proposes two-stage eviction. In the first stage, cache lines are selected based on sizes.
Depending on the characteristics of the workload, this stage may favor uncompressed lines (optmize for NVM write 
traffic), or cache capacity (optimize for read-dominant workloads).


