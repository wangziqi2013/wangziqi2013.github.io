---
layout: paper-summary
title:  "PMEM-Spec: Persistent Memory Speculation"
date:   2021-05-16 06:24:00 -0500
categories: paper
paper_title: "PMEM-Spec: Persistent Memory Speculation"
paper_link: https://dl.acm.org/doi/abs/10.1145/3445814.3446698
paper_keyword: NVM; PMEM-Spec; Speculation
paper_year: ASPLOS 2021
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes PMEM-Spec, a hardware mechanism for enforcing strict write ordering between the memory consistency
model and NVM device. 
The paper points out that write ordering is critical for NVM-related programming, since the order of memory operations
performed on the hierarchy often differs from the order that NVM device sees them. This is caused by the fact that the
cache hierarchy can arbitrarily evict blocks back to the NVM, while programmers have only very limited control over the
lifespan of blocks in the hierarchy.
Current x86 platform provides two primitives, namely clflush for flushing a dirty block back to the backing memory,
and sfence for ordering cache flushes. A software persist barrier is formed by using multiple clflush followed by 
an sfence in order to establish write orderings for write operations before and after the barrier.

Software barrier, however, is far from optimal, since it is implemented on software, and therefore, stalls the 
processor with fence instructions while previous stores are being persisted. 
This also restricts parallelism, since a thread can have at most one outstanding barriers, limiting hardware
level parallelism of the NVM device that can be taken advantage of.

Previous hardware proposals, on the other hand, solve this issue by allowing multiple "strands" of persist barriers
to be active for a single thread at the same time, or decouples memory consistency (i.e., visibility of a modification
from the perspective of other threads) from NVM consistency (i.e., when the NVM device sees a dirty block) such that 
persistency is moved to the background without stalling the core pipeline.
These proposals optimize towards efficiency, but the paper points out that they often bring excessively complicated
hardware, and are intrusive to the existing hierarchy such that the coherence protocol is modified.
Besides, the paper also points out that more complicated persistence models need programmers to annotate the
source code, which requires programmers to learn these models first as well as to understand the subtlety of 
different models.

PMEM-Spec adopts a different approach by not proactively enforcing memory consistency and NVM consistency, but 
optimistically assuming that these two will be equivalent in the majority of cases. In other words, it speculates
under the assumption that these two orderings are always identical. In the rare case where they are not, the hardware
detects a violation, and interrupts the offending process of the ordering issue. The process then rolls back 
the current transaction or FASE, and restarts. 

We next introduce the details of the design as follows. 
PMEM-spec adds one more data path from the processor to the NVM device. On conventional platform, cache blocks are
brought into the L1 cache on write misses, which are then updated, and then evicted via either natural evictions,
or clflush instructions. 
PMEM-spec, on the contrary, demands that any write operation performed at L1 level must be sent to the NVM
controller immediately, without using the ordinary data path. This can be done by sending write data to the 
controller via the on-chip network when write instructions retires from the store buffer (i.e., when it is 
drained into the L1 cache). 
Dirty evictions from the LLC will then be ignored by the NVM controller (actually, only data will be discarded,
and the eviction message itself serves an important purpose and must be sent by the LLC).
There are two important things to note in this process. First, this essentially turns the hierarchy write-through
since all writes are directly sent to the NVM controller. The dirty bit, on the other hand, must still be preserved,
which is used for monitoring the lifespan of dirty blocks, as we will see later.
Second, although stores are drained from the store buffer to the L1 cache and to NVM controller at the same time,
this process is still not atomic, i.e., events from other cores may observe the short window of inconsistency when
the updates have been applied to the L1, but not to the controller. 
This opens the possibility of mis-speculation where the memory consistency differs from NVM consistency, which is the 
topic in the following paragraphs.

The first type of consistency problem occurs when a load from another processor misses the hierarchy and accesses NVM.
If the load happens after the store is issued to L1, but before the corresponding modification is received by the 
NVM controller, the load operation will read a stale value from the NVM device, while the most up-to-date value
is still on the way being sent to the NVM controller. 
Although this is extremely unlikely, since the block must be evicted by the entire hierarchy before the direct write
arrives at the NVM controller, the chances is still not zero.
In this case, the load operation from another core accesses a stale version of data, since according to memory
consistency, the load happens after the store, but the NVM access ordering disagrees.
