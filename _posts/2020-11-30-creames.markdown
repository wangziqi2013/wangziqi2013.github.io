---
layout: paper-summary
title:  "On-Line Memory Compression for Embedded Systems"
date:   2020-11-30 21:27:00 -0500
categories: paper
paper_title: "On-Line Memory Compression for Embedded Systems"
paper_link: https://dl.acm.org/doi/10.1145/1698772.1698785
paper_keyword: Compression; DRAM Compression; CREAMES; Embedded compression
paper_year: ACM Transactions 2010
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes CREAMES, a main memory compression solution for embedded systems where memory is a scarce
resource, while no disk is present as swap space. 
This paper points out that conventional solutions for main memory compression will not work on embedded platform,
since these prior solutions usually assume disks as the backing store of the virtual memory system.
The paper also lists five challenges for a main memory compression solution on embedded platforms. 
First, compressed and uncompressed memory region must be carefully maintained, such that the application will not
be terminted due to lack of usable memory. There is a trade-off: On one hand, if too much storage is dedicated
to storing compressed pages, application's memory allocation requests might fail, while the compression region still
has free space. On the other hand, if the compression region is too small, the overall compression ratio would be 
insignificant, limiting the effectiveness of compression.
Second, the compression algorithm must achieve a balance between compression ratio, compression latency, and energy 
compsution. Although this is the general requirement for nearly all compression solutions, it is particularly important
on embedded platforms, due to limited processing power and battery size.
Third, the memory management module for compressed pages should utilize compressed memory region efficiently to reduce
fragmentation, since compressed pages are variable sized. These pages should be stored as compactly as possible, while
minimizing relocations when the compressed size changes.
Fourth, the compressed memory region should be dynamically adjusted to fit dynamic program behavior, as different
applications demonstrate different memory footprints.
Lastly, the memory overhead for supporting compression should be minimized, since memory is a scarce resource on 
embedded platform. 

The paper assumes an embedded platform with virtual memory, but no disk for swapping out pages when physical memory
is over-committed. The main memory size is tens of MBs, in which some of them is dedicated to storing program code 
and the firmware image, which are copied from the embedded to embedded flash chips to the main memory at startup time. 
Although this paper only discusses data compression, meaning that only working data in the main memory is considered
for compression, code compression can also be applied to the flash image, which is orthogonal to CREAMES, and can
be applied individually.
The Operating System of the embedded platform handles page faults when a non-existing page is accessed by applications.
In normal use cases, this will raises an insufficient memory error, resulting in the termination of the application.
In a compressed platform, the page fault will be handled by the memory compression routine, as we discuss later.

CREAMES works as follows. The main memory is divided into two partitions, one for storing working data, which are mapped
into application's address space, and can be accessed directly by program code. The other partition is dedicated to
storing compressed pages, which are not accessed directly, and must be retrieved by the decompression routine on a 
page fault.
Compression and decompression are driven by page faults. The OS will over-commit the main memory by allowing more 
physical pages than the size of the first partition to be allocated to applications, and mark some of them as "not 
present", which will trigger page faults when they are accessed. For example, the OS can choose to maintain all 
available physical pages in the first partition as a linked list. When the application requests a new page, the OS will 
select one from the list, and populate the page table using the physical address of the selected page. If, however,
the linked list is currently empty, the OS must swap out an existing physical page (which is maintained by another
list structure, possibly with LRU or other access information maintained) to the compressed region, and then 
populate the page table entry for the requested address using the physical address of the swapped out page frame.
The swapped out page is then compressed, in the background, by an OS thread, and then stored in the compressed 
page partition. The page table entry of the virtual address whose physical page was just swapped out is also marked 
as "not present", such that when the page is accessed later, the page fault handler will search the compressed area, 
decompress the compressed page, and load it back to the main memory. During this process, another page might be 
selected by the OS for swapping, and compressed before stored into the compressed page partition.

The compressed page partition is maintained differently than working data, since compressed pages are of different 
sizes, and cannot be mapped to physical frames using one-to-one correspondence. To maximize the efficiency of the 
compressed page partition, the paper suggests that a dedicated allocator be used to manage storage for the variably
sized compressed pages. Compressed pages are searched using a 256KB, direct-mapped partition header. The header
maps the identifier of the swapped out page (virtual address + process ID) to the physical address of the compressed
page, which allows the page to be retrieved when a page fault is raised on the virtual address where the page used to
be mapped.
