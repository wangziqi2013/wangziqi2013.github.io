---
layout: paper-summary
title:  "Finding and Fixing Performance Pathologies in Persistent Memory Software Stacks"
date:   2019-10-25 17:19:00 -0500
categories: paper
paper_title: "Finding and Fixing Performance Pathologies in Persistent Memory Software Stacks"
paper_link: https://dl.acm.org/citation.cfm?id=3304077
paper_keyword: NVM; File System
paper_year: ASPLOS 2019
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper evaluates the performane of NVM on several disk-based and NVM-based storage systems. The emergence of NVM
raises the new chanllenge of programming for fine-grained persistence devices that can be directly accessed by processors.
On the other hand, existing software stacks, such as file systems, are mostly designed for block persistent devices 
such as magnetic disks or SSD. These designs provide an entirely different set of interfaces from what we are failiar with
for in-memory applications in order to interact with the device, and/or manage data/metadata in a way that may not perform
very well for newer hardware. Both of these make it a challenging task to migrate them from the old platforms to newer 
NVM-based platform.

One of the most prominent differences between disks and NVM is that NVM can be directly accessed by loads and stores
issued by user application, while disks can only be accessed via a predefined set of OS interfaces. A direct transformation
from existing applications to NVM applications without changing the interface (i.e. treat NVM as a block device by only 
accessing it in block granularity) may imply a potential performance problem, since the overhead of software stack, the 
read and write characteristics of NVM, and extra consistency requirements for NVM can all easily become a performance
bottleneck. In the next paragraphs we discuss these problems pinpointed by the paper.

The first observation is that metadata changes are costly even on NVM file systems, and we should avoid doing so as 
much as possible. The paper uses SQList as an example. SQLite supports four different modes of logging, including both
redo and undo logging. In the undo logging scheme, a logging file is generated for each transaction, keeping track of
the before-image of modified data items during the transaction to endure atomicity. This logging file is either truncated 
or deleted, or marked as invalid by an extra persistent write at the end of the transaction. The redo logging, on the other
hand, maintains a redo log across transactions, the content of which is replayed onto the database image after a crash. 
Experiments show that undo logging generally has inferior performance compared with redo logging, because of the file
manipulation at the end of every transactions. Such file operations (delete or truncate) will very likely involve changing 
multiple metadata pages, which will then be synchronized onto the NVM to guarantee persistence. On the contrary, redo logging
does not require costly file operations at the end of the transaction, and hence can sustain a higher throughput.
