---
layout: paper-summary
title:  "Better I/O Through Byte-Addressable, Persistent Memory"
date:   2021-01-04 17:11:00 -0500
categories: paper
paper_title: "Better I/O Through Byte-Addressable, Persistent Memory"
paper_link: https://dl.acm.org/doi/10.1145/1629575.1629589
paper_keyword: NVM; BPFS; File System
paper_year: ISCA 2009
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper introduces BPFS, a file system built on top of byte-addressable NVM. The paper recognizes that NVM is a 
promising storage medium for future file storage platforms due to its byte-addressability and low latency accesses. 
Compared to a conventional file system, BPFS has several advantages. First, its low read and write latency makes BPFS 
signifantly faster than conventional file systems. Second, even compared with block-based file system ported to NVM, 
BPFS still outperforms them using a unique form of shadow paging that reduces write amplification and data copy.
Third, the byte-addressability of NVM makes small writes and certain meatdata operations fast, since only a few writes
are sufficient instead of copying and writing back entire blocks. Lastly, although not mentioned by the paper, BPFS
is capable of operating without an OS intermediate buffering level on the critical path, since the address space of the 
NVM can be directly mapped to the protected virtual addresses of the file system driver. Most notably, no data movement
between NVM and DRAM is required and hence DRAM buffer is only optional, unlike previous designs.
Besides all these, BPFS also provides a stronger semantics guarantee without incurring any overhead other than those
already exist in shadow paging.

BPFS is implemented as a block-based file system on top of NVM using shadow paging as one of the the main updating
algorithm. BPFS is compatible with the conventional file system interface, and provides a rather strong semantics
guarantee: All operations should complete (almost) atomically in the order that they are issued by the application. 
Here "atomicity" refers to failure atomicity of data and metadata, meaning that an update operation on either or both
will either be fully committed, or rolled back, after a crash, leaving the system itself as well as data always in a
consistent state. 
Compared with conventional file systems, which usually only guarantees the consistency of metadata via journaling,
but not data, in order to reduce write amplification of writing the same data twice, BPFS's consistency model is
a rather strong one since both metadata and data are protected from crashes.

BPFS is implemented as a collection of B+Tree trees, which represent the inode file, directory files, and user files. 
Just like regular B+Trees, each file in BPFS is stored as an on-NVM tree with inner nodes storing pointers to next-level
nodes, and leaf nodes stroing pointers to data blocks. One particular optimization of the BPFS B+Tree is that some
subtrees can be trimmed if the subtree contains nothing else but just NULL pointers at the leaf level to save
some storage and reduce tree traversal latency. 
The tree traversal function, called "crawlers", will return immediately once this is encountered, as if a NULL pointer 
is found at leaf level. Note that subtree trimming is only a physical level optimization that does not affect the 
logical content of the tree. The size of the file represented by that tree is independent from how the tree structure
is, and the crawler functions always treat missing levels or nodes as if they store NULL pointers at leaf.

Files in BPFS are referred to using pointers to the root address of the B+Tree. In addition, the height of the tree is
encoded into lower bits of the pointer (tree pointers are 4KB aligned, so there should be 12 redundant bits which is
more than sufficient). Encoding tree heights, although unnecessary at the first glance, helps crawler functions to
identify trimmed subtrees, in which case NULL pointers will be observed at a non-leaf level.
The height field and tree pointer can be updated atomically using a 64-bit write, in the case of creating new root 
nodes, such that the tree reference should always be consistent.

The entry point of BPFS is the inode file, which stores inode objects indexed by inode numbers. There is only one global
inode B+Tree in the file system, the root pointer of which is stored in a known location in the super block.
The inode serves the same purposes as in conventional file systems, storing metadata such as the file pointer, file
size, date, time, and permission bits. Inode zero is by default the root directory of the file system, and the starting
point of file system traversal.
Directory is represented as a special type of file, whose content is just an array of directory entries. Each entry 
stores the file name, inode number, entry type, etc. 
Directory files and user files are also represented with B+Trees.

File system updates are performed using two techniques. The first technique relies on the hardware guarantee that 
64-bit writes are always atomic with regard to failures. If the update operation only causes a single 64-bit field to
be written, then no special construct is needed, and BPFS will directly write the field and flush it to the NVM for
persistence. One of the most typical use cases is when application appends data to the file, and the append operation 
allocates a new data page. 
In this case, BPFS first allocates a data page from the free page pool, populates the page with data, and then installs
the page to the B+Tree leaf node by simply overwriting the leaf pointer with the address of the data page.
Metadata updates, such as access date/time, can also be performed with this technique.

The second update technique is called short-circuit shadow paging, which involves copying the page to be updated, 
performing the actual updates on the copied page, and then updating the reference of the old page to the new page in 
the parent. Note that in a conventional file system, this update will cause an so-called "avalanche effect", 
since the update to the parent page itself requires the duplication of the parent page, which recursively propagates 
to the root of the tree, causing huge write amplification. 
Recall that the entire structure of BPFS is a B+Tree collection, this issue can only become worse, since 
In BPFS, since pointer updates are always atomic, the paper figures out that the recursion can stop as long as the
parent page update can be conducted by a single pointer update.

