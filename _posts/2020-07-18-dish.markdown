---
layout: paper-summary
title:  "Dictionary Sharing: An Efficient Cache Compression Scheme for Compressed Caches"
date:   2020-07-18 00:49:00 -0500
categories: paper
paper_title: "Dictionary Sharing: An Efficient Cache Compression Scheme for Compressed Caches"
paper_link: https://ieeexplore.ieee.org/document/7783704
paper_keyword: Compression; Cache Compression; DISH
paper_year: MICRO 2016
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Highlight:**

1. Using fixed length code words for compression, which results in a fixed compression ratio (4:1). This eliminates problems
   such as space allocation and compaction, since all lines are statically mapped to only one possible location in the 
   physical data slot.

**Lowlight:**

1. This paper is syntatically and grammaritically poor, and in general the delivery of ideas is also poor.
   
2. I don't get why adding another line for compression is that complicated (3 parallel comparisons for B1 and 4 parallel 
   comparisons for B2). Shouldn't we just load the current 
   dictionary, treat it as if we were half way done during compression, and compress the incoming line by either
   generating a code word for existing dictionary entries, or inserting a new entry?
   Why dividing the dictionary into "B0's dictionary" and "B1's dictionary"?

3. When a line is invalidated, how to invalidate dictionary entries that no longer contribute to the compression for the 
   rest of lines?
   One way is just to use brute-force and compute the reference count of each dictionary entry. Entries with ref count
   being zero should be removed after invalidating the line. Dictionary entries do not need to be compacted since 
   each entry has a valid/invalid bit.

This paper proposes Dictionary Sharing (DISH), a novel dictionary-based cache line compression scheme with high compression
ratio and low design cost. This paper identifies several issues with previous compressed cache proposals, such as DCC, 
SCC and YACC. First, to simplify storage management within a physical slot, these designs mandate that all cache lines
that share a physical slot must be from the same super block, and must be of the same size class after compression. 
Under such restriction, even if a physical slot still has storage for one compressed line, if the size class of the line
does not match the size class of the slot, the line cannot be stored, casuing space wastage. In the worse case, N tags
have to be allocated for lines in the same super block, if these blocks fall into N distinct size classes.
Second, when a dirty cache line is written back from the upper level, chances are that the compressed size of the new 
line no longer fits into its original physical slot. In this case, the old line's physical slot is invalidated, and 
a new slot is allocated by either evicting existing lines, or performing compaction of existing lines to free some 
storage that was unusable due to external fragmentation. This paper argues that compaction of existing lines is a heavy-weight
task, since it reads out all compressed lines from the dara array, re-aligns them, and writes the compacted block back.
Lastly, despite the fact that these previous schemes perform tagging in super block granularity, which consist of four or 
more regular cache lines, the compression itself is still performed on individual cache lines without any consideration 
of inter-block redundancy or block compaction. In fact, inter-block redundancy within a super block is a good starting 
point for further optimization, as we will see below.

DISH leverages two important observations on value locality. First, value locality not only exists in the same cache
line, but also across adjacent cache lines within the same super block. The number of distinct values is significantly
smaller than the number of words in a super block, potentially enabling a compression method that uses these values
as dictionary, and encodes individual words using dictionary index with shorter code words. 
The second type of value locality, called upper data bits locality, suggests that even if values across cache lines
may not be strictly identical, there are still large chances that the upper bits of these values are equal, while lower
bits being more random. This is common in arrays, especially pointer arrays of the same type of objects, since 
malloc() will most likely attempt to place these object in the same memory pool as close to each other as possible as 
spatial locality optimization. 
Other compression algorithms that perform delta encoding, such as BDI, also take advantage of this observation by subtracting 
a selected base from such values and encoding the delta with less number of bits. DISH, on the contrary, only saves the 
higher bits in the dictionary which could be indexed with shorter code words in the compressed data. The lower few bits 
are just stored as-is together with the index array, which will be concatenated with dictionary entries to recovery the 
original uncompressed value.

We describe the overall architecture of DISH before introducing its dictionary-based compression algorithm. From a high
level, DISH attempts to compress at four block super block level by extracting common 32-bit words (or partial words) to 
form the dictionary, and encoding 32-bit values in cache lines with shorter code words. In the best case, all four blocks
can be compressed into the same physical slot. One of the biggest difference between DISH and presiously proposed super 
block compression schemes, however, is that compressed cache lines are always of the same size, as a result of using 
fixed length dictionary indices as code words, essentially only attempting a 4:1 compression ratio. 
This fixed length code word scheme has two obvious advantages. First, cache lines are statically mapped to only one possible
offset in the data slot of the super block, since it is guaranteed that all four lines can be accommodated by a physical slot,
as long as they can be compressed successfully. No dedicated storage management hardware is therefore needed in the cache 
controller.
The second advantage is that no physical slot compaction is required when write back happens, since cache line sizes 
after compression is always 16 bytes, unless compression fails. In the case of a failure, the block that failed to be 
compressed must be invalidated in the current physical slot, and then allocated a new slot before compression is attempted.
