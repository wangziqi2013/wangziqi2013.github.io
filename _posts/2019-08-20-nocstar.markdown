---
layout: paper-summary
title:  "Scalable Distributed Last-Level TLBs Using Low-Latency Interconnects"
date:   2019-08-20 21:26:00 -0500
categories: paper
paper_title: "Scalable Distributed Last-Level TLBs Using Low-Latency Interconnects"
paper_link: https://ieeexplore.ieee.org/document/8574547
paper_keyword: TLB; NOCStar; NOC
paper_year: MICRO 2018
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper proposes Nocstar, a novel design for fast access of distributed L2 TLB. Traditionally, to perform address
translation, when L1 TLB misses, the page walker will be initiated to query the in-memory page table. Due to the high
latency of memory accesses and the long sequence of dependent operations during the page walk, an L2 TLB is further 
added to reduce the misses which result in costly page walks. 

The design of L2 TLB has become a problem, since the trade-off between access latency and capacity as for caches
also appy to L2 TLB (which is organized similarly to a cache). This paper presents three existing designs of L2 TLB: private, 
monolithic shared, and distributed shared. All three have problems that can prevent them from being practical in future
large systems. For private L2 TLB design, since each core can maintain a private copy of an entry in its own TLB,
there is a waste of storage, because entries are duplicated across many L2 TLBs. In addition, in the private design,
the capacity of each TLB is fixed at manufacturing time. In the run time, even if load imbalance occurs, which results in
the scenario where some TLBs are not large enough to hold the working set, while some other TLBs are not fully utilized 
at all. Such static allocation of resources may harm performance by limiting the maximum size of the working set at each core.
Shared monolithic L2 TLB, while solving the entry duplication problem and resource allocation problem, on ther other 
hand, suffers high access latency. The paper points that on Skylake architecture, the L2 TLB has 1536 entries, which 
takes 9 cycles to access. Given that address translation is performed on all memory instructions, this can be a disadvantage
especially for applications whose working sets are larger than the L1 TLB, offseting the higher hit rate of shared L2 TLB. 
The last design, distributed L2 TLB, divides the monolithic TLB into small slices. Each core is responsible for one slice, 
and addresses are mapped to slices using a hash function (not necessarily just using the middle bits in the address). If 
an address maps to a remote TLB slice, the processor relies on the on-chip communication network to send the request and 
receive the response. This design has the obvious advantage that accesses to local TLB slice is as fast as a private TLB, 
while also being scalable as the number of core increases. The problem, however, is that the distributed design incurs a 
network communication every time a remote access is required. This can further increase both the network traffic and 
access latency.

Nocstar, which stands for "NOCs for Scalable TLB Architecture", uses a dedicated, circuit-switching network infrastructure 
to access remote TLB slices. Nocstar has a different design philosophy from typical NOCs in which messages are transferred 
and relayed by intermediate nodes in the form of packets and filts. The design decision is made based on an important
observation made by the paper: In general, L2 TLB requests are latency sensitive due to the reasons we mentioned in the 
previous paragraph, and therefore, they must obey very strict timing constraints. On the other hand, the degree of 
concurrency in terms of number of requests waiting to be served when another request is sent to a slice, is low. Only a 
few percentage of requests are processed by the same slice when there are other requests outstanding. The above observation
implies a design that has both low latency for performance, and low bandwidth for economics. 