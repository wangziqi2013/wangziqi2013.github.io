---
layout: paper-summary
title:  "Failure-Atomic Persistent Memory Updates via JUSTDO Logging"
date:   2019-08-08 23:44:00 -0500
categories: paper
paper_title: "Failure-Atomic Persistent Memory Updates via JUSTDO Logging"
paper_link: https://dl.acm.org/citation.cfm?id=2872410
paper_keyword: Logging; JUSTDO; NVM
paper_year: ASPLOS 2016
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper proposes JUSTDO logging, a novel logging and recovery method for non-volatile memory. In ordinary logging schemes,
such as undo logging, redo logging, and shadow logging, the amount of data (i.e. log records and metadata) is proportional
to the number of store operations. One log entry is generated for each store operation, which contains the address, data,
and other control information in order to roll back or redo the affected region. These schemes generally have three problems. 
First, they enforce write ordering at the guanularity of every store or every transaction commit. In the former case,
a log entry is generated before the store is performed, and the entry must be fllu flushed to the NVM device before the 
store could proceed to guarantee recoverability. Although with new hardware primitives and probably new architectures, the 
overhead of persisting can be overlapped by smart scheduling (e.g. hardware level write ordering enforcement), it still poses
a major overhead of logging schemes that require per-store logging, e.g. undo logging. In the latter case, the log records are 
flushed to the NVM at transaction commit point, which incurs a burst of traffic to the network and NVM. Even worse, the 
chance of overlapping this burst of memory write backs with other useful work is slim, because transaction commit always 
happens as the last action a transaction will take, which puts the persistence of log records on the critical path. This 
happens with redo logging. The second problem is that extra metadata, either on-chip or off-chip (e.g. on memory controller)
might be used to track the state of stores. These metadata themselves also require persistence for correct crash recovery, 
which introduces extra traffic, storage and complexity. The last problem is that the recovery of all except shadow-mapping 
are data-centric, which means that the time complexity of recovery depends on the amount of data the current transaction
or critical section has generated. The availbility of the system might be affected because of slow recovery. For shadow-mapping,
only simple recovery is performed, because by nature, this scheme has a "write-once" property for non-volatile data, and 
only overwrites old data when it is safe to do so. The downside, however, is that the amount of working data will be multiplied
due to multiversioning. The system generally has to stall and perform garbage collection when storage runs out to reuse
older versions of data.

Justdo logging takes advantage of two important observations. First, most datarace-free applications use locks as the basic 
synchroniation primitive. Lock acquisition and release can be interpreted as granting and giving up permission of accessing
shared states. Applications often only observe inconsistent states of shared data within critical sections (i.e. holding 
at least one lock), and leave shared data in consistent states before leaving the critical section. The crash recovery,
therefore, could only focus on these (potentially nested) critical sections called "Failure-Atomic Sections (FASE)",
and recover the system back to a state that is equivalent to a time point in normal execution where no lock is held by
any thread. The second observation is that, although data-centric recovery may simply the algorithm, because the recovery handler 
just needs to iterate over all log records, and apply the data to the address indicated by the record, the same 
sequence of stores can actually be reproduced by running the same FASE with the same input (the paper fails to identify that
if the FASE itself is non-determinstic, the sequence could not be reconstructed, in which case re-executing with the 
same input will not help), which eliminates the need of value logging. As long as the recovery process can resume the 
inerrupted execution of a FASE with the same environment (e.g. local stack frames, arguments, etc.), it is guaranteed 
that the same output be generated, and the same state as if the original FASE were not interrupted by the crash can be reached.