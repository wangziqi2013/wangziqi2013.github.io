---
layout: paper-summary
title:  "System Software for Persistent Memory"
date:   2020-06-23 21:07:00 -0500
categories: paper
paper_title: "System Software for Persistent Memory"
paper_link: https://dl.acm.org/doi/10.1145/2592798.2592814
paper_keyword: NVM; PMFS
paper_year: EuroSys 2014
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper introduces PMFS, a file system designed and optimized for NVM. Unlike previous NVM file system papers in which
a concrete problem with other designs are identified and then solved, this paper is closer to a design document covering
the high-level designs while explaining design motivations. The paper begins by identifying three different ways of extending
existing file system paradigm to work with NVM. The first is to abandon file system totally, and shift the responsibility
of resource management to the OS's virtual memory manager. The second way is to only change the block layer interface and
use the NVM as conventional block device with lower latency and higher throughput. The last is to partially abandon the 
block layer abstraction and the disk buffer cache, reducing software stack and data movement overhead, but maintain the
conventional file system interface and semantics.

The paper chose the last for three reasons. First, legacy applications relying on file system interface still work on
PMFS with reduced latency and increased bandwidth, which eases software migration. Second, by getting rid of the software 
block layer, PMFS does not suffer from abstraction and data movement overhead, since data exchange happens directly
between the application and NVM storage. The last reason is that applications can also benefit from more powerful interface
such as mmap(), which directly maps a range of virtual addresses to the NVM physical address, enabling the application 
to directly read from and write into NVM.

The paper then proceeds to discuss three different techniques for forcing write ordering on the current architecture.
The first uses non-cachable pages, which are set via page table attribute bits. Loads and stores to these pages will
bypass the cache. The problem, however, is that non-cacheable accesses incurs performance overheads for all memory
operations, while only a subset of them needs to see a certain memory ordering. Besides, all memory operations will
occur on the system bus if caching is disabled, which has a limited bandwidth. Frequent accesses to the NVM device
not only saturates NVM bandwidth quickly, but also impacts performance of other unrelated applications.
The second option is non-temporal stores, which are special store instructions whose data is not expected to be accessed
in the near future, thus bypassing the cache hierarchy. Due to the extensive usage of non-temporal stores in streaming 
workloads, the processor is also equipped with a special write combining buffer, which tries to combine multiple smaller 
stores into a full cache line as much as possible to reduce write amplification of the store. Using non-temporal
stores, however, may imply unexpected results or complicated interactions with cached loads. The implementation
should then be very careful on placing memory barriers to avoid loading stale data or severe performance loss.
The last option is to use a persistence barrier consisting of cache line flush instructions and a store fence. The 
persistence barrier is compatible with most other instructions. PMFS chooses persistence barrier to enforce 
write ordering due to its simplicity and efficiency. In order to issue flush instructions, the paper assumes that software
should track dirty data in cache line granularity, and only flushes dirty data with a barrier. It is not discussed
how dirty data tracking can be implemented, though.
The paper also mentions hardware accelerated persistence such as epoch persistence. This approach, as pointed out
by the paper, requires extensive hardware modification, such as cache line tagging and customized eviction algorithms.
It is unlikely that future hardware with adopt this due to its complexity.

The paper then gives an overview of PMFS's architecture. Two special design considerations affect the design decisions
of PMFS. The first consideration is byte-addressability of NVM, which enables fine-grained logging at arbitrary byte
boundaries. PMFS is therefore optimized to use smaller granularities for logging. In addition, some operations are 
intrinsically atomic on the current architecture. Logging is not required for those operations.
The second consideration is based on the fact that the NVM device can be directly mapped to the virtual address space.
If not protected properly, a stray write within the kernel or device driver can permanently alter data or metadata on 
PMFS, leading to data corruption. The design of PMFS should therefore take address space protection into consideration,
reducing the possibility that unintended writes be made to the mapped address to a minimum.

The layout of PMFS is relatively simple. In the mapped address space, the first two chunks of storage are standard file 
system super blocks that store global metadata of the file system. A logging area that stores log entries for in-flight
metadata operations follows.
inodes are structured as B+Trees, as opposed to in
conventional file systems where they are organized as a fixed size table after the super block. B+Tree nodes are allocated
from the NVM as pages. inodes also use B+Trees to organize blocks. The root block offset is stored within the inode.
The rest of the storage are maintained by an allocator. The allocator manages the rest of the storage as pools of 
fixed sized pages. 4KB page is used for internal data structure and the metadata, while 4KB, 2MB and 1GB pages are allocated
for data pages. Although the details of the allocation algorithm is not discussed, the paper suggests that it is still
at an early stage of development, and only simple strategies, such as coalescing a freed page with neighboring free pages,
are used.
The allocator, as mentioned later in the paper, maintains its internal data structure in volatile memory,
which is only written back on an ordered shut down, and loaded on the next mount. 
On a system crash, the internal states will be lost. The recovery handler should scan all nodes in the file system to
rebuild the allocation map after all other recovery steps are taken.

When mmap() is called on a file, PMFS will decide whether the file is mapped with regular 4KB pages, or larger sized 
pages to reduce TLB and page fault overhead. Using large pages, on the other hand, can negatively impact performance
by introducing external fragmentation on the physical address space, and/or incuring extra write amplification when 
file data can potentially be modified by different processes via copy-on-write (note that this is different from the 
copy-on-write technique for ensuring atomic data updates, as we will see below).
The paper suggests that the page size is either given at mount time, or inferred from file system calls that reveal
possible future use pattern.

The paper then discusses techniques for performing atomic updates. Three common techniques are discussed: shadow paging,
undo logging, and redo logging. Shadow paging is assumed to be performed on page granularity. It features low write 
amplification when the majority of the page is updated, but suffers from high write amplification when only a few 
bytes are updated. Undo and redo logging, on the contrary, has a static write amplification of two. Undo logging
requires the pre-image to be persisted to the NVM before data items are updated, thus requiring one persistent barrier
per update. Redo logging, on the other hand, only requires two barriers, one between log write and commit record, the 
other between data update and truncate record. PMFS selects cache line granularity undo logging as the method for atomic 
updates of metadata, since metadata updates are often small and sparse on a page. In addition, redo logging requires read 
redirection if the log has not been replayed on its home address, but the dirty data is read by the same transaction. 
In this case, the read request must search the read log to avoid accessing the stale version on the home address. 
Data page updates are made atomic using shadow paging.

Three extra lightweight techniques are used to perform atomic updates in-place without logging or shadow paging. The first
is 64 byte store primitives, which are always atomic as long as they are not crossing cache line boundaries. The 
second is the stronger double-word compare and swap instruction, LOCK CMPXCHG16B, which atomically swaps two consecutive
words in the same cache line. This locked version of instruction guarantees that the cache line will not be evicted 
halfway between the first and the second swap. The last method is to rely on Restricted Transactional Memory (RTM) support.
RTM attempts to retain all dirty cache lines written by a transaction, until the hardware limit is reached, in which case
the transaction is aborted. Before this happens, the transaction can update as many cache lines as it wants to without 
worrying about evictions. If too many aborts occur for a single update attempt, the working set of the update may not fit 
into RTM. The fall back path with conventional logging will be used instead.
