---
layout: paper-summary
title:  "FaaSnap: FaaS made fast using snapshot-based VMs"
date:   2023-01-28 21:29:00 -0500
categories: paper
paper_title: "FaaSnap: FaaS made fast using snapshot-based VMs"
paper_link: https://dl.acm.org/doi/10.1145/3492321.3524270
paper_keyword: Serverless; Cold-Start Latency; Snapshotting; Virtual Machine; Firecracker
paper_year: EuroSys 2022
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper presents FaaSnap, a serverless (Function-as-a-Service, FaaS) framework that enables fast function invocation
by using efficient snapshotting. The paper is motivated by the high cold-start latency of function invocation in
production environments. While prior works have already explored the idea of leveraging memory snapshots for fast
startups, FaaSnap further optimizes the snapshotting algorithm such that both the I/O cost and the latency penalty 
for loading the snapshot is minimized. When evaluated on a variety of serverless workloads, FaaSnap achieves nearly 
ideal performance compared with warm startups which also outperforms prior works in general.

The paper begins with a review of prior techniques for reducing the cold-start latency, which is a phenomenon observed
in serverless functions that inhibits efficient function invocation if the function is "cold", i.e., its working set 
has not been loaded into the main memory yet. These techniques can be classified into three different types.
The first is called Keep-Alive policy, which caches an existing function instance in the main memory for a 
while after it has completed execution. Both the process image and the accessed files are hence "warm" in the 
main memory or the page cache, enabling future execution to be started with low latency. However, the paper noted
that function Keep-Alive has limited benefits because it consumes main memory resources for an extended period of time
which may prevent useful work from being done on the host. The situation is further aggravated by the fact that
most functions are not repeatedly called, meaning that future invocations will likely still be cold starts.

The second technique is to leverage unikernels and lightweight virtual machines to reduce the amount of work during 
function startup, hence enabling millisecond-level function invocation latency, as exemplified by Firecracker. 
Furthermore, Firecracker also supports taking the memory snapshot of a virtual machine by mapping the guest memory
into a disk file on the host system. In this scenario, when a page fault is raised to the host kernel, if the 
page belongs to a VM instance whose memory state is mapped to a file, the page fault will be handled as a major page
fault, and the corresponding disk file is mapped to the guest physical memory by the kernel. Changes made to the 
guest physical address space can then be committed back to the file.
While this approach lowers startup latency compared with the regular VMs, the paper observes that Firecracker 
still suffers unnecessarily higher latency due to slow DISK I/Os. In addition, anonymous memory in the guest system 
is mapped to the file as part of the memory snapshot and must be loaded back on snapshot recovery. 
However, unused anonymous memory, in practice, can be directly initialized on the host as anonymous memory rather 
than being read from the snapshot file, which wastes processor cycles and I/O bandwidth to fetch it from the snapshot 
file. 

The last technique is REAP, which adopts prefetching to accelerate the startup process. REAP records the working set
of functions by checking the "Access" bit in the guest page table entries. Pages that are accessed during function
execution will then be saved to a disk file as a prefetch set and reloaded back to the main memory the next time
the function is invoked. While this approach can effectively turn major page faults incurred by cold starts into 
minor page faults that only require searching the page cache and establishing address mappings, the paper identifies
two problems. First, REAP only records the first run of a function and uses the working set to generate the prefetch
set. Consequently, the performance benefit of REAP is sensitive to the stability of the working set. If the working set 
changes, REAP still needs to read from the original memory file as in a cold start. Second, REAP loads the prefetch set
into the main memory synchronously at the beginning of execution, which constitutes part of the startup latency.

