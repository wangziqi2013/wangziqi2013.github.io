---
layout: paper-summary
title:  "Distributed Shared Persistent Memory"
date:   2020-09-01 00:35:00 -0500
categories: paper
paper_title: "Distributed Shared Persistent Memory"
paper_link: https://dl.acm.org/doi/10.1145/3127479.3128610
paper_keyword: NVM; Redo Logging; Shadow Paging; Hotpot
paper_year: SoCC 2017
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Lowlight:**

1. Some important abstractions should be elaborated in more details. For example: 
   (1) How are directories of which VA is mapped to which PA on which machine maintained? When a chunk migrates, 
       how to update these directories?
   (2) Are chunks or pages allocated on initialization once and for all, or chunks can be allocated incrementally?
   (3) Is there a unified physical address space, or the resource manager always
   (4) Does DSPM assume that local NVM capacity must be larger than the dirty working set?
   (5) How does NVM chunk allocation and deallocation work?

This paper presents Distributed Shared Persistent Memory (DSPM), an architecture and software stack for large-scale 
distributed NVM deployment. Prior proposals already implement distributed shared DRAM and distributed storage array
based on DRAM and disks. The paper points out, however, that the same technique cannot be directly ported to support 
distributed NVM for two reasons. First, although conventional DSM implements software-based coherence, caching, etc.,
some critical features for NVM are still missing, such as distributed object naming and replication support. Second, 
distributed storage systems are mostly implemented as file system services or object-based stores. The software
overhead for maintaining the abstraction is too large, given the relatively fast access speed of NVM.

DSPM, on the contrary, combines features from both sides to make a design that is both fast and reliable. It borrows
the object naming semantics, data replication and distributed commit from storage architectures to manage persistent
data in a consistent and reliable manner. 
Meanwhile, DSPM also borrows coherence and byte-granularity access from conventional DSM, enabling fast data accesses 
via memory instructions over the network.

We first describe the baseline distributed system as follows. DSPM is implemented as a kernel level software stack, which 
manages virtual address translations, page faults, and runtime metadata.DSPM assumes a distributed system where computation
nodes communicate via high-bandwidth low-latency network, such as Infiniband. The communication is abstracted
by RDMA into RPC requests and responses, which is implemented as a customized networking stack. 
DSPM nodes serve as both computation and storage nodes, which are equipped with NVM devices. DRAM is also installed
in order to run the OS and maintain volatile states. DSPM assumes that tasks can be started at any node in the system,
and each task can consist of multiple threads. The same task, however, must only be executed on the same machine 
to avoid both radical changes to the OS and over-complicating the commit protocol.
The NVM address space is divided into a shared part and a private part. The shared NVM is publicly accessible by all
nodes, under the abstraction of a unified virtual address space per-process, while private NVM
is used as both a cache for frequently accessed pages, and storage for inactive replications.

DSPM nodes maintains per-process metadata for the mapping between virtual pages and physical resources. The matadata
tracks, at a potentially different granularity (the paper suggests 4MB chunks), the ownership and the sharing status
of virtual addresses. A virtual address chunk can be mapped to either a local physical chunk, or a remote physical chunk.
In the latter case, the remote node ID and the chunk ID must be stored in the metadata.
DSPM leverages existing paging mechanism to translate virtual address accesses.
Those that are not cached locally are marked as invalid in the page table, which will raise a page fault on the first 
access. The page fault handler first queries the metadata table to locate the chunk, and then fetchs it from the remote
node, after which the chunk is installed in the private NVM. The page table entry is then updated to point to the 
local page.

The paper does not mention whether a unified physical address space for all processes are maintained, or virtual addresses 
from processes are directly translated into node ID and chunk ID. The paper also does not mention who is responsible for 
maintaining the global address mapping between processes' virtual chunks to node and chunk IDs. In either case, a remote 
chunk fetch request must evetually be translated into a node ID and a chunk ID, and be sent to the corresponding node. 
In the below discussion, we assume that process maintains its own virtual chunk to IDs mapping for better sclability,
i.e., there is no unified physical address space, and memory allocation directly returns the two IDs.
The paper does indicate, however, that each node maintains metadata for chunks in its public NVM. For each chunk, the
node tracks the sharers with a bit vector, and the locking status of the chunk for commit operations. 

Chunks are assigned to processes via the resource allocation service, which maintains a global resource map. The allocation
service tracks allocation status of chunks on all nodes. When a request is received, it finds a chunk in round-robin
manner to evenly distribute workload, and returns the node ID and chunk ID to the requestor. Chunk release simply
marks the chunk as available in the resource map.

Each chunk can have several replications in different states. The one indicated by node ID and chunk ID is regarded as 
the "owner" chunk, which is responsible for replying to chunk fetch requests and handling chunk commits.
A chunk is fetched when it is first accessed by the computation. A fetched chunk is stored in the private part of the 
local NVM. A chunk can be in one of the three states when it is stored in the private NVM (i.e., when it is not the owner):
redundant, committed, and dirty. A redundant chunk copy is clean, and it has not yet been mapped. Redundant chunks are
"pushed" from the owner node to achieve a certain replication level (N copies, where N can be set by the user). 


On page faults, redundant chunks can also satisfy the request locally with contacting the owner node.

// Chunk ownership, states and state transition

We next describe details of DSPM implementation. We start with process initialization and object naming. When a process
is started, it must register itself with a centralized service, called the "Central Dispatcher", which runs on a single
node. The new process 