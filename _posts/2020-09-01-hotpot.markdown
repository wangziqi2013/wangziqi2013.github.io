---
layout: paper-summary
title:  "Distributed Shared Persistent Memory"
date:   2020-09-01 00:35:00 -0500
categories: paper
paper_title: "Distributed Shared Persistent Memory"
paper_link: https://dl.acm.org/doi/10.1145/3127479.3128610
paper_keyword: NVM; Redo Logging; Shadow Paging; Hotpot
paper_year: SoCC 2017
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Lowlight:**

1. Some important abstractions should be elaborated in more details. For example: How are directories of which
   VA is mapped to which PA on which machine maintained? When a chunk migrates, how to update these directories?

This paper presents Distributed Shared Persistent Memory (DSPM), an architecture and software stack for large-scale 
distributed NVM deployment. Prior proposals already implement distributed shared DRAM and distributed storage array
based on DRAM and disks. The paper points out, however, that the same technique cannot be directly ported to support 
distributed NVM for two reasons. First, although conventional DSM implements software-based coherence, caching, etc.,
some critical features for NVM are still missing, such as distributed object naming and replication support. Second, 
distributed storage systems are mostly implemented as file system services or object-based stores. The software
overhead for maintaining the abstraction is too large, given the relatively fast access speed of NVM.

DSPM, on the contrary, combines features from both sides to make a design that is both fast and reliable. It borrows
the object naming semantics, data replication and distributed commit from storage architectures to manage persistent
data in a consistent and reliable manner. 
Meanwhile, DSPM also borrows coherence and byte-granularity access from conventional DSM, enabling fast data accesses 
via memory instructions over the network.

We first describe the baseline distributed system as follows. DSPM is implemented as a kernel level software stack, which 
manages virtual address translations, page faults, and runtime metadata.DSPM assumes a distributed system where computation
nodes communicate via high-bandwidth low-latency network, such as Infiniband. The communication is abstracted
by RDMA into RPC requests and responses, which is implemented as a customized networking stack. 
DSPM nodes serve as both computation and storage nodes, which are equipped with NVM devices. DRAM is also installed
in order to run the OS and maintain volatile states. DSPM assumes that tasks can be started at any node in the system,
and each task can consist of multiple threads. The same task, however, must only be executed on the same machine 
to avoid both radical changes to the OS and over-complicating the commit protocol.
The NVM address space is divided into a shared part and a private part. The shared NVM is publicly accessible by all
nodes, under the abstraction of a unified virtual address space per-process, while private NVM
is used as both a cache for frequently accessed pages, and storage for inactive replications.

We next describe details of DSPM implementation. We start with process initialization and object naming.