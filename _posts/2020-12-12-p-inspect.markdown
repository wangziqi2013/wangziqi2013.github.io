---
layout: paper-summary
title:  "P-Inspect: Architectural Support for Programmable Non-Volatile Memory Frameworks"
date:   2020-12-12 09:33:00 -0500
categories: paper
paper_title: "P-Inspect: Architectural Support for Programmable Non-Volatile Memory Frameworks"
paper_link: https://www.microarch.org/micro53/papers/738300a509.pdf
paper_keyword: NVM; P-Inspect
paper_year: MICRO 2020
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper prposes P-Inspect, a hardware enhancement for supporting reachability-based programming on NVM applications.
Conventional, software-based reachability test either incurs too much run-time cycle overhead, or requires tagging 
the entire address space. P-Inspect, as a hardware solution, achieves both efficiency and flexibility by moving 
high-frequency, easy-to-implement condition checks to hardware, while still having software implementing the majority
of the functionality.

P-Inspect assumes reachability-based NVM programming model. In this model, data structures, which consist of individual 
objects, can be allocated both on DRAM and NVM, but only those in NVM can be preserved after a system crash or power 
loss. The goal of the programming model is that: (1) Initially, objects can be allocated on the DRAM for faster access,
but cannot be pointed to by another object in persistent memory; and (2) If an object is to be referenced by a 
persistent object, the object must be moved to the NVM for consistency.

Prior researches propose that software instrumentations be used to check every load and store to ensure that the 
transitive closure of any persistent objects is all persistent.
These researches assume a managed language environment where reference fields (pointers) can be identified at run-time. 
When a reference field is being updated, the stubs check whether the object being pointed to (the value object) is
in DRAM and whether the host object is in NVM. If true, the run-time will first duplicate the value object in the 
NVM, and mark it as "Queued". Then the original DRAM object is marked as "Forward", meaning that all accesses to the
original object should be redirected to the NVM copy. In the last step, the run-time recursively adds all objects 
referred to by the value object also into NVM, such that the transitive closure is all persistent.
The "Queued" mark indicates that the transitive closure of the object is currently being moved to NVM, which 
blocks further attempts to reference the object from other host objects until the mark is cleared, since otherwise
in a multi-threaded environment, other threads may still set a pointer to the value object, and observes inconsistent
states after a crash.
After the transitive closure has been moved, the Queued mark is cleared, and the invoking pointer update operation
is performed.
DRAM objects that are marked "Forward" will be GC'ed, after all references to them are dropped.
Note that the reachability-based framework has nothing to do with synchronization or atomic persistence. 
Thread synchronization for shared objects should use locks or other techniques, and atomic persistence is achieved with
logging, which are both orthogonal to reachability test. 

The above software framework has two drawbacks that hinders its adoption. First, the extra instructions for checking 
conditions and querying the mapping table add significant cycle overhead. Second, most condition checks on the Queued 
and Forward bit will result in false, meaning that no special action is taken. 
Based on the second observation, the paper proposes that conditions checks be delegated to hardware. There are 
three types of condition checks: (1) When a host object's reference field is updated to point to a value object;
(2) When a host object's primitive field is updated; and (3) When any field is read from a host object.
These three checks are implemented by three different instructions: CheckStoreBoth, CheckStoreH, and CheckLoad.
Field addresses are encoded in the form of base plus offset, such that both the field address and the object's base
address can be obtained from the instruction encoding.

In addition, two global bloom filters, "TRANS" and "FWD", are added for "Queued" and "Forward" mark bookkeeping, 
respectively. These two bloom filters are mapped to the process's metadata page, which can be accessed by both
system software and the cache controller.
Each L1 cache also has a small buffer for both filters, which enables fast access. 
Bloom filter accesses from different cores are coordinated by regular coherence protocol, and the small buffer also
responds to coherence messages. 
To avoid race conditions on updating the multi-cache line filters, the paper suggests that the L1 controller 
should first acquire all cache lines of the filters, lock them in the small buffer, before updates can be applied.
This serializes all update operations in the form of ascending 2PL, which is free of deadlock, and has guaranteed
total ordering.
On context switches, the small filter buffer should be flushed, since they essentially act as a virtual address cache.
