---
layout: paper-summary
title:  "MemZip: Exploring Unconventional Benefits from Memory Compression"
date:   2020-07-18 21:42:00 -0500
categories: paper
paper_title: "MemZip: Exploring Unconventional Benefits from Memory Compression"
paper_link: https://ieeexplore.ieee.org/document/6835972
paper_keyword: Compression; Memory Compression; Subranking; Memzip
paper_year: HPCA 2014
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Lowlight:**

1. I don't get why you don't just store the burst length in the header of the compressed line. In this case you just burst
   the first 8 bytes, check the number, and read the rest with more bursts (the cache is still needed, though). 
   By placing metadata at the end of the current row, you have to activate the ranks that store metadata, which takes
   one more activation. Furthermore, even if it is stored at the end of the row, you still have to burst at least 8 bytes 
   to fetch metadata, which has no obvious advantage.

This paper proposes MemZip, an application of memory compression technique that aims at improving performance and power 
efficiency of the memory system. The paper points out at the beginning that most existing memory compression schemes aim
at improving storage efficiency via flexible allocation of variable sized pages and clever placement of data. These
schemes are not always optimal for several reasons. First, although cache lines become shorter bacause of compression,
the DRAM access protocol still bursts 64 bytes as in uncompressed memory, which over-fetches the line. The paper argues 
that over-fetching only makes sense if adjacent lines are also stored in compressed form together, and when access locality
is high. The second reason is that memory compression schemes often need to maintain large amount of metadata in order to
perform address remapping from uncompressed address space to compressed address space. Since compressed cache lines are 
no longer stored on their home addresses, metadata access must be serialized with data access, which are both on the critical
data access path. Although previous designs also propose metadata cache for hiding such extra cost in most cases, the 
effectiveness of metadata caches are usually limited to smaller working sets, since metadata entries tend to be large
(e.g. 64 bytes per 4KB page). The last, less mentioned reason is that memory compression does not work well with ECC, since
most previous schemes do not store ECC data explicitly with compressed lines. As cache lines can be stored arbitrarily
on any location, ECC bit accesses will likely not result in row buffer hits with compressed data. This both complicates 
ECC memory design for ECC protected systems, and adds extra latency and power consumption on the critical path, which
are hardly evaluated in previous works.

MemZip solves the above issues by not seeking to reduce memory footprint, but merely aiming at reducing bandwidth and 
power consumption and staying compatible with existing ECC schemes. Compressed cache lines are always stored in their
home locations, eliminating the need for address translation. In addition, only the compressed line body is transferred
over the memory bus at 8-byte granularity instead of the original 64 byte granularity, which saves bandwidth and energy
when the compressed size is significantly smaller than 64 bytes. 
The paper also proposes mechanisms for storing ECC bits and other special encoding bits in the "padding" space at the 
end of the 8-byte word that are not used due to alignment.
These extra bits can further reduce ECC access cost, or optimize bus data transfer.

The MemZip architecture is based on DRAM subranking, which is a technique for reducing access graularity in the DRAM 
access protocol. The paper assumes DDR3 interface, in which all DRAM accesses will involve eight consecutive bursts
of 8 byte each, fetching an full 64 byte cache line in one access request. The number of bursts, however, is 
not changable in DDR3 protocol, which is unfortunate, since a memory access must then always fetch 64 bytes as long as 
the granularity of a single burst is eight bytes. On the other hand, memory subranking reduces the access granularity 
to one byte by selectively activating only a subset of all ranks in the memory. By adopting memory subranking, each
DDR3 burst sequence only fetches 8 1-byte blocks, totalling to 8 byte per access. 
Previous publications have pointed out that subranking is helpful in reducing power consumption and improving performance,
at the cost of more access transactions and longer access latency.
In this paper, memory subranking is leveraged to fetch compressed data, which may use significantly less storage than
a full 64 byte cache line. In this case, subranking not only saves bus bandwidth by not over-fetching unused bits from
the memory, but also improves performance, since shorter compressed lines only require a few burst cycles to read.

MemZip operates as follows. Cache lines are compressed and decompressed when it enters and leaves the memory controller
respectively. MemZip does not specify any particular compression algorithm to use, but BDI and FPC are suggested to be
used together for reasonable compression ratio and fast decompression (the paper claims that both can be implemented
with 1 - 2 cycles decompression latency).
A compressed line is always stored at its home location without any address translation. Read and write
accesses therefore always use the physical address generated by the on-chip MMU. 
Cache lines written back from the upper level are compressed with both BDI and FPC in parallel, and the better result
among these two are selected. 
Compression metadata is stored in the first byte of the home address. The initial three bits indicate the compression 
scheme (BDI uses seven of them, and the last is left to FPC), while the rest 5 bits store DBI information which we discuss 
later. If DBI is used as suggested by the DBI field in the metadata byte, the next one to three bytes store DBI bits, 
which must be read before any data transfer since DBI optmizes data transfer via bit flipping.

Dirty cache lines may find itself larger than the old version after compression, which requires no extra handling, unlike 
previous proposals in which the line may "overflow" to a special region or compaction must be performed in the background.

