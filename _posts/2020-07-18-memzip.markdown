---
layout: paper-summary
title:  "MemZip: Exploring Unconventional Benefits from Memory Compression"
date:   2020-07-18 21:42:00 -0500
categories: paper
paper_title: "MemZip: Exploring Unconventional Benefits from Memory Compression"
paper_link: https://ieeexplore.ieee.org/document/6835972
paper_keyword: Compression; Memory Compression; Subranking; Memzip
paper_year: HPCA 2014
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes MemZip, an application of memory compression technique that aims at improving performance and power 
efficiency of the memory system. The paper points out at the beginning that most existing memory compression schemes aim
at improving storage efficiency via flexible allocation of variable sized pages and clever placement of data. These
schemes are not always optimal for several reasons. First, although cache lines become shorter bacause of compression,
the DRAM access protocol still bursts 64 bytes as in uncompressed memory, which over-fetches the line. The paper argues 
that over-fetching only makes sense if adjacent lines are also stored in compressed form together, and when access locality
is high. The second reason is that memory compression schemes often need to maintain large amount of metadata in order to
perform address remapping from uncompressed address space to compressed address space. Since compressed cache lines are 
no longer stored on their home addresses, metadata access must be serialized with data access, which are both on the critical
data access path. Although previous designs also propose metadata cache for hiding such extra cost in most cases, the 
effectiveness of metadata caches are usually limited to smaller working sets, since metadata entries tend to be large
(e.g. 64 bytes per 4KB page). The last, less mentioned reason is that memory compression does not work well with ECC, since
most previous schemes do not store ECC data explicitly with compressed lines. As cache lines can be stored arbitrarily
on any location, ECC bit accesses will likely not result in row buffer hits with compressed data. This both complicates 
ECC memory design for ECC protected systems, and adds extra latency and power consumption on the critical path, which
are hardly evaluated in previous works.

MemZip solves the above issues by not seeking to reduce memory footprint, but merely aiming at reducing bandwidth and 
power consumption and staying compatible with existing ECC schemes. Compressed cache lines are always stored in their
home locations, eliminating the need for address translation. In addition, only the compressed line body is transferred
over the memory bus at 8-byte granularity instead of the original 64 byte granularity, which saves bandwidth and energy
when the compressed size is significantly smaller than 64 bytes. 
The paper also proposes mechanisms for storing ECC bits and other special encoding bits in the "padding" space at the 
end of the 8-byte word that are not used due to alignment.
These extra bits can further reduce ECC access cost, or optimize bus data transfer.

The MemZip architecture is based on DRAM subranking, which is a technique for reducing access graularity in the DRAM 
access protocol. The paper assumes DDR3 interface, in which all DRAM accesses will involve eight consecutive bursts
of 8 byte each, fetching an full 64 byte cache line in one access request. The number of bursts, however, is 
not changable in DDR3 protocol, which is unfortunate, since a memory access must then always fetch 64 bytes as long as 
the granularity of a single burst is eight bytes. 
