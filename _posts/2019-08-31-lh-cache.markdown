---
layout: paper-summary
title:  "Efficiently Enabling Conventional Block Sizes for Very Large Die-Stacked DRAM Caches"
date:   2019-08-31 05:03:00 -0500
categories: paper
paper_title: "Efficiently Enabling Conventional Block Sizes for Very Large Die-Stacked DRAM Caches"
paper_link: https://dl.acm.org/citation.cfm?id=2155673
paper_keyword: L4 Cache; DRAM Cache; LH Cache
paper_year: MICRO 2011
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper presents a design for DRAM-based L4 cache implemented with Die-Stacked DRAM. Conventional DRAM is not suitable
for implementing any form of caching, because accessing the DRAM cache before going to the home location can only
increase latency in all situations. Die-Stacked DRAM, however, makes DRAM caching feasible due to its lower access latency.
Previous researches reported lower latency of Die-Stacked DRAM, ranging from half to one-fourth of the latency of conventional 
DRAM, which implies the possibility of using on-chip Die-Stacked DRAM to implement an additional L4 cache between the LLC
and conventional DRAM.

This paper points out, however, that due to the large size of DRAM caches, storing the full set of tags can be difficult. 
For example, to support 1GB DRAM cache, the size of the tag store reoported by the paper is 96MB (i.e. 9.4% storage overhead), 
which already exceeds the maximum amount of fast SRAM implementable with today's technology (the paper was written in 2011, 
but I believe even in 2019 this amount of SRAM is either impossible or extremely slow/expensive). The paper also identifies 
several solutions to address the tag store issue. The first solution is to increase the size of the block, and hence 
reduce the number of tags for the same amount of cached data. The problem, however, is that large blocks (e.g. 4KB) have 
to be read from and written into their home locations as an indivisible unit. Without sufficient locality, which is typically 
the case with lower level caches because the locality has been filtered out by higher level caches, only a few smaller blocks 
will be used in the large unit, resulting in wastage of bandwidth and contention on the memory bus (because a larger block takes 
longer to transfer). The second solution is sector cache, in which the granularity of blocks are unchaned, but multiple 
blocks, called a "sector", are grouped together and only associated with a single tag. Blocks can be transferred and evicted
at the granularity of 64 byte blocks as on-chip caches. A bit vector is also added to indicate which blocks are present in 
the sector and which are not. The problem with sector cache, however, is that it assumes spatial locality of access, i.e. 
if a block is brought into the sector cache by an access, then later accesses are expected to occur on nearby addresses. 
As argued in the previous point, this assumption does not work well for lower level caches due to the filtering effect from
higher level caches. As a result, higher miss rates are observed with sector caches compared with a regular cache. The 
last solution is to store tags in a separate DRAM store area which is organized as a tag array. Before data in the DRAM is 
accessed, we first read the tag array to locate the data block (or generate a miss), and then use a second access to 
read the data block. The problem with this method is that three accesses are incurred for every cache hit. The first two are 
to tag and data store, and the last access updates the LRU status. Although the last access is not on the critical path
of a cache hit, the extra DRAM update request increases the level of contention on the memory controller, delaying later 
requests.

This paper argues that DRAM caches can be made efficient by reducing the latency of accessing the tag store. The paper
assumes that the row size of the Die-Stacked DRAM is 2KB, though other row sizes are also applicable. One important observation
is that current generations of DRAM are equipped with a row buffer. The row buffer is populated with the bits read
from the internal array when a read command is sent. The content of the row buffer can be preserved even after the read 
operation, which enables the next read to "hit" the row buffer if it accesses the same row as the one current opened. 
This "open page" design gives opportunity to DRAM caches if we co-locate tags and data in the same row, allowing them 
to be accessed with one relatively expensive row buffer read, and several chaper column accesses. 

The DRAM cache is described as follows. We still use the conventional set-associative cache organization, with an extremely
high associativity. The paper suggests that we use a DRAM row to store an entire set, including data, tags and metadata. For 
2KB rows, at most 29 data blocks can be stored, which takes 1856 bytes. Assuming 6 byte tags (48 bit physical address),
we need an extra 174 bytes to store the tag as a compact array, such that they can be "streamed" out of the DRAM with burst 
reads. The remaining 18 bytes can be dedicated to other metadata such as dirty bit, coherence states or storing profiling
data.

The operation of the DRAM cache is described as follows. When the L3 cache misses, a probe into the L4 DRAM cache is 
initiated. First, the row number of cpmputed using middle bits in the miss address in exact the same way as in a conventional
cache. The target row number of computed by adding this set index onto the base row number of the cache region (configured 
once at system startup time). After computing the target row, the cache controller then sends an activate command to the 
DRAM cache with the target row number. 