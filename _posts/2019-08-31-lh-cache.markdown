---
layout: paper-summary
title:  "Efficiently Enabling Conventional Block Sizes for Very Large Die-Stacked DRAM Caches"
date:   2019-08-31 05:03:00 -0500
categories: paper
paper_title: "Efficiently Enabling Conventional Block Sizes for Very Large Die-Stacked DRAM Caches"
paper_link: https://dl.acm.org/citation.cfm?id=2155673
paper_keyword: L4 Cache; DRAM Cache; LH Cache
paper_year: MICRO 2011
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper presents a design for DRAM-based L4 cache implemented with Die-Stacked DRAM. Conventional DRAM is not suitable
for implementing any form of caching, because accessing the DRAM cache before going to the home location can only
increase latency in all situations. Die-Stacked DRAM, however, makes DRAM caching feasible due to its lower access latency.
Previous researches reported lower latency of Die-Stacked DRAM, ranging from half to one-fourth of the latency of conventional 
DRAM, which implies the possibility of using on-chip Die-Stacked DRAM to implement an additional L4 cache between the LLC
and conventional DRAM.

This paper points out, however, that due to the large size of DRAM caches, storing the full set of tags can be difficult. 
For example, to support 1GB DRAM cache, the size of the tag store reoported by the paper is 96MB (i.e. 9.4% storage overhead), 
which already exceeds the maximum amount of fast SRAM implementable with today's technology (the paper was written in 2011, 
but I believe even in 2019 this amount of SRAM is either impossible or extremely slow/expensive). The paper also identifies 
several solutions to address the tag store issue. The first solution is to increase the size of the block, and hence 
reduce the number of tags for the same amount of cached data. The problem, however, is that large blocks (e.g. 4KB) have 
to be read from and written into their home locations as an indivisible unit. Without sufficient locality, which is typically 
the case with lower level caches because the locality has been filtered out by higher level caches, only a few smaller blocks 
will be used in the large unit, resulting in wastage of bandwidth and contention on the memory bus (because a larger block takes 
longer to transfer). The second solution is sector cache, in which the granularity of blocks are unchaned, but multiple 
blocks, called a "sector", are grouped together and only associated with a single tag. Blocks can be transferred and evicted
at the granularity of 64 byte blocks as on-chip caches. A bit vector is also added to indicate which blocks are present in 
the sector and which are not. The problem with sector cache, however, is that it assumes spatial locality of access, i.e. 
if a block is brought into the sector cache by an access, then later accesses are expected to occur on nearby addresses. 
As argued in the previous point, this assumption does not work well for lower level caches due to the filtering effect from
higher level caches. As a result, higher miss rates are observed with sector caches compared with a regular cache. The 
last solution is to store tags in a separate DRAM store area which is organized as a tag array. Before data in the DRAM is 
accessed, we first read the tag array to locate the data block (or generate a miss), and then use a second access to 
read the data block. The problem with this method is that three accesses are incurred for every cache hit. The first two are 
to tag and data store, and the last access updates the LRU status. Although the last access is not on the critical path
of a cache hit, the extra DRAM update request increases the level of contention on the memory controller, delaying later 
requests.