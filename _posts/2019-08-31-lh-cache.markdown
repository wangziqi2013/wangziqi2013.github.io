---
layout: paper-summary
title:  "Efficiently Enabling Conventional Block Sizes for Very Large Die-Stacked DRAM Caches"
date:   2019-08-31 05:03:00 -0500
categories: paper
paper_title: "Efficiently Enabling Conventional Block Sizes for Very Large Die-Stacked DRAM Caches"
paper_link: https://dl.acm.org/citation.cfm?id=2155673
paper_keyword: L4 Cache; DRAM Cache; LH Cache
paper_year: MICRO 2011
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper presents a design for DRAM-based L4 cache implemented with Die-Stacked DRAM. Conventional DRAM is not suitable
for implementing any form of caching, because accessing the DRAM cache before going to the home location can only
increase latency in all situations. Die-Stacked DRAM, however, makes DRAM caching feasible due to its lower access latency.
Previous researches reported lower latency of Die-Stacked DRAM, ranging from half to one-fourth of the latency of conventional 
DRAM, which implies the possibility of using on-chip Die-Stacked DRAM to implement an additional L4 cache between the LLC
and conventional DRAM.

This paper points out, however, that due to the large size of DRAM caches, storing the full set of tags can be difficult. 
For example, to support 1GB DRAM cache, the size of the tag store reoported by the paper is 96MB (i.e. 9.4% storage overhead), 
which already exceeds the maximum amount of fast SRAM implementable with today's technology (the paper was written in 2011, 
but I believe even in 2019 this amount of SRAM is either impossible or extremely slow/expensive). The paper also identifies 
several solutions to address the tag store issue. The first solution is to increase the size of the block, and hence 
reduce the number of tags for the same amount of cached data. The problem, however, is that large blocks have to be read
from and written into their home locations as an indivisible unit. Without sufficient locality, which is typically the case
with lower level caches because the locality has been filtered out by higher level caches, only a few smaller blocks will
be used in the large unit, resulting in wastage of bandwidth and contention on the memory bus (because a larger block takes 
longer to transfer).