---
layout: paper-summary
title:  ""
date:   2020-07-03 19:38:00 -0500
categories: paper
paper_title: ""
paper_link: https://www.sciencedirect.com/science/article/pii/S1383762100000308
paper_keyword: Cache; SCMS; Cache Compression
paper_year: Journel of System Architecture 2000, Issue 46
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Highlight:**

1. Letting OS decide which blocks should be compressed and hardware only follows is a illusrtration of good division of 
   responsibility (The OS initialize the page table and compressed data layout, and hardware follows this layout unless
   a fat write occurs).

2. The seamless integration between page-based memory compression and super-block based cache compression saves metadata
   overhead. Compressed blocks are not decompressed before and after transferred over the system bus, maintaining a uniform
   layout between memory and LLC.
   
3. It is a good balancing point between efficiency and simplicity to only compress adjacent even-odd blocks.

4. Using page table to store the small compression vector for describing page layout to achieve minimum DRAM metadata overhead.

5. Using adjacent sets to store compressed blocks in the LLC to minimize LLC metadata overhead

**Lowlight:**

1. Writing is really poor. I do suggest whoever that wrote Section 1 and 2 perform a basic proof read on later sections.
   In addition, whoever wrote later sections should first make sure correct terminologies are used, and don't write like
   spagetti. 
   Overall, the paper looks like a product of pipelined writing, which is fine, but before submission can you please
   at least unify the writing style across sections.

This journal article proposes Selective Compressed Memory System (SCMS), a unified cache and memory compression scheme
for reducing storage overhead and increasing performance.
The journal identifies three challenges of cache and memory compression designs. First, decompression is on the critical
path of access operations. The decompression architecture must be able to deliver raw bytes with high throuhgput and low
latency. For a unified LLC and DRAM compression scheme, the decompression bottleneck occurs on the datapath between the
LLC and the upper level cache, which requires certain hardware structure to deal with.
Second, compression algorithms, in the worst case, may produce blocks even larger than the uncompressed size due to extra
metadata bits in the compressed stream. The compression scheme must be able to identify memory blocks that are not easily
compressible, and disable compression on these blocks to minimize the negative impact. 
The last challenge is that the size of compressed blocks may change, requiring potential layout changes if compressed blocks
are stored compactly. The article calls this as "fat write" problem. When fat writes occur, either the layout of compressed 
blocks is changed, or the compression mechanism leaves some "slack" to absorb such changes in some degrees.

SCMS solves the above issues using a unified DRAM and LLC compression architecture. First, compressed blocks are transferred
in the compressed form over the system bus. No extra compression and decompression is performed on data exchange between
DRAM and the LLC. Second, SCMS only compresses two adjacent even-odd numbered cache line sized blocks into one block,
achieving a maximum compression ratio of 2:1. This design decision not only simplifies the decompression algorithm due
to the lower requirement on compression ratio, but also allievates "fat write" problem, since it can tolerate slight
size changes after writes, as long as the compression ratio is still above 2:1. Lastly, SCMS features simple layouts of 
compressed blocks on both DRAM and LLC. Blocks are still stored as pages in the DRAM, just with two different size class
pages, which can be addressed with simple offset arithmetics. In the LLC, two compressed blocks always occupy the same 
physical slot, which can be in one of the two sets these two blocks' addresses map to. The simple layout reduces metadata
overhead to a minimum, as neither extra tag in the cache nor extra metadata storage in DRAM is required to perform
address mapping for compressed blocks.

TODO: Adding overall arch, i.e. how selective works, how unified works, how adjacent blocks are compressed 

TODO: Adding three tradeoff, i.e. Only consider 2:1 compression; Only compression two blocks to the same slot; Only 
use two size classes

TODO: Adding compression algo at the end