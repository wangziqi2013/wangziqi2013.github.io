---
layout: paper-summary
title:  "TLC: A Tag-Less Cache for Reducing Dynamic First Level Cache Energy"
date:   2021-08-23 01:46:00 -0500
categories: paper
paper_title: "TLC: A Tag-Less Cache for Reducing Dynamic First Level Cache Energy"
paper_link: https://dl.acm.org/doi/10.1145/2540708.2540714
paper_keyword: Cache; TLB; TLC; Tag-Less Cache
paper_year: MICRO 2013
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes the Tag-Less Cache (TLC) architecture to reduce energy consumption of conventional L1 caches.
The paper is motivated by the fact that conventional caches are rather power hungry, which is caused by two 
design choices. First, cache accesses always need tag comparisons in order to locate the data block in one of the 
many ways, or declare a cache miss. 
Second, to reduce access latency, both the tags and data blocks in the set are read in parallel after the set index is 
computed, and only one of the blocks is selected in the case of a cache hit, which wastes the energy consumed by 
accessing the rest of the set.
The waste can be quite significant, since the entire set is accessed, while at most one of them will be actually useful.

Previous works attempt to reduce energy issues of cache accesses with two techniques. The first one, staged access, 
serializes the tag access and data access into two phases. Only the way in the data bank that contains the requested 
block will be accessed, if it is a hit, or the data bank is not accessed at all, if it is a miss.
This approach saves energy of data bank accesses, but increases the access latency by one cycle.
The second technique, way prediction, attempts to predict the way that an access will likely to hit, and only checks 
the predicted way. It avoids parallel lookup as well as associative set lookup by speculatively reading the data bank
first, and checking the tag array only for confirmation. If speculation fails, a conventional lookup is still performed,
which results in both increased latency and energy consumption.

This paper, on the other hand, proposes that way information of addresses can be stored in the TLB, and that 
the L1 cache can simply get rid of the tag array.
To be more specific: For each TLB entry, we add 64 1-bit flag indicating the existence of the corresponding 
block address in the L1 cache, and 64 log2(K)-bit way indicators, one for each block. 
The L1 cache does not have a tag array. Instead, it only has one data array which can be accessed by giving the set
and way number. For each data block, a back pointer storing the index of the TLB entry is maintained, such that
the TLB entry can be located quickly when the block is evicted.

We next describe the operations in details. On a cache access, the virtual address is first used to probe the TLB.
If the TLB hits, then the corresponding validity bit, which is indexed with bits of the address between the 
page number and block offset, will first be checked. If the validity bit is zero, meaning that the block
does not exist, then a cache miss can be signaled immediately, and the physical page number is read out to form
the address to be fetched from the lower level.
If the validity bit is one, then the access hits the cache, and the way number is retrieved, which, combined with the
set index (generated in the same manner as in a conventional cache), is used to access the data array.
The paper noted that in the case of a cache hit (which should be the majority of cases), the physical page number
does not need to be retrieved from the TLB, since the way number is sufficient to access the data bank.

On a cache miss, an existing entry needs to be evicted from the L1. The eviction takes place by following the back 
pointer of the entry to be evicted to locate the TLB entry, and resetting its validity bit.
When the new entry is fetched from the lower level, the corresponding TLB entry is also initialized by first
setting the validity bit, and then storing the way number.

On TLB misses, an existing TLB entry is evicted, and the new entry is brought in. When evicting the existing entry,
all cache blocks mapped by that entry also should be evicted from the L1 cache. 
The new TLB entry is initialized by setting all validity and way number fields to zero, since the eviction 
protocol guarantees that addresses not mapped by the TLB will not be cached.

One issue of the above baseline design is that evicted TLB entries may also evict cache blocks, causing unnecessary 
misses. Although this may not be a huge problem for programs with good locality, as the coverage of the TLB is much 
larger than the coverage of the L1 cache, for programs with low locality, the worse case scenario is that each TLB entry
only has one valid block, and the total number of valid blocks that can co-exist in the L1 cache is the number of 
entries in the TLB, which is far less than the number of L1 data slots.
