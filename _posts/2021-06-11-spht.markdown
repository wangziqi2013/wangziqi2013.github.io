---
layout: paper-summary
title:  "SPHT: Scalable Persistent Hardware Transactions"
date:   2021-06-11 01:00:00 -0500
categories: paper
paper_title: "SPHT: Scalable Persistent Hardware Transactions"
paper_link: https://www.usenix.org/system/files/fast21-castro.pdf
paper_keyword: NVM; SPHT; HTM
paper_year: FAST 2021
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper presents SPHT, a durable hardware transactional memory design that achieves high scalability.
The paper recognizes three major difficulties in designing a durable TM. The first difficulty is atomicity of
transactions in volatile memory, which must be enforced by some volatile TM subsystems. 
Two options are available: STM and HTM. The issue, however, is that STM will incur non-negligible instrumentation
and metadata overhead, which harms overall performance especially when transactions are short.
HTM, on the other hand, is largely free of runtime overheads for ensuring atomicity. Their flexibility, however,
is inferior compared with STM, as most commercial HTM implementations nowadays will not allow a cache line to be flushed
out of the cache during transaction, which will result in an immediate abort. 
This feature, unfortunately, makes it challenging to implement write-ahead logging (WAL) for durability at the same
time, since WAL enforces write orderings between log entries and data blocks using cache line flush instructions.
The paper mentions that prior designs either adopt shadow paging to avoid dirty transaction data from being written
back to the NVM image, or perform complicated non-destructive logging to avoid enforcing any write ordering.

The second difficulty is scalability of commit. The paper notices that durable transactions must guarantee that the
durability order (i.e., the order of transactions that logically commit on the NVM) must be consistent with the 
memory consistency order (i.e., the order of transactions that logically commit on volatile memory via load-store
ordering). Otherwise, during recovery, the effect of a transaction that had been acknowledged by the TM may not be 
recovered, causing the lost update anomaly, since the transaction whose updates are lost may have already incurred some
external effect (e.g., a message sent to the user) that will not be undone by the crash.

The last difficulty is the scalability of log replay during recovery and log truncation. The latter is far more
commonly used, and is hence more performance-critical. The paper points out that prior proposals all use a single
background thread for log replay, which does not scale with the number of worker threads, as the number of log entries
to replay will grow proportionally to the number of worker threads.
