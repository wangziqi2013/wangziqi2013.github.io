---
layout: paper-summary
title:  "A Case for Core-Assisted Bottleneck Acceleration in GPUs: Enabling Flexible Data Compression with Assist Warps"
date:   2020-07-28 05:23:00 -0500
categories: paper
paper_title: "A Case for Core-Assisted Bottleneck Acceleration in GPUs: Enabling Flexible Data Compression with Assist Warps"
paper_link: https://dl.acm.org/doi/10.1145/2749469.2750399
paper_keyword: Compression; GPU; BDI; CABA
paper_year: ISCA 2015
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Note: I am not an expert on GPGPU architecture, and have only read very basic GPGPU architecture literatures. The following
paper summary may be inaccurate and/or incorrect when it involves GPGPU internals. I will strive to clarify as much as I can,
and put the focus on compression rather than GPGPU.**



**Lowlight:**

I am not really confident on my understanding of GPGPU, so I can't judge design and writing problems of this paper



This paper proposes Core Assisted Bottleneck Acceleration (CABA), an architecture that leverages idle cycles and resources 
on GPGPUs to perform out-of-band tasks for acceleration.
The paper points out that resources on modern GPGPUs are not fully utilized in many cases due to several reasons. 
First, the memory bandwidth and inter-component link bandwidth are often underutilized, due to the speed discrepancy between
execution units and memory modules. As a result, if most threads are stalled on memory instructions, cycles will be 
wasted since no progress can be made during the stalled cycles.
Second, on-chip resources, such as registers and local memory storage, which are allocated statically by compilers
and scheduled dynamically by hardware, are often under utilized as well. This is caused by the hardware scheduler not
being able schedule an entire thread block, which is the unit of scheduling, with partial resources on the current
processing unit. These registers and local storage will be wasted, which can be leveraged to perform background tasks
as prposed in this paper.

The CABA architecture in this paper is described as follows. CABA adopts a software-hardware co-design approach to achieve
a balance between efficiency, usability and hardware complexity. Pure hardware or software schemes are sub-optimal, due
to the level of new hardware components added to existing GPGPU, and the limitation of existing GPGPU programming models.
On the software level, CABA customizes compilers to allow each kernel to be accompanied by one or more special helper 
routines, which are executed in the background when the main kernel is stalled or when some events are triggered.
These routines are largely transparent to the main kernel except some necessary data exchange at the beginning and the 
end of the execution.
The compiler is aware of the semantics of helper routines, and will compile them accordingly based on the architecture
specification. One of the most notable features of the compiler is that the register and local storage of the helper 
routine should be explicitly allocated by the compiler statically, which is added to the total resource requirement
of each thread block. The GPGPU hardware is unaware of the extra resources that are dedicated to the execution of 
these helper routines.

On the hardware side, when GPGPU is programmed with the kernel, the helper routine is also transferred to GPGPU's instruction
memory, and buffered on an on-chip instruction cache. The helper routine is executed as Assist Warps in a similar way
as the main kernel. Assist Warps are not executed, until some predefined conditions or events occur, in which case they
are scheduled on the processing unit.
The paper proposes three extra hardware components added to the GPGPU for tracking, scheduling, and maintaining states
for these warps. The first component is the Assist Warp Store (AWS) that stores the program body of the helper routines.
More than one routines can be stored at the same time, which are indexed using a globally unique source ID (SR.ID) and 
instruction ID (Inst.ID) for accessing individual instructions. 

The second component is Assist Warp Buffer (AWB), which is a buffer for decoded Assist Wrap instructions. The paper suggests
that this buffer may share the same storage with the existing Instruction Buffer (IB), while providing a few extra slots
for low priority tasks which can be issued concurrently with the main warp in the background, without contending for
resources with the latter.

The last component is Assist Warp Controller (AWC), which schedules Assist Warps on the processing unit. During normal
execution, the AWC monitors trigger events or conditions. When any of the triggers occur, it schedules out the main kernel
and brings in Assist Warp's instruction. Note that GPGPUs are typically not equipped with proper hardware for context
switch. The paper suggests that the Assist Warp executes under the same context as the main kernel. The compiler should
be aware of this, and reserve dedicated registers for the Assict Warp to use to avoid corrupting main kernel's state
unexpectedly.

The paper then discusses the possibility of using CABA to perform compression on GPGPU's L2 cache and main memory.
The peper gives three reasons why compression should better be implemented with CABA rather than with specialized
control and data path. First, CABA leverages existing buffering and execution units on the GPGPU, while adding 
dedicated components for compression would change the hardware significantly with less generality achieved. Second, 
different workloads exhibit different data regularity. No universal compression algorithm is better than any other.
To make compression widely applicable to GPGPU, the compression scheme should be able to implement several common
algorithms, such as FPC, BDI, C-PACK, which tackle different types of data. With dedicated hardware this would be 
difficult or impossible. 
