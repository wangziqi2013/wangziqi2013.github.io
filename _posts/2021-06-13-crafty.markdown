---
layout: paper-summary
title:  "Crafty: Efficient, HTM-Compatible Persistent Transactions"
date:   2021-06-13 20:10:00 -0500
categories: paper
paper_title: "Crafty: Efficient, HTM-Compatible Persistent Transactions"
paper_link: https://dl.acm.org/doi/10.1145/3385412.3385991
paper_keyword: NVM; HTM; Crafty; TSX/RTM
paper_year: FAST 2021
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Comments:**


This paper proposes Crafty, a novel software transaction memory design based on re-execution. The paper begins 
by observing that all previous designs have non-negligible run-time costs that root deeply into the methodology.
The paper investigates into three typical mechanisms: undo logging, redo logging, and shadow paging.
Undo logging requires write ordering between the log entry and the data block being written. As a result, each
write operation must be preceded by log generation and persistence, which has a large overhead since the log 
flush uses what is called a persist barrier. The barrier, which typically consists of several cache flush
instructions followed by a store fence (clwb and sfence on x86), stalls the pipeline for at least the amount
of time required for a round-trip between the cache hierarchy and the NVM controller, which is usually a few
hundreds of cycles, causing performance degradation.

Redo logging, on the other hand, does not require per-operation write ordering. As long as dirty data is written
in-place after the transaction commits (which can be implemented as write ordering between all dirty blocks
and the commit mark, but in practice, it is usually implemented as shadowing the objects in volatile memory),
no extra write ordering is enforced. The paper points out, however, that redo logging needs read operations 
to also check the read log in order to access the most up-to-date value written by the current or 
earlier committed transactions, 
incurring a non-negligible overhead for reads. Since reads are more common than writes in most workloads, the
overall performance may still be affected (again, this can be avoided by shadowing objects to volatile memory).

Shadow paging differs from logging approaches by not maintaining fixed home locations for data items. Instead,
a global mapping table remaps objects from their old locations to a new copy, preserving the consistency of the
old snapshot, which can be restored to if the transactions fails to commit due to a crash. 
Despite the fact that shadow paging eliminates explicit write ordering, the paper notes that shadow paging
needs to enforce consistency between memory consistency ordering and the order of durability. In other words,
the logical ordering of transactions in the memory consistency order must also be observed in persistence
order (which, under the context of shadowing, is the order that the mapping table is updated), because 
otherwise, the recovery may result in a state that cannot be achieved by any serial execution
of committed transactions. This can result in non-scalable designs, since in shadow paging, transactions are 
actually committed by atomically updating the centralized mapping table. 
In addition, in some designs, global ordering must be established using a single timestamp counter, which
is difficult to scale.

The paper then notices that Hardware Transactional Memory is a potentially powerful mechanism for implementing
durable transactions. Existing HTM implementations, however, disallow writing back cache lines from the 
hierarchy to the NVM, which will cause an immediate transaction abort because of the way speculative states
are maintained in the hierarchy.
Such restriction makes it impossible to generate and persist log entries during hardware transactions, and
as a result, naive logging will not work.

Crafty, at a high level, adopts HTM for atomicity while avoiding the dilemma of logging with re-execution of 
the transaction body. The first "dry-run" execution only generates undo log entries that will be persisted 
after the transaction, while the actual memory updates are discarded. This eliminates the write ordering 
requirement, since memory updates that correspond to the undo log entries have not been committed yet. 
In the second execution, memory updates are performed, which will be committed at the end. 
Both executions use HTM to ensure atomicity. 
From a different perspective, the HTM transaction body can be considered as a monolithic memory operation that 
completes atomically, whose updates and can be determined in priori. Durability is achieved by first generating 
and persisting the undo log entries using the before value of these memory updates, and then actually executing
the monolithic memory operation, assuming that its semantics remain consistent with the undo logs (i.e., 
memory locations to be updated and data to be written).

