---
layout: paper-summary
title:  "Vorpal: Vector Clock Ordering for Large Persistent Memory Systems"
date:   2019-11-15 15:08:00 -0500
categories: paper
paper_title: "Vorpal: Vector Clock Ordering for Large Persistent Memory Systems"
paper_link: https://dl.acm.org/citation.cfm?doid=3293611.3331598
paper_keyword: NVM; Vorpal; Vector clock; FASE
paper_year: PODC 2019
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Vorpal, a novel vector clock based scheme for ensuring memory persistence order on byte-addressable 
NVM. The paper assumes a programming model called Failure-Atomic Sections (FASE), which are critical sections synchronized
using locks whose modifications are expected to be persisted to the NVM. An FASE must observe two properties. First, all
modifications within an FASE should be atomic, i.e. either all stores are persisted, or none of them is persisted. Second,
during recovery, if a FASE is rolled back, then none of the FASEs that depent on it should be persisted. In other words,
data flow ordering should also be obeyed when determining whether a FASE should persist or be rolled back during recovery.
In the past, researchers proposed to use runtime libraries to instrument lock and unlock operation to delineate the boundaries
of FASEs, and then use a background thread to compute data flow ordering. The background thread computes the most recent
consistent image by performing BFS on the graph based on unlock-lock relation, advancing the "frontier" to determine whether
a FASE should persist or be rolled back. FASEs within the frontier can be persisted safely (since their dependent FASEs
are also committed), while those not within the frontier must wait.

Vorpal is a hardware scheme that uses vector clock to encode the ordering of critical sections and store operations. 
In the paper, it is assumed that the system consists multiple cores, which can be on different sockets. Each socket
has a few memory controllers that process read and write requests from the LLC. Since physical address space might be 
partitioned between these sockets, a store request generated at a core might be sent to a remote socket and written
into remote memory. Local writes are processed by one of the local memory controllers on the chip. The paper assumes 
a fixed address-to-controller mapping, i.e. we use a hash function to map a physical address to a controller, and this 
relation is fixed. Each socket also has an interfacing component, called the gateway, which is responsible for communicating 
local reads and writes to remote sockets. All remote traffic must go thruogh this component. As we will see later, this
component can be critical to reducing the storage overhead when the scale of the system is large.

Having multiple memory controllers can introduce problems for write ordering. For example, on today's architecture, we 
enforce memory ordering using memory barriers, which consist of a sequence of cache line flush or write back instructions, 
and a store fence at the end. The store fence will be removed from the pipeline only when data written back to the NVM 
are acknowledged by the memory controller. The controller guarantees the persistence of the store once it has been 
enqueued, even on power failures (e.g. using ADR). The implicit assumption about this ordering guarantee is that: (1) memory
controllers persist memory writes in exact FIFO order. Otherwise, another store instruction after sfence executed may be 
persisted before the one that gets enqueued before the sfence instruction, causing ordering problem; (2) If there are 
multiple memory controllers, writes from the same thread must always be mapped to the same controller. Because otherwise,
two writes separated by a persist barrier might be written back in the wrong order by two different memory controllers,
even if both of them process writes in the queue in strict FIFO order. 

Vorpal uses vector clocks to address the problem that hardware cannot resolve dependent writes without more ordering
information. In the baseline design of Vorpal, each core has a vector clock of size C, where C is the number of cores
in the system. Each element i in the vector of core j (except the case where i == j) represents the timestamp of processor 
j when last time i and j communicates. Element j on processor j is this process's local timestamp. This local timestamp
is incremented to indicate that all stores after the increment must be persisted only if stores before this timestamp
are. We later show how this is achieved by letting the memory controller compare timestamps. 
