---
layout: paper-summary
title:  "High Performance Cache Replacement Using Re-Reference Interval Prediction"
date:   2018-06-19 05:43:00 -0500
categories: paper
paper_title: "High Performance Cache Replacement Using Re-Reference Interval Prediction"
paper_link: https://dl.acm.org/citation.cfm?id=1815971
paper_keyword: RRIP; Cache Replacement; LRU
paper_year: ISCA 2010
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper proposes RRIP, a machanism for determining whether a cache line should be replaced 
by predicting the interval from the current time till its next reference. The motivation for 
RRIP is locality anomaly, which is not uncommon for lower level caches, such as L2 and LLC.
The classical Least Recently Used (LRU) algorithm works pretty well, sometimes approximating
Belady's optimal OPT algorithm, for access patterns that have high locality, which is particularly 
true for L1 cache. On L2 and LLC, LRU may not work well for three reasons. First, most of the locality
has been filtered out by L1, and only those misses the L1 cache can be observed by L2 and LCC. This 
property violates the assumption of LRU that both a cache miss and cache hit imply accesses to the 
same line in the near future. Second, when the size of the working set is larger than the cache, 
*cache thrashing* can happen. The most typical example is accessing an array in a circular manner. 
All accesses will result in cache misses, because the LRU algorithm always selects array elements 
that are smaller for eviction, which are accessed earlier and hence are closer to the LRU position. 
The third reason is scan pattern, which accesses non-temporal blocks. Scan pattern also violates the 
assumption of LRU, because cache misses bring in data that will never be accessed in the near future. 
Overall speaking, LRU is not suitable for L2 and LLC caches as a replacement algorithm, as a consequence 
of different locality assumptions.

Cache replacement algorithms can be described using policies. Cache replacement policies define the state 
transition when particular events take place. For example, the insert policy defines the status of a loaded
and perhaps existing cache blocks when a block is loaded. Similarly, the hit policy and miss policy define 
the transition of states when a cache block is hit and when a miss is signaled respectively. In LRU, the 
insert and hit policy both put the 

To accommodate to the fact that in lower level caches, misses and hits do not necessarily imply future 
reuse, the insert, hit and miss policy should be altered from LRU to 