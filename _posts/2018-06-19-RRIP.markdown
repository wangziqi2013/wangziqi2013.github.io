---
layout: paper-summary
title:  "High Performance Cache Replacement Using Re-Reference Interval Prediction"
date:   2018-06-19 05:43:00 -0500
categories: paper
paper_title: "High Performance Cache Replacement Using Re-Reference Interval Prediction"
paper_link: https://dl.acm.org/citation.cfm?id=1815971
paper_keyword: RRIP; Cache Replacement; LRU
paper_year: ISCA 2010
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper proposes RRIP, a machanism for determining whether a cache line should be replaced 
by predicting the interval from the current time till its next reference. The motivation for 
RRIP is locality anomaly, which is not uncommon for lower level caches, such as L2 and LLC.
The classical Least Recently Used (LRU) algorithm works pretty well, sometimes approximating
Belady's optimal OPT algorithm, for access patterns that have high locality, which is particularly 
true for L1 cache. On L2 and LLC, LRU may not work well for three reasons. First, most of the locality
has been filtered out by L1, and only those misses the L1 cache can be observed by L2 and LCC. This 
property violates the assumption of LRU that both a cache miss and cache hit imply accesses to the 
same line in the near future. Second, when the size of the working set is larger than the cache, 
*cache thrashing* can happen. The most typical example is accessing an array in a circular manner. 
All accesses will result in cache misses, because the LRU algorithm always selects array elements 
that are smaller for eviction, which are accessed earlier and hence are closer to the LRU position. 
The third reason is scan pattern, which accesses non-temporal blocks. Scan pattern also violates the 
assumption of LRU, because cache misses bring in data that will never be accessed in the near future. 
Overall speaking, LRU is not suitable for L2 and LLC caches as a replacement algorithm, as a consequence 
of different locality assumptions.

Cache replacement algorithms can be described using policies. Cache replacement policies define the state 
transition when particular events take place. For example, the insert policy defines the status of a loaded
and perhaps existing cache blocks when a block is loaded. Similarly, the hit policy and miss policy define 
the transition of states when a cache block is hit and when a miss is signaled respectively. In LRU, the 
insert and hit policy both put the block at MRU position, which is the furthest possible position from LRU.
The miss policy simply evicts the block at LRU position. As stated above, the LRU policy works sub-optimally
for lower level caches, because the locality on these levels usually discourage predicting references for hit
and inserted blocks as to be in the near future.

RRIP fixes LRU by predicting newly inserted cache blocks to be accessed in the long future, therefore prioritizing 
them for victim selection on cache misses. To implement the policy, each cache block is associated with an M-bit 
saturating counter. In the paper M is chosen to be two. A value of zero indicates the block is expected to be accessed
in the near future. The larger the value is, the longer the predicted interval of future accesses will be, and hence 
the more likely the block will be chosen for victim selection. A newly inserted block will have a counter value of 
(2<sup>M</sup> - 1). On victim selection, the cache controller attempts to evict the block with counter value 2<sup>M</sup>.
If such a block does not exist, then the counters of all blocks are incremented by one, and the controller will repeat
until it finds one. On a cache hit, the controller can either reset the counter of the hit block to zero immediately, 
demonstrating strong belief that the block will be accessed in the near future, or act less aggressively and only
decrement the counter by one. The former is called Hit Proprity (HP) and the latter Frequency Priority (FP). 