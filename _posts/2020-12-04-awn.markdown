---
layout: paper-summary
title:  "Restrictive Compression Techniques to Increase Level 1 Cache Capacity"
date:   2020-12-04 19:45:00 -0500
categories: paper
paper_title: "Restrictive Compression Techniques to Increase Level 1 Cache Capacity"
paper_link: https://ieeexplore.ieee.org/document/1524171/
paper_keyword: Compression; L1 Compression
paper_year: ICCD 2005
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Lowlight:**

1. Although the compression and decompression algorithm is indeed simple, the paper ignores the fact that
   there are implicit complexities, such as finding a slot for insertion. In an uncompressed cache, this is
   as simple as selecting the bottom of the LRU stack. In our case, we also need to evaluate whether to evict
   a compressed line, or an uncompressed line. 
   One possible argument is that this is not on the critical path, so it does not directly affect access latency.
   It is still, however, likely that the excessive tag access will contend with normal access.

2. The paper also does not discuss how eviction candidates are selected. For example, when both compressed and 
   uncompressed slots exist in a set, and we are inserting a compressed line. Should we use the compressed line
   eviction logic, or use the uncompressed line eviction logic?
   We definitely cannot choose an arbitrary two compressed lines and evict, since they may not be in the same
   slot. It is also unlikely that compaction will happen across slots.

This paper proposes a restrictive cache compression scheme for improving L1 cache effective capacity.
Applying compression to L1 caches can help further improve performance and reduce energy consumption, since
more data blocks are cached by the L1, and less requests are sent to lower levels L2.
The paper points out at the beginning, however, that conventional compression algorithms are not feasible for L1
cache compression, since they introduce a few cycle's latency during compression and decompression. Even though
the absolute number of cycles can be small, they are still significant relative to L1 cache's access latency,
which is typically only a few cycles.

The paper observes that, all previous compression schemes change the bit offset of words in the compressed line, in a 
way such that they must be explicitly computed, incurring more cycles. 
In order to design a compression algorithm for L1, individual words in the compressed line must be able to easily 
recovered with random access, such that they can be fed into the pipeline immediately after the compressed line 
is read out. The paper calls this property as "restrictive" as it narrows down the design space to only a few 
possibilities.

The L1 cache architecture is described as follows. Each physical cache line slot is over-provisioned with an extra tag,
which supports up to 2x compression ratio. The extended tag contains at least an address tag, MESI state tag, and 
LRU tag. Besides, each physical slot also has one bit indicating whether compression is enabled for the slot. If
compression is enabled, the slot stores two lines compressed to half of the slot size, and each of the two tags 
describes a compressed line. Otherwise, compression is disabled on the slot, meaning that the slot stores an 
uncompressed line, and only one tag is active.

Compression is performed as follows. On inserting into the L1 cache, the compression circuit checks whether all bits in
the higher half of each word to be compressed equals the sign bit of the lower half. If true, the word is compressed to
half of its original size by eliminating higher bits. Otherwise, the word is uncompressable, which also renders the 
entire line uncompressable. 
The compression circuit is extremely simple: On the insertion data path, we add an array of 
comparators to check higher bits in parallel for all words, and then use a multiplexer to determine which version of 
data is output. The per-slot bit is also set, if the line is compressable, and the slot is currently empty (if the
slot already contains a compressed line, then just fill it in the slot).

Decompression is exactly the reverse of compression. If the cache line to be accessed by LSU is from a compressed slot 
(the per-slot bit is set to "1"), the byte offset into the uncompressed cache line is right shifted by one, which
is translated to the byte offset of the corresponding word in the compressed line. Then the half word at the byte offset
is read, and the original value is recovered by duplicating the sign bit.
Multi-word accesses can also be implemented similarly, except that the size to be is also right shifted by one.
The decompression circuit is even simpler: Right shift and sign extension can be performed by combinational logic 
without any shift register or states. The final result is delivered to the LSU using a mutiplexer, which selects from
the uncompressed and the compressed version using the per-slot bit.


