---
layout: paper-summary
title:  "2DCC: Cache Compression in Two Dimensions"
date:   2020-07-22 19:55:00 -0500
categories: paper
paper_title: "2DCC: Cache Compression in Two Dimensions"
paper_link: https://past.date-conference.com/proceedings-archive/2020/html/0897.html
paper_keyword: Compression; Cache Compression; 2DCC; Deduplication
paper_year: DATE 2020
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Two-Dimentional Cache Compression, 2DCC, a cache compression design leveraging both intra-line and
inter-line redundancy. This paper begins by pointing out that both intra-line and inter-line redundancy exist in the
working set of typical workloads, as cache lines, as a whole, tend to contain similar to identical content in addition 
to the more classical redundancy between individual words within a line. Conventional cache compression schemes,
however, only takes advantage of intra-line dependency, since it is most natural to process individual lines rather 
than combing several lines together, as cache lines are the most fundamental unit of data communication between components
in the memory hierarchy. On the other hand, [prior LLC deduplication schemes]({% post_url 2020-06-24-llc-dedup %})
proposes LLC deduplication using a hash table to perform fast detection of duplicated blocks. This scheme does not 
benefit from intra-line redundancy, which is also suboptimal.

2DCC combines conventional cache line compression and deduplication as follows. First, compression is still performed
on individual cache lines, and stored in a decoupled data array. Tags are over-provisioned, and is organized such that
one tag can map to an arbitrary data slot in the data array. Second, hash values of the most recent few lines are maintained
in a separate hash table, which is also organized as a set-associative lookup table. Cache lines written back from upper
levels and fetched from lower levels are hashed and compared with table entries. On a value and data match, the incoming 
cache line will not be stored. Instead, only a tag entry is allocated, and the tag points to the data slot that has already
been in the cache. Lastly, the paper also proposes a simple but yet effective replacement policy for the fully associative 
data array. Cache lines are evicted not based on recency of usage, but simply based on random sampling and sharer count, 
as we will see below.

The tag array is still organized as a classical set-associative array indiced using lower bits of the requested block 
address. The tag array is over-provisioned, such that either more sets are present, or each set has more entries
(i.e. higher associativity) than before. In addition to the conventional information such as valid/dirty bit, coherence
states and replacement status, each tag also contains a global data array pointer, and two global tag array pointers.
The data array pointer indicates the associated data slot for the tag entry, which must be accessed in a serialized
manner. 
