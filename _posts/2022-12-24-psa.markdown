---
layout: paper-summary
title:  "Page Size Aware Cache Prefetching"
date:   2022-12-24 23:48:00 -0500
categories: paper
paper_title: "Page Size Aware Cache Prefetching"
paper_link: https://ieeexplore.ieee.org/document/9923823/
paper_keyword: Prefetching; Set Dueling; Huge Page
paper_year: MICRO 2022
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Page Size Propagation Module (PPM), an extension to the existing spatial cache prefetchers that
enables prefetching on 2MB huge pages. The paper was motivated by the fact that 2MB huge pages are prevalent in 
most workloads, but the existing spatial prefetchers cannot prefetch across the regular 4KB page boundary.
The paper addresses this problem by allowing page size information to be passed by the MMU to the prefetcher in the 
low-level cache's prefetchers, such that the prefetcher can dynamically decide whether or not to prefetch across
4KB page boundaries. 

This paper focuses on improving spatial prefetchers, one of the two types of prefetchers that predict future
addresses for memory accesses based on past spatial access patterns (e.g., deltas). Compared with temporal 
prefetchers, which record histories 
of past cache misses and replay the accesses to fulfill the misses in the hope that future accesses will reproduce
the past pattern, spatial prefetchers have three apparent advantages.
First, spatial prefetchers require much less metadata, as it only maintains metadata for describing the pattern.
Temporal prefetchers, on the contrary, need to keep track of access traces in order to reproduce them.
Secondly, spatial prefetchers can prevent compulsory misses, since the predicted addresses do not have to occur 
in the previous execution history. By contrast, temporal prefetchers need to see the trace at least once in order to
reproduce them later, hence suffering compulsory misses when the address is accessed for the first time.
Lastly, spatial prefetchers also have better row buffer locality as the prefetching request is spatially 
close to the recent addresses being accessed. Temporal prefetchers, on the other hand, do not have such guarantees
and therefore exhibit a lower row buffer hit ratio, resulting in higher latency and energy consumption.

Despite the many advantages of spatial prefetchers, the paper identifies that one of the biggest problems of spatial 
prefetchers is their inability to prefetch across 4KB page boundaries in the past literature. Two factors contributed 
to such a uniform design decision. First, due to the regular 4KB granularity address mapping, pages that are consecutive
on the virtual address space are not necessarily consecutive on the physical address space. Consequently, in order to
prefetch across page boundaries on the physical address space (which is mandatory as lower-level caches are tagged
using physical addresses), the prefetcher must have access to translation metadata. However, translation metadata is 
usually only accessible to the L1 cache, since the TLB is physically located at that level.
Secondly, without translation metadata, speculatively allowing prefetchers to read past virtual page boundaries may
raise serious security concerns, as the prefetcher can become a side channel that breaks address space isolation
and leaks data not belonging to the current process.

Based on the above reasons, the paper investigated the feasibility of prefetching on huge pages, which is a 
feature supported by recent x86 architectures. With huge pages enabled, either the application programmer or the 
Operating System kernel can choose to map physical memory in bigger granularity, such as 2MB or 1GB (the latter
is only available using the `hugetlbfs` library). The paper also made several observations on the interaction
between huge pages and spatial prefetching. First, the paper evaluated several benchmarks with Transparent Huge 
Page (THP) enabled. Results show that most of the workloads can efficiently utilize huge page support during 
the entire execution, indicating the feasibility of performing prefetching at huge page granularity.
Secondly, the paper also conducted experiments by enabling the prefetcher to prefetch across 4KB page boundaries
when the page size allows with pattern information still maintained at 4KB granularity. The result shows that 
extra benefits can be gained on most workloads due to the increased opportunity exposed.
