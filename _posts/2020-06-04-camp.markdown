---
layout: paper-summary
title:  "Exploiting Compressed Block Size as an Indicator of Future Reuse"
date:   2020-06-04 12:19:00 -0500
categories: paper
paper_title: "Exploiting Compressed Block Size as an Indicator of Future Reuse"
paper_link: https://ieeexplore.ieee.org/document/7056021/
paper_keyword: CAMP; Cache; Compression
paper_year: HPCA 2015
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Compression Aware Management Policies (CAMP), which is a set of cache management policy specifically
tuned for compressed caches. Cache compression has been proposed by prior publications to reduce memory traffic as well
as to increase effective cache size. Its effectiveness, however, also depends on the cache management policy. Conventional
policies such as LRU, or, commercially adopted policies such as RRIP, may not be optimal for compressed caches. To solve
this issue, more factors need to be taken into consideration when designing management policies for compressed caches.

The paper makes two critical observations. First, due to the fact that compressed blocks are of different sizes, sometimes
significantly different, Belady's OPT cache replacement algorithm may no longer be optimal in terms of number of misses, 
if multiple cache blocks need to be evicted as replacement for a larger block. In this case, cache replacement policies 
that implicitly or explicitly relies on future reuse distances may no longer provide good performance numbers.
The second observation is that cache blocks of similar sizes may have a similar access pattern, because of loops in program
code and arrays in the data structure. 
For example, a nested loop iterating over array elements can generate regular access patterns to the array. 
In addition, arrays in program data structures typically contain data of the same type, sometimes even within a small 
dynamic value range, such as pointers to small objects allocated by a malloc(). Most malloc() implementations will
only allocate from a pool of larger memory blocks, and will optimize for locality. The resulting objects will be close 
to each other in the address space, and hence easily compressible to similar sizes. Another example is sparse matrix
in which most elements are zero. In such a matrix, most cache lines will be compressed to contain only one or two non-zero
values, with the rest being easily compressible. 