---
layout: paper-summary
title:  "Exploiting Compressed Block Size as an Indicator of Future Reuse"
date:   2020-06-04 12:19:00 -0500
categories: paper
paper_title: "Exploiting Compressed Block Size as an Indicator of Future Reuse"
paper_link: https://ieeexplore.ieee.org/document/7056021/
paper_keyword: CAMP; Cache; Compression
paper_year: HPCA 2015
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Compression Aware Management Policies (CAMP), which is a set of cache management policy specifically
tuned for compressed caches. Cache compression has been proposed by prior publications to reduce memory traffic as well
as to increase effective cache size. Its effectiveness, however, also depends on the cache management policy. Conventional
policies such as LRU, or, commercially adopted policies such as RRIP, may not be optimal for compressed caches. To solve
this issue, more factors need to be taken into consideration when designing management policies for compressed caches.

The paper makes two critical observations. First, due to the fact that compressed blocks are of different sizes, sometimes
significantly different, Belady's OPT cache replacement algorithm may no longer be optimal in terms of number of misses, 
if multiple cache blocks need to be evicted as replacement for a larger block. In this case, cache replacement policies 
that implicitly or explicitly relies on future reuse distances may no longer provide good performance numbers.
The second observation is that cache blocks of similar sizes may have a similar access pattern, because of loops in program
code and arrays in the data structure. 
For example, a nested loop iterating over array elements can generate regular access patterns to the array. 
In addition, arrays in program data structures typically contain data of the same type, sometimes even within a small 
dynamic value range, such as pointers to small objects allocated by a malloc(). Most malloc() implementations will
only allocate from a pool of larger memory blocks, and will optimize for locality. The resulting objects will be close 
to each other in the address space, and hence easily compressible to similar sizes. Another example is sparse matrix
in which most elements are zero. In such a matrix, most cache lines will be compressed to contain only one or two non-zero
values, with the rest being easily compressible. 

The above two observations motivate CAMP, which consists of two policies: Minimum Value Eviction (MVE), which is an
improved version of existing eviction policies, and Size-based Insertion Policy (SIP), which determines the priority
of blocks for eviction when they are fetched into the cache. These two policies complement each other. The initial priority
for eviction when a line is brought into the cache is determined by SIP, while the replacement decision is made using a 
combination of the eviction priority and other factors (as we will see below). Eviction prioirties are also updated 
when cache blocks are hit.

In the baseline version of MVE, the paper assumes a conventional set-associative cache organized into sets and ways. 
No particular replacement algorithm is assumed, but the algorithm must use a priority value to rank all ways in a set
as candidates, and the one with the highest priority value is evicted. In LRU, this value corresponds to the distance
of the way in LRU stack to the top of the stack (i.e. the one at the tail of the stack has the highest priority). 
In RRIP, the priority value is the re-reference prediction value, RRPV. On eviction, the MVE computes a value Vi for
each way in the set using the formula: Vi = pi / si, where pi is the inverse of the priority, while si is the size
of the compressed block corresponding to the tag. 
This formula discourages the eviction of blocks with smaller sizes, since several smaller blocks may need to be evicted
in order to serve one large, hardly compressed block, which reduces the effective cache size.
Note that the value pi actually denotes the "importance" of the block.
In LRU, it is the distance of the block in the LRU stack to the tail of the stack, while in RRIP, it is computed by
subtracting RRPV from the maximum RRIP counter value. 
After computing the Vi for each i in the set, the cache controller evicts the one with the smallest Vi. 

SIP works by setting up one or more "virtual caches" which is driven by accesses to a small subset of sets in the main cache. 
Each virtual cache runs a ranking algorithm with different parameters (i.e. which size class should be given a higher priority).
The virtual cache does not address any data slot. Instead, it only serves as a "dry run" of the intended ranking algorithm, 
and maintain statistics.

