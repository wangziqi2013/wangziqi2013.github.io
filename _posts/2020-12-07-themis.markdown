---
layout: paper-summary
title:  "(Almost) Fence-Less Persistent Ordering"
date:   2020-12-07 18:12:00 -0500
categories: paper
paper_title: "(Almost) Fence-Less Persistent Ordering"
paper_link: https://www.microarch.org/micro53/papers/738300a539.pdf
paper_keyword: NVM; Persistent Barrier; Themis
paper_year: MICRO 2020
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Themis, a lightweight persistent ordering architecture for NVM applications. The paper begins by
observation that most NVM applications, if not all, uses persistence barrier to guarantee write ordering. Using undo
logging as an example, the log entry must reach the NVM before dirty data does, because otherwise, if the system
crashes before the two persistence operations, dirty data would not be able to recovery to the pre-image before
the transaction, corrupting program data.

The paper then claims that persistent barriers are detrimental to performance, mainly because: (1) They require a 
cache line flush followed by memory fence, which prevents any possible coalescing and reordering of NVM writes;
(2) The sfence will prevent instructions after it from committing in the ROB, which stalls the pipeline given the
relatively long latency of NVM writes. 

The paper also makes a few observations on common usage patterns with persistence barriers. First, non-temporal stores
(e.g., ``movnt``) bypasses the cache hierarchy, and can therefore be used to write log entries to the NVM. The advantage
of using non-temporal stores is to avoid bringing the address used for logging into the cache, causing cache pollution,
since the log buffer will never be read again during normal operation.
Second, the persistence barrier works equally for both temporal stores (i.e., regular ``mov``) and non-temporal stores,
except that the cache line flush is unnecessary for non-temporal stores.
The ``sfence`` instruction can be employed to order non-temporal and temporal stores without distinction.
Third, non-temporal stores typically reach the memory controller faster, since they are generated earlier, and they 
have shorter data paths. To elaborate: A non-temporal store will be directly sent to the memory controller after
they are generated, while a temporal store followed by cache line flushes need to go through the entire cache hierarchy,
which costs more cycles. 
Lastly, modern memory controllers are in the persistence domain, thanks to the Asynchronous DRAM Refresh (ADR) feature.
Once a memory request reaches the memory controller's queue, they are guaranteed to be persistent even on power losses.
It is, therefore, sufficient to declare that an NVM write has been persisted once the processor received the ACK
packet from the memory controller.

The paper assumes the following architecture. The non-temporal stores are handled by a write-combining buffer (WCB)
connected to the Load-Store Unit (LSU). The LSU directly puts the request into the WCB without invoking coherence
(it is unclear, however, whether the L1 cache will be updated if the block to be written is already in L1 with
sufficient permission. This does not affect the correctness of the design, though.).
Write requests in the WCB are coalesced (writes to the same cache line on different locations are combined into a 
single request), combined (writes to the same location in the same line are also combined) and reordered for better
write performance. Once the arbitrator grants bus access, and the memory controller indicates that it could accept
more requests, the WCB controller sends a request to the controller, and wait for the ACK.
