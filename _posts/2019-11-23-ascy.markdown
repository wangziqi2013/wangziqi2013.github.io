---
layout: paper-summary
title:  "Asynchnized Concurrency: The Secret to Scaling Concurrent Search Data Structures"
date:   2019-11-23 23:41:00 -0500
categories: paper
paper_title: "Asynchnized Concurrency: The Secret to Scaling Concurrent Search Data Structures"
paper_link: https://dl.acm.org/citation.cfm?id=2694359
paper_keyword: Concurrency; Hash Table; ASCY
paper_year: ASPLOS 2015
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Asynchronous Concurrency (ASCY), a programming principle for designing concurrent data structures.
The paper first identifies that the major source for scalability issues in concurrent data structures are store instructions
to shared data which will cause these stores be serialized by the cache coherence protocol. In addition, without careful
evaluation of these stores, they may also incur long-than-usual dynamic code path (e.g. in the form of retry), which 
negatively affects both overall performance and power dissipation. In addition, the paper also points out that optimizations
to concurrent data structures are often limited to certain architectures and to certain workloads. If the optimized
data structure is evaluated on a different platform or using a different set of workloads, they may even perform worse
than unoptimized version. One of the example given by the paper is Read-Copy-Update, which is a commonly used concurrent
data structure technique in Linux kernel. The paper points out that RCU is designed for read-dominant workloads, in which
writes are very rare compared with reads. If the ratio of writes increase, the performance of RCU will drop sharply.

The paper proposes that concurrent data structures be evaluated on multiple platforms, workloads, and on several
metrics. Metrics used by this paper include latency of operations, throughput of operations, power consumption, and 
distribution of latencies (tighter distribution implies more predictable performance). Furthermore, the paper also proposes
that non-concurrent version of the data structure under evaluation can be used as an upper bound of performance to
demonstrate the cost of synchronization. As reported by the paper, a "good" implementation is able to achieve less than
10% performance loss compared with the non-concurrent counterpart. 

The paper then shows the four rules of ASCY using common data structures as examples. The first rule of ASCY suggests that
operations that are semantically read-only (i.e. key lookup, range scan, etc.) should neither write to shared memory, nor
block on a lock or retry. Here "write" includes acquisition and release of a lock, help-along other threads to complete 
an operation, and also updating states in shared objects. This rule implies that read should be lock-free, non-blocking,
and should not help other threads even if the concurrency protocol allows transient states be fixed by all threads 
observing the state. To achveie this, updates on data structure objects must only transform the object from one consistent 
state to another, without the possibility of observing inconsistent partially updated values. In addition, reader threads
should be able to figure out such transient states when they see one, and recover the consistent image of the object locally
without helping along. In practice, some data structures, such as Harries linked list as pointed out by the paper, requires
that reader threads fix transient states by physically removing nodes that are removed from the list using CAS. If the removal
fails, the thread should retry traversal from the list head. The paper also shows 10% -- 30% performance improvement after 
optimizing out the help-along for reader threads. 

The second rule of ASCY