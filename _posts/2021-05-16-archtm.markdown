---
layout: paper-summary
title:  "ArchTM: Architecture-Aware, High Performance Transaction for Persistent Memory"
date:   2021-05-16 21:26:00 -0500
categories: paper
paper_title: "ArchTM: Architecture-Aware, High Performance Transaction for Persistent Memory"
paper_link: https://www.usenix.org/system/files/fast21-wu-kai.pdf
paper_keyword: NVM; STM; ArchTM
paper_year: FAST 2021
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Highlights:**

1. Object-level shadowing for atomic durability, and two version MVCC w/ locking for transactional synchronization.



**Comments:**

1. The paper did not mention how to expand the table when new object IDs are larger than table size. The simplest
   way is to use mmap() to allocate a huge array without backing memory.

2. Why do you need both ownership record in table entry and the write lock? The ownership record itself can be used
   as write lock. NULL means unlocked, while any non-NULL value means locked, and the owner of the object is the
   record pointer which points to the persistent transaction descriptor.

This paper presents ArchTM, a durable software transactional memory design for byte-addressable non-volatile memory.
The paper is motivated by the discrepancy between previous NVM transactional framework proposals that use emulated or 
simulated NVM device and the actual hardware performance characteristics of commercial NVM device.
Previous designs tend to overestimate write performance and underestimate the write granularity for NVM devices due
to lack of precise information and using throttled DRAM device for emulation.

The actual NVM device, however, demonstrates different characteristics than DRAM in the following way.
First, NVM devices generally sustain much lower write throughput than read throughput. Writing large amount of 
data into the device, therefore, will severely degrade performance.
This discourages putting non-essential metadata on the NVM since metadata will be updated frequently.
Second, NVM devices exhibit optimal performance on multiples of 256B writes. This is caused by its internal
write combining buffer whose size is exactly 256 bytes. Writes belonging to the same 256 byte block will likely
be combined in the buffer, without accessing the physical storage medium.
This implies that application should refrain from writing small data items to randomly addresses; In other words,
NVM devices prefer large and consecutive writes in order to achieve optimal performance.

ArchTM leverages the above observations to achieve better transactional performance.
First, ArchTM adopts shadowing as the persistence protocol for atomic durability. Conventional logging-based
schemes at least doubles the write traffic to the NVM since the same data item must be written twice, once for 
the log, and second time for committed data. Besides, logging requires excessive persist barriers consisting of 
cache line flushes and store fences, which can also degrade performance.
ArchTM, on the contrary, copies an object when it is to be modified within a transaction for the first time, such that
both the old and new version are present. The consistent memory image is still valid as the old object is preserved.
This requires less write traffic and less persist barrier, since only objects modified during a transaction
are flushed at transaction commit point.

Second, ArchTM reduces non-essential and small, random NVM writes by maintaining object-mapping metadata in the DRAM, 
since metadata update may happen far more frequently than data updates, and they usually only write a few words.
The object mapping will be lost during a crash, and rebuilt during recovery by scanning allocated segments, thanks
to the per-object annotation that we discuss later.
Similarly, memory allocator metadata is also maintained in the DRAM. Allocated and free blocks are recognized
during the after-crash scan by reading the annotation on the block header.

Lastly, ArchTM optimizes memory allocation such that small objects are allocated from a single free list,
rather than from several size classes. Despite increased fragmentation when these objects are freed, 
this is justified by the observation that objects allocated together tend to be written together, increasing
the write locality, making it easier for the device to combine writes.

Overall, ArchTM is a software transactional memory framework that follows the conventional transactional interface.
Here, "transaction" means two independent semantic guarantees. First, reads and writes performed in different 
transactions are synchronized with snapshot isolation (SI) semantics, meaning that each transaction observes a 
consistent snapshot at transaction begin time, and successfully commits if no earlier transaction commits
on its write set. 
Second, transactions are also basic units for atomic durability, meaning that the memory updates made by a  
transaction are either all written to the NVM, or none of them is. This is important for data consistency, since
writes that transit the system from one consistent state to another are most likely non-atomic to the NVM device.
ArchTM is also object based, and objects must be "opened" before it is accessed for read or write.
Objects in ArchTM are accessed using a special handler called the object ID, which is an abstract representation
of the object instance pointer. 
Object IDs are indices to an object mapping table, the entry of which stores pointers to old and new version of the 
object for shadowing purposes. 

The framework spawns worker threads to execute given transactional functions. Each worker thread maintains pre-thread
metadata such as object allocation list and free list (on DRAM). 
The framework also spawns background threads for garbage collection and defragmentation.

We next describe data structures. Each thread has a thread-local transaction descriptor that stores the current status
(not active, active, committed) of the transaction, the begin timestamp and the commit timestamp. The two timestamps
are distributed by a global hardware counter (e.g., x86's RDTSC), which serve as logical timestamps for transaction
begin and commit. When a transaction is not active, these two timestamps are considered as invalid regardless of the 
actual value. 

The object mapping table in the DRAM maps object ID to the pointer of the instance. The table must only be accessed
within a transaction, using the object ID and the transaction's begin timestamp.
The mapping table is a direct-mapped, linear array, and is indexed directly using the object ID. 
Although the paper did not mention the concrete implementation, the table could be allocated as a huge array on the
virtual address space (which has 48 valid bits) without any backing physical page. Table entries are demand-paged 
in as the table expands.
Each entry of the table consists of two pointers to old and new versions of the object, an ownership record that 
contains a pointer to the transaction descriptor, and a write lock. 
The write lock is acquired before the object is accessed for writes, and not locked for reads.

Object IDs are allocated as a new instance of any object is created. The ID allocator is just a global ID counter
and a free list. Freed objects will return their IDs back to the free list for recycling.
The object ID allocator is maintained by the memory allocator, such that when a new object is allocated, a new
ID is also allocated. The memory allocator then registers the object to the table by writing the pointer 
value to the table entry (the index of which is the object ID), and clearing all other fields. The allocator 
returns the object ID as a handler. All object accesses must pass the handler to the corresponding access functions.

Memory allocators also maintain volatile metadata in the DRAM. The paper proposes two different allocation strategies.
For large objects, the usual jemalloc is used, and objects are allocated from a thread-local size-specific free list.
Smaller objects, however, are allocated from a simple per-thread memory segment like a log. Objects are stored 
compactly within the segment for better write locality. 
Per-thread memory segments are allocated from a global pool of segments.
In the segment-based allocator, free objects are not returned to the segment since the allocator will not be able to
recycle them anyway. Instead, they are inserted into a per-thread free list, which is responsible for recycling
freed blocks (the simple allocator searches the free list before allocating a new object on the segment).
The per-thread free list is also periodically merged into a global free list for balancing reasons, i.e., it 
balances free blocks between threads that performs more allocation than deallocation and threads that does the opposite.
(**I am not too sure about whether this paragraph precisely reflects what the paper intends to convey. 
I just described what I though was the best strategy. The paper may actually
say otherwise. In practice, this can be implemented in several different ways.**)

Since all allocation metadata is in the volatile memory, the object layout in the NVM will be lost on a crash.
To enable the recovery handler to recognize valid objects after a crash, the allocator always annotates objects
with a special header that contains the object ID, the transaction ID that last writes it, and the object size.
Both allocated and free objects must possess this header, such that a memory segment can be scanned by 
linearly by adding the size in the current annotated header to the starting address of the object to compute the
header address of the next annotated header. 

Initially, a segment is treated as one monolithic free object with the size being the size of the segment minus
the header size. When a new object is to be allocated on the free space (free space is treated as a free object), 
if the allocation size matches the size of the free space, its header is updated with object info, persisted
by a barrier, and allocation completes. Otherwise, the free space is split into two by first writing a new header
at the split point with size being the size of the free space minus allocated size, persisting the new header with
a barrier, and then updating the existing header with object info using a second barrier.
This way, object headers on the NVM image is always consistent with the actual allocation map, which guarantees 
that all allocated objects can be found with one linear scan, and that no memory leak would occur, since objects 
are allocated atomically with the persistence of the annotated header.
(**The paper did not go through such details. I guess this is what is actually happening.**)
