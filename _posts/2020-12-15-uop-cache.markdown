---
layout: paper-summary
title:  "Improving the Utilization of Micro-Operation Caches in x86 Processors"
date:   2020-12-15 06:37:00 -0500
categories: paper
paper_title: "Improving the Utilization of Micro-Operation Caches in x86 Processors"
paper_link: https://www.microarch.org/micro53/papers/738300a160.pdf
paper_keyword: Microarchitecture; uop cache
paper_year: MICRO 2020
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Lowlight:**

1. Not really a low light, but the paper suggests that the uop cache is addressed using the base address of the PW.
   But a PW can be stored in multiple entries due to limitations or i-cache line boundaries, how are these multiple
   entries addressed individually even if they are in the same PW?
   My best guess is that they still use the actual physical address of the instruction that corresponding to the 
   first uop in the entry, but each entry also indicates the address of the next instruction.

This paper proposes two techniques for optimizing the performance of micro-operation (uop) cache on x86 processors.
The uop cache is an important component on the frontend pipeline, since it stores and feeds decoded uops to the 
backend without involving the instruction cache and the decoder. This reduces both pipeline depth, instruction feed
latency, and energy consumption, which all help in performance improvement.
Current implementation of the uop cache suffers from external fragmentation, which originates from the fact that uop
cache entries are variably sized. A fragmented uop cache negatively affects performance, since it reduces the effective 
size of the cache, which can further stall the backend pipeline due to insufficient instruction bandwidth.

The paper assumes a uop cache architecture as follows. The uop cache itself is implemented just like a regular cache,
with ways and sets and fixed size (64 bytes) slots. Uops, once they are generated by the instruction decoder, are 
first aggregated in an accumulation buffer, after which they are inserted into the uop cache as individual entries.
One uop cache entry consists of a stream of uops translated from consecutive instructions. 
One of the most important features of uop entries is that they are variably sized, i.e., entries do not always consist
of a fixed number of uops, despite the fact that uops are of equal length for easier processing, just like RISC
instructions.
These entries are sub-sequences of actual dynamic uop streams that are executed. An uop cache entry starts when a
branch instruction changes the control flow to a target instruction, or when a previous entry has been formed and
a new entry starts naturally. A uop entry ends on several conditions: (1) When the uop stream of the entry come from
instructions in a different i-cache slot, i.e., the decoder terminates the current entry when the current i-cache
entry is exhausted. This is to avoid mapping more than one i-cache entries to the same uop cache slot, in which case,
when the i-cache invalidates entries due to self-modifying code, the uop cache needs to be searched extensively for
all occurrances of uops that correspond to the cache line being invalidated; (2) When a taken branch uop occurs. 
The branch decision is an output from the branch predictor, and is used by the accumulation buffer to delimit
uop cache entries; (3) When the maximum number of uops, certain fields (immediate values, offset values) are
reached, or when the entry is already full.


