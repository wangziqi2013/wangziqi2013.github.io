---
layout: paper-summary
title:  "Improving the Utilization of Micro-Operation Caches in x86 Processors"
date:   2020-12-15 06:37:00 -0500
categories: paper
paper_title: "Improving the Utilization of Micro-Operation Caches in x86 Processors"
paper_link: https://www.microarch.org/micro53/papers/738300a160.pdf
paper_keyword: Microarchitecture; uop cache
paper_year: MICRO 2020
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Lowlight:**

1. The paper never gave a precise definition of a PW, while suggesting that the uop cache is addressed using the base 
   address of the PW. This raises many questions:
   (1) A PW can be stored in multiple entries due to limitations or i-cache line boundaries, how are these multiple
   entries addressed individually even if they are in the same PW?
   Shouldn't they have address conflict, if all entries are tagged with the same PW base address?
   My best guess is that they still use the actual physical address of the instruction that corresponding to the 
   first uop in the entry, but each entry also indicates the address of the next instruction.
   (2) How are indices generated from uop cache entry address to sets?
   In order for CLASP to work, it must be guaranteed that entries in the same PW but delimited by cache line
   boundaries be mapped to adjacent sets in tne uop cache.
   This suggests a different addressing scheme than the paper states.

This paper proposes two techniques for optimizing the performance of micro-operation (uop) cache on x86 processors.
The uop cache is an important component on the frontend pipeline, since it stores and feeds decoded uops to the 
backend without involving the instruction cache and the decoder. This reduces both pipeline depth, instruction feed
latency, and energy consumption, which all help in performance improvement.
Current implementation of the uop cache suffers from external fragmentation, which originates from the fact that uop
cache entries are variably sized. A fragmented uop cache negatively affects performance, since it reduces the effective 
size of the cache, which can further stall the backend pipeline due to insufficient instruction bandwidth.

The paper assumes a uop cache architecture as follows. The uop cache itself is implemented just like a regular cache,
with ways and sets and fixed size (64 bytes) slots. Uops, once they are generated by the instruction decoder, are 
first aggregated in an accumulation buffer, after which they are inserted into the uop cache as individual entries.
One uop cache entry consists of a stream of uops translated from consecutive instructions. 
One of the most important features of uop entries is that they are variably sized, i.e., entries do not always consist
of a fixed number of uops, despite the fact that uops are of equal length for easier processing, just like RISC
instructions.
These entries are sub-sequences of actual dynamic uop streams that are executed. An uop cache entry starts when a
branch instruction changes the control flow to a target instruction, or when a previous entry has been formed and
a new entry starts naturally. A uop entry ends on several conditions: (1) When the uop stream of the entry come from
instructions in a different i-cache slot, i.e., the decoder terminates the current entry when the current i-cache
entry is exhausted. This is to avoid mapping more than one i-cache entries to the same uop cache slot, in which case,
when the i-cache invalidates entries due to self-modifying code, the uop cache needs to be searched extensively for
all occurrances of uops that correspond to the cache line being invalidated; (2) When a taken branch uop occurs. 
The branch decision is an output from the branch predictor, and is used by the accumulation buffer to delimit
uop cache entries; (3) When the maximum number of uops, certain fields (immediate values, offset values) are
reached, or when the entry is already full.

The uop cache is indexed using the physical address of the prediction window (PW), which is a consecutive range
of instructions that the branch predictor has predicted to be executed next. A PW can be divided into multiple 
uop entries, and stored in the uop cache individually. The paper makes an critical observations that, if two
uop cache entries are from the same PW, representing a consecutive uop stream, but they are delimited by i-cache 
line boundaries, these two entries must be mapped to adjacent sets in the uop cache (although the paper never
elaborates the index generation scheme for uop caches).
