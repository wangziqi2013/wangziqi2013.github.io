---
layout: paper-summary
title:  "FPTree: A Hybrid SCM-DRAM Persistent and Concurrent B-Tree for Storage Class Memory"
date:   2019-11-17 15:35:00 -0500
categories: paper
paper_title: "FPTree: A Hybrid SCM-DRAM Persistent and Concurrent B-Tree for Storage Class Memory"
paper_link: https://dl.acm.org/citation.cfm?id=2915251
paper_keyword: NVM; B+Tree
paper_year: SIGMOD 2016
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper presents FPTree, a B+Tree designed for byte-addressable non-volatile memory. This paper points out, at a high 
level, several challenges of implementing data structures for NVM. First, the data structure should be able to recover to
a consistent state after a crash. This is typically achieved by flushing certain data back to the NVM to enforce write
ordering between metadata (commit mark, log entry, etc.) and data using persistence barrier. Considering that NVM writes
are only atomic on 8 byte aligned words at the CPU side, and that data persistency is only atomic on cache line granularity
at the NVM side, partial writes might occur after a crash, if the crash happens during or after the write. There are three 
common patterns for ensuring consistency. The first pattern is writing data into invalid area (assumed to be initialized
to zero). This happens when inserting elements into a B+Tree node. In this case, we first acquire storage for the write, 
and then perform write. A valid mark is set only after the write completes and is flushed back to the NVM. The valid mark 
can be appended after the write as a log, or be set in a separate header structure. A second flush on the valid mark commits 
the write operation. Since the mark can be set with only 8 byte atomic writes, the commit operation is also atomic with 
regard to failures. The second pattern is writing data into invalid area, but allows data to be invalidated (deleted) before 
they can be written into again. This corresponds to insert and delete operation on a tree node. The first half (insertion)
is exactly the same as the previous pattern. The second half can be easily achieved by resetting the valid mark during 
the first half to a value indicating deleted entry, and then flushing the mark, which commits the delete operation. 
Garbage collection is required to reclaim deleted entries to avoid fragmentation. In the third pattern, an entry is first
inserted, which is handled as in the first pattern, and then updated without being deleted. If the update can be done with
8 byte atomic write, then we simply perform the update and then flush the value. Otherwise, we write a redo write-ahead 
log to describe the operation, flush the log, after which the actual content is updated in-place. The log can only be 
removed after we flush the in-place update back to the NVM. During recovery, the log entries are first processed by replaying
the operations recorded in the log.

The second challenge is data recovery. This paper assumes using direct access (DAX) provided by the file system to expose
the NVM address space to users using virtual memory. Users need to call mmap() on the NVM file, which allocates a range 
of virtual addresses that are mapped to the corresponding physical pages that are used to store the content of the file. 
DAX provides better control over access permission and storage management, since the file system only exposes the part of 
physical addresses that are used to store the file to users. Unrelated NVM storage is protected by the MMU. When users 
expand the NVM file, the file system allocates new pages from the NVM address space, and adds them to the file inode. 
Compared with conventional DAX implemented on block storage, DAX on NVM does not use damand paging and rely on page faults
to bring file contents into a memory buffer before access. Instead, the OS directly maps the requested virtual addresses
to physical pages on the underlying NVM hardware. Furthermore, fsync() on NVM DAX file no longer flushes the buffer. Instead,
we issue flush instructions on every cache line of the affected page to ensure that the page is persistent on NVM.
This process, however, does not guarantee that the same NVM file can always be mapped to the same virtual address on each
map, as the virtual address space can be occupied by libraries and/or other DAX files, reaulting in virtual address pointers
being invalidated. To preserve the semantics of pointers, this paper proposes using DAX-aware pointers, which is a 16 byte
pair storing the file ID and offset within the file. No special compiler instrumentation is needed, as the pointer
format is only used within the B+Tree. Users always observe normal virtual address pointers in the rumtime. 

The third challenge is memory allocation. Memory blocks already allocated to the application should not be re-allocated
after the crash due to incorrectly recovered metadata. Similarly, memory blocks should always be tracked by either 
the application or by the allocator. Failure to observe the second requirement results in memory leak, which is harder
to resolve than in volatile memory, since memory leaks are also persistent. Prior researches propose to change the 
allocator interface to enable atomic ownership transfer: On memory allocation, the address of the target pointer
for receiving the allocated block is passed as an argument. The allocator ensures that either the memory block
is allocated after recovery and the target word stores the address of the block, or that the block is not allocated,
and the target word's value does not change. The same applies to deallocation: On memory free, the address of the 
target word holding the block to be freed is passed as an argument. The allocator should ensure that either the 
block is returned after recovery and the target word is set to NULL, or that the block is not returned, and the 
target word's value unchanged. The memory allocator can internally achieve this using redo logging. 

We now describe FPTree node layout as follows. Internals nodes of FPTree are volatile and never persisted. The memory
for internal nodes are allocated by regular memory allocator in volatile address space. The node consists of three
parts: A counter recording the number of entries, an array of sorted keys, and an array of values corresponding to the 
keys. Internal nodes are always sorted and updated in-place like regular B+Tree nodes. No lock field is needed for 
thread synchronization, since FPTree worker threads wrap the critical section in hardware transactions (TSX). Leaf nodes
of FPTree have more fields than interna nodes for three reasons. First, updates to leaf nodes must be persisted
back to the NVM, which is slower than internal node updates. Leaf nodes therefore use log-structured update without 
enforing key ordering. In other words, updates to leaf nodes simply append the key and value to the next available
storage slot, flush the content of the slot, and sets a "valid" bit in the node header bitmap. Second, since keys are
not sorted in leaf nodes, node search requires scanning the leaf node. To reduce the extra cost of node scan, a "fingerprint"
field is added to help locating entries within the node. The fingerprint field stores 64 1-byte key hashes of all valid 
keys in the leaf node, supporting a maximum node capacity of 64 entries. The fingerprint field is also aligned to cache 
line boundary, such that it can be read from NVM using one bus request, and then checked against the hash of the search 
key with SIMD instructions efficiently. Analysis shows that hash collision within a node is extremely rare, such that
on average only one extra probe is needed to determine whether the key truly exists. Thirdly, since only leaf nodes
are persisted on NVM, on recovery, we must be able to find all leaf nodes, and then rebuild inner levels. To achieve
this, all leaf nodes include a "next node" pointer that points to the address (relative address) of the next node. 
A pointer to the first node is stored on a well-known location to help recovery process finding the leaf chain.
Fianlly, A "lock" bit in leaf nodes is used to synchronize update operations on the leaf. Threads first traverse 
to the leaf node using HTM, lock the leaf node, and commits the HTM transaction. This can be thought of as a 
coarse grained hand-over-hand locking, as the entire upper level (non-leaf levels) traversal is made atomic
with HTM. 

We next describe basic operations as follows. We begin with reads. Read operations (and also the traversal stage of 
insert, delete, update) use hardware transactional memory to ensure atomicity. The paper recommends using the speculative 
lock construct in Intel TBB as a mature solution. The speculative lock will first attempt to execute the critical
section in HTM mode, and falls back to software mode if HTM transactions aborts repeatedly or because of unrecoverable
reasons. The software mode uses a single global lock to synchronize all threads on the tree. As discussed in the 
previous paragraph, the advantage of using HTM is ease of programming and verification. The tree traversal procedure
can be thought of as a coarse grained hand-over-hand lock coupling, in which inner level traversal is made atomic
by the speculative lock without having to pay the serialization overhead of using a single "upper level" lock due to
fine grained conflict detection privided by HTM. 

Tree traversal begins at root level after acquiring the speculative lock, and ends at leaf level like regular tree 
traversals. During the traversal, any attempty to modify inner nodes will conflict with traversal threads if the inner 
node under modification is also in the read set. Once leaf node is found, the thread first checks whether the lock
bit in the leaf node is set. If true, it aborts the transaction, since a concurrent update is modifying the leaf node,
which makes it unsafe to read. The read operation also adds the leaf bit into the read set of the transaction, such
that any later acquisition of the lock will force the transaction to abort, serializing the node read operation with
concurrent node updates (all node updates will acquire the lock physically). The thread then performs key search within 
the node by reading in the fingerprint field, checks the hash of the search key against the 1-byte fingerprints, and 
probes the actual key slot(s) if there is one or more matches. A key is found if the key values match and the bitmap
indicates a valid key. The value is returned after the critical section is committed.