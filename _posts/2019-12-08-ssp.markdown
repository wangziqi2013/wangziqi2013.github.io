---
layout: paper-summary
title:  "SSP: Eliminating Redundant Wrints in Failure-Atomic NVRAMs via Shadow Sub-Paging"
date:   2019-12-08 22:01:00 -0500
categories: paper
paper_title: "Optimizing Systems for Byte-Addressable NVM by Reducing Bit Flipping"
paper_link: https://dl.acm.org/citation.cfm?id=3358326
paper_keyword: NVM; SSP; Shadow Mapping; Double Buffering
paper_year: MICRO 2019
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes SSP, a hardware framework for achieving persistent transactions on byte-addressable non-volatile 
memory system. The paper points out that existing software schemes, such as logging, shadow mapping, and log-structured
update, are all inefficient in terms of both latency and throughput. For example, in the most two common logging schemes,
redo logging and undo logging, old or new data must be written and flushed back to the NVM before the corrsponding location 
can be updated in-place to avoid dirty data cache line being evicted back to the NVM before the log entries are persisted. 
This introduces two problems. The first problem is write amplification, since data is written twice (if we consider 
metadata writes and log truncation overhead, the overhead is even larger). The second problem is that the write ordering
between log entries and in-place updated data is critical for correctness. Programmers must issue a software write barrier
which stalls the processor until previous stores are accepted by the memory controller every time write ordering is desired. 
For shadow mapping, one of the biggest concerns is write amplification, which can be significant when the amount of dirty
data per tracking unit (e.g. 4KB pages) is small. Even if only a few cache lines are updated per page, the original
page must be duplicated before update is conducted. In addition, shadow paging requires that every load and store to 
NVM area be translated by a translation layer, which identifies the current active version to be updated, and re-writes
the target address for the memory operation. On commit, all dirty contents are written back to the in-memory shadow page, 
and the stable version is switched from the old page to the new page atomically. This can be achieved by software 
instrumentation provided by the compiler, or by a hardware mapping structure. Both will add the overhead of address translation
and metadata maintenance to the critical path. Log-structured updated leverages the fact that NVM performs the best for 
sequential writes, and that atomic multi-word update can be achieved in an append-only manner by first appending data
to the end of the log, and then switching the pointer to the log tail with an atomic 8-byte write. Updated are not committed
until the atomic write is flushed back to the NVM. To facilitate runtime address computation, log-structured systems maintain
a virtualized linear address space, in which addresses are mapped to offsets into the log by a volatile mapping table.
The mapping table is updated every time an address is updated and appended to the end of the log. On a recovery, we rebuild 
the mapping table by scanning the entire log without having to recover the potentially inconsistent image from the NVM
(the table is in volatile DRAM for lower access latency). The major concern of log-structured system is the overhead of 
garbage collection, resulting in unnecessary data movement and data fragmentation.

SSP is based on shadow paging that we described above, but overcomes the problem of write amplification by using finer 
granularity tracking and hardware assistance for address remapping. The old page is not necessarily duplicated into the 
buffer before updates can be applied, since SSP tracks shadow pages in cache line granularity. At a high level, each virtual 
address mapped to NVM region is backed by two physical pages, page 0 and page 1. Page 0 is mapped by the OS naturally as 
in any virtual memory system, while page 1 is assigned to the virtual page by hardware (selected from a pool of free physical
pages on the NVM device). At any given moment, the current consistent image of memory can reside in both pages at cache 
line granularity, which is encoded by the per-page "committed" bit vector. To elaborate: Each page has a persistent committed 
bit vector as part of the page metadata. The number of bits in the committed vector equals the number of cache lines in 
the page (64 for x86-64). A "1" bit indicates that the corresponding cache line is part of the current consistent image, 
while a "0" bit indicates that the cache line can be used to buffer speculative updates that may become committed in the 
future. The memory controller is responsible for maintaining the committed vectors in a separate region of the NVM, and 
updates the committed bit vectors in an atomic manner when the consistent image moves from one state to the next (e.g. 
at the end of a persistent transaction). The memory controller also maintains the invariant that one and only one bit is 
set in the committed bit vector of the two sibling pages assigned to a virtual page frame. When a cache line is to be updated, 
we first consult the committed bit vector using the physical address of the page (i.e. the one mapped by the OS) to determine 
which page to write into. If page 1 is to be used (i.e. the current committed version is on page 0), then we return the 
address of page 1 to the cache controller, with the data read from page 0 (since the granularity of update is smaller 
than the granularity of memory transaction, a read is still needed in case other words is accessed later). When the cache
controller receives the line, it rewrites the line tag using the address of page 1, such that the cache line is mapped
to the same offset of page 1. This hardware remapping guarantees that even if the dirty line is evicted out of the cache
during the transaction, the committed image will not be affected, and that the system can always recovery to the consistent
state by using the committed image. When the transaction commits, the cache controller writes all dirty lines that is still
in the cache back to the NVM. Note that since the cache tag has been changed to the alternate location, the write back 
is just like normal cache line write back. The memory controller must guarantee that the write back of data and the update
of committed bits are atomic, i.e. when the system crashes, either committed bits reflect the after-commit state, and all
slots contain committed data, or the committed bits reflect the before-commit state, and we discard data (which can be partially
written) in the alternate location. 

We next describe the architecture of SSP. The design of SSP consists of two parts, the on-core part and off-core part. The
on-core part of SSP consists of an extended TLB and special logic on the cache controller. The TLB is extended with three bit
vectors for access tracking, and an extra address field to rewrite an incoming cache line based on the location of the 
committed data. The three bits vectors are committed vector, as described above, current vector, indicating the current 
read image, and updated vector, tracking the write set. Note that the current vector may not equal the committed vector, 
since a transaction may read dirty data written by its own. In this case, the dirty data should be returned rather than 
being determined using the committed vector.

SSP follows a transactional interface, but does not provide transactional memory capabilities. Applications start and end 
a failure atomic region using SSP_BEGIN and SSP_END instructions. All stores within the failure atomic region will be 
persisted in an atomic manner, i.e. either all of them are available after a crash, or none of them is. The paper suggests
that applications should synchronize with locks or other approaches. 

When a cache line is read by a load instruction, the processor checks the TLB as in a normal read. If the TLB misses,
a page walk will be initiated to fetch the page table entry from memory. SSP extends the page walker such that the 
alternate page address and the three bit vectors are also loaded from the memory controller using the physical address.
(**Note: This is likely a design flaw since we must serialize memory controller access after the page walk. Using
virtual address to fetch the vectors will be better, but this requires that memory controller perform context switch.**)
After the TLB entry is populated, we check the committed and current bit to determine whether the data should be read from 
page 0 or page 1. The physical address for probing the L1 cache is formed using page 0's address (i.e. the one in the 
page table) if committed bit indicates that consistent data is in page 0 and that current bit is clear. We read from
page 1 if committed bit indicates page 0 but current bit is set, or committed bit indicates page 1 and current bit is clear.

The process is more complicated in the case of a write, since write operation allocates the slot from the non-committed 
page. If TLB misses, we populate the TLB entry as described above. A speculative cache line is created, if the current
bit is not set, by first performing a read as described above, and then changing the cache tag using the alternate 
address. We set the current bit to indicate that the cache line has been allocated, and that all future writes can
directly be performed on the speculative slot. If the current bit is already set, then we simply generate the address
using the page address not indicated by the committed bit.


Current NVM hardware does not support atomic update of multiple cache lines, and it is 
unlikely in the future that such support would be available. To solve this issue, the paper proposes .