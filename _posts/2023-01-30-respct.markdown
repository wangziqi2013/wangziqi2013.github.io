---
layout: paper-summary
title:  "ResPCT: Fast Checkpointing in Non-volatile Memory for Multi-threaded Applications"
date:   2023-01-30 10:41:00 -0500
categories: paper
paper_title: "ResPCT: Fast Checkpointing in Non-volatile Memory for Multi-threaded Applications"
paper_link: https://dl.acm.org/doi/10.1145/3492321.3519590
paper_keyword: NVM; ResPCT; Undo Logging; Memory Snapshot; Epoch-Based Snapshot
paper_year: EuroSys 2022
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper presents ResPCT, a software epoch-based non-volatile memory snapshotting framework. ResPCT protects 
the integrity of shared data from system failures by periodically checkpointing the states of variables to the NVM.
To maximize performance while minimizing programmer effort, ResPCT adopts in-cache line undo logging and leverages
programmers' notations to mark checkpoint boundaries. ResPCT achieves only marginal performance loss while 
providing the benefit of failure atomicity. Compared with prior works, ResPCT also achieves a significant reduction
in runtime overhead due to its in-cache line logging model.

ResPCT aims to provide an epoch-based checkpointing model for lock-based data structures. In the epoch-based model,
the multithreaded execution is divided into non-disjoint intervals, called epochs, and the state of shared data
is persisted to the NVM at the end of an epoch and before the next one starts. On crash recovery, the system finds
the latest persisted epoch and then restores the system state to the one recorded by that epoch. 
ResPCT assumes that the application periodically creates a checkpoint using an interface function it provides.
ResPCT also assumes that the application programmer can explicitly mark the Restart Points (RP) to which the system 
state can potentially be restored. 
In the case of multithreaded applications, this model implies that the checkpoint is taken when all threads reach
the next RP after the checkpoint request is made.
For common use cases such as data structures, the RPs can be easily identified
at some points in the code region that is not within a critical section (as the code within critical sections will 
mutate shared data and leave it in a temporarily inconsistent state). 
Currently, ResPCT only supports critical section-based use cases. Lock-free data structures, for example, are not 
supported because ResPCT's internal logging API is not safe for concurrent invocations.

Prior works that leverage undo logging often exhibit low performance due to the inevitable usage of persist barriers.
A persist barrier consists of one or more cache line flushes or write backs, followed by a memory fence, which maps to
clflush/clwb and sfense on x86 architecture, respectively. Persist barriers are detrimental to performance because 
excessive cache line flushes will contend memory bandwidth with regular operations, and the store fence forces 
the processor pipeline to stall until the flushes are completed. Unfortunately, software implementations of undo logging
often involve one persist barrier per memory write within an epoch, i.e., when a cache block is first time written
in the epoch, the undo log entry is generated and then immediately flushed into the NVM log buffer using a persist
barrier. The actual store can only be performed after the barrier since undo logging enforces the write ordering 
between log write and data write. 

ResPCT gets rid of the excessive persist barrier of undo logging using a technique called in-cache line logging (InCLL).
In ResPCT, persistent variables are wrapped by templates that generate two other implicit variables. The first 
implicit variable is the undo entry of the same type. The second implicit variable is an epoch number indicating the 
epoch that the undo entry belongs to. The template is carefully crafted such that the compiler will put the three 
variables on the same cache block. On x86 architecture (and many other architectures), since the basic unit of 
persistence is no less than a single cache line, such a data layout can guarantee that the three variables will
always be persisted atomically.

ResPCT maintains a global epoch counter that indicates the current global epoch number. The epoch counter will be 
incremented by one when the global epoch advances by calling the `checkpoint` function.
When a variable is being modified, the template wrapper function will first check whether the epoch of the variable
equals the current global epoch. If true, then no logging needs to be done, as the variable has already been
modified under the same epoch. Otherwise, the wrapper function will copy the old value of the variable into the
undo entry and then update the variable's epoch value to the current global epoch, hence generating the undo
log entry. Since the undo entry and the data item are on the same cache block, they will either be written back
to the NVM together, or the value on the NVM remains unchanged since the last modification. In the former case,
recovery is performed by copying the undo log entry back to the data item. In the latter case, no recovery is needed
as the updated value in the current epoch is never written back to the NVM.

Each thread in ResPCT also maintains a thread-local list that stores the addresses of variables that have been
modified during the current epoch. When a log entry is generated, the software wrapper inserts the address
of the variable into the list. The list is traversed during epoch commit in order to flush all modifications back
to the NVM.

As mentioned earlier, the current epoch is committed to the NVM when a thread calls the `checkpoint` function,
which sets a global flag `timer` to indicate that threads should checkpoint their modified states.
In this case, all threads, excluding the caller, must reach a global stop at the next RP. 
To achieve this, each thread maintains a local flag variable to indicate whether the thread has entered the RP. 
When a thread calls the `RP` function, it first checks whether the global `timer` flag is set, i.e., whether 
a checkpoint is requested. If true, the thread will traverse its local list of modified variables and flushes them
back by issuing a persist barrier. After the persist barrier returns, the thread sets its local flag to signal
the initiator thread that the checkpoint has been completed.
The initiator of the checkpoint, on the other hand, first flushes its local modifications as described earlier,
and then keeps polling the local flags of all other threads until all threads have completed the local flush.
At this point, the checkpoint has been globally completed and the initiator clears the `timer` flag after
advancing the global epoch counter. 
The global epoch counter is also persisted to the NVM such that the recovery procedure can read it and decide
the last committed epoch. 
On observing that the `timer` flag has been reset (in a polling loop), the rest of the threads will return from 
the `RP` function and proceed with their execution.

When the system crashes, some modifications after the last committed epoch will be lost due to not being written back
to the NVM. However, some other modifications will be reflected in the post-crash NVM image, causing an inconsistent
state which needs to be restored.
The restoration procedure first determines the last committed epoch by reading the global epoch counter.



