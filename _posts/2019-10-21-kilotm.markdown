---
layout: paper-summary
title:  "Hardware Transactional Memory for GPU Architectures"
date:   2019-10-21 14:40:00 -0500
categories: paper
paper_title: "Hardware Transactional Memory for GPU Architectures"
paper_link: https://dl.acm.org/citation.cfm?id=2155655
paper_keyword: HTM; GPU; KiloTM
paper_year: MICRO 2011
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper proposes KiloTM, a hardware transactional memory design for GPGPU. This paper identifies several benefits
of introducing HTM to GPGPU. First, GPGPU features heavily threaded workloads. Interactions between threads are difficult
to manage, which poses great challenge for debugging and program verification. Second, the SIMT execution model used by 
GPGPUs can cause deadlock for fine-grained locking. Threads waiting for a lock that has already been taken by some other
threads can be permanently blocked due to the fact that SIMT introduces new implicit control dependencies between threads. 
GPGPUs do not attempt to solve this problem, but rather, they offload the responsibility of finding synchronization bugs 
to programmers. In this situation, HTM can be of great help since they are non-blocking, and enables fast prototyping
of concurrent data structures at the cost of slightly slower execution.

Although numerous HTM designs have been proposed, and few are already implemented on commercial CPUs, designing a working 
HTM for GPGPUs is still a challenging task due to the different data accessing pattern of GPU programs. The paper lists
several of them. First, while traditional CPU-based HTM can maintain transactional states in the L1 private cache at the 
cache line granularity, this can result in massive false sharing for GPU workloads, since GPU transactions tend to access 
adjacent words within a cache line. Second, while CPU-based HTM can detect conflicts using the existing coherence protocol,
GPUs typically do not possess coherent L1 caches. In fact, the paper points out that the private cache of a SIMT core is 
only used to access private data, and when global data is to be accessed by the core, the private cache will not 
be used. Broadcasting all accesses on the bus is also not acceptable, since every thread will send a message to all 
others when a memory instruction is executed, incurring O(n^2) traffic for n threads. The last challenge is commit overhead.
For massively threaded GPU applications, transaction commit can easily become a bottleneck if transactions are validated
in a serial manner. Even if some form of serialization is inevitable to ensure correct ordering, a properly designed commit 
algorithm should reduce this extra serialization to a minimum.

KiloTM involves three major hardware changes. The first change is on the SIMT execution model by allowing transactions to 
be retried after they abort. In a classical stack-based SIMT execution model, any diverging instruction such as branches
will result in two records to be pushed onto the SIMT stack, each representing an execution path controlled by a bit mask.
In KiloTM, the ISA is extended with a transaction begin and commit instruction, which are both diverging instructions. 
On transaction begin, two records, R and T, are pushed onto the stack, with the PC pointing to the next instruction after 
transaction begin, and the RPC undefined. The R record is pushed first, which records failed transactions in the current 
round. The bit mask of the R record is initialized to empty. The T record is pushed after R record, which has an all-set 
bit mask, and the SIMT core will start execution of the transaction body immediately using the T record. On commit, transactions 
are validated. Those that are successfully committed are removed from the bit mask, and those aborted set the corresponding
bits in the bit mask of the R record. The transaction commit instruction pops the T record off the stack, and inserts a 
new T record initialized from the R record, after which the bit mask in the R record is cleared. This allows transactions 
that failed validation to be retried until they succeed, at which moment both T and R record have an empty bit mask. In
this case, both records are popped from the stack, and execution continues with the next entry on the stack.

The second major hardware change of KiloTM is version management. Instead of maintaining read and write sets using bits
in the cache tag, KiloTM stores reads and writes in the form of (addr, value) tuples in the on-chip DRAM. To leverage 
caching and private scratchpad on GPGPU, log entries generated by the same instruction are mapped to one consecutive 
chunk of memory, such that the write coalescing hardware can efficiently stream this entry back to DRAM in one batch.
While the transaction is executed, no modification is made to shared memory. In order for a transaction to read its own
dirty value, a per-thread bloom filter is also added to detect read-after-write within the same transaction. The write log
is walked to forward dirty data to the read instruction if necessary.

The last major change is the addition of validation hardware. KiloTM adopts backward OCC with value validation. Transactions
are assigned commit timestamps (cts) by a centralized counter when they attempt to validate at the end of the transaction. 
In general, a transaction can commit successfully if its read set is not written by any of the overlapping transaction 
during the execution. In a naive serial commit scheme, transactions are validated by the validation hardware serially. A 
validating transaction walks its read set and compares the value in the read log against the current value in the shared 
memory. If all values match, then the read set is considered to be consistent, and the transaction commits. This scheme, 
however, introduces intolerable delay as transactions are eventually serialized at the commit point. To increase parallelism,
two optimizations are introduced into KiloTM. In the first optimization, the address space is partitioned into several
parts, each with a piece of the validation hardware processing requests within the address range. This shortens the 
delay of processing a single validation, but still does not increase parallelism. 
