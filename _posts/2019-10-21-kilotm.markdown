---
layout: paper-summary
title:  "Hardware Transactional Memory for GPU Architectures"
date:   2019-10-21 14:40:00 -0500
categories: paper
paper_title: "Hardware Transactional Memory for GPU Architectures"
paper_link: https://dl.acm.org/citation.cfm?id=2155655
paper_keyword: HTM; GPU; KiloTM
paper_year: MICRO 2011
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper proposes KiloTM, a hardware transactional memory design for GPGPU. This paper identifies several benefits
of introducing HTM to GPGPU. First, GPGPU features heavily threaded workloads. Interactions between threads are difficult
to manage, which poses great challenge for debugging and program verification. Second, the SIMT execution model used by 
GPGPUs can cause deadlock for fine-grained locking. Threads waiting for a lock that has already been taken by some other
threads can be permanently blocked due to the fact that SIMT introduces new implicit control dependencies between threads. 
GPGPUs do not attempt to solve this problem, but rather, they offload the responsibility of finding synchronization bugs 
to programmers. In this situation, HTM can be of great help since they are non-blocking, and enables fast prototyping
of concurrent data structures at the cost of slightly slower execution.

Although numerous HTM designs have been proposed, and few are already implemented on commercial CPUs, designing a working 
HTM for GPGPUs is still a challenging task due to the different data accessing pattern of GPU programs. The paper lists
several of them. First, while traditional CPU-based HTM can maintain transactional states in the L1 private cache at the 
cache line granularity, this can result in massive false sharing for GPU workloads, since GPU transactions tend to access 
adjacent words within a cache line. Second, while CPU-based HTM can detect conflicts using the existing coherence protocol,
GPUs typically do not possess coherent L1 caches. In fact, the paper points out that the private cache of a SIMT core is 
only used to access private data, and when global data is to be accessed by the core, the private cache will not 
be used. Broadcasting all accesses on the bus is also not acceptable, since every thread will send a message to all 
others when a memory instruction is executed, incurring O(n^2) traffic for n threads. 