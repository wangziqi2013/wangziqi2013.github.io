---
layout: paper-summary
title:  "Devirtualizing Memory in Heteogeneous System"
date:   2018-12-25 00:01:00 -0500
categories: paper
paper_title: "Devirtualizing Memory in Heteogeneous System"
paper_link: https://dl.acm.org/citation.cfm?doid=3173162.3173194
paper_keyword: TLB; Virtual Memory; Accelerator; GPU
paper_year: ASPLOS 2018
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper proposes Devirtualized Memory (DVM), which aims at providing high memory bandwidth to accelerators that
can access virtual address space. This is crucial for improving the usability and efficiency of accelerators 
such as GPUs for three reasons. First, accelerators need to access virtual memory for protection and resource 
multiplexing. The virtual memory protection bits and remapping fits into this perfectly. Second, if accelerators could 
access the virtual memory the same way as the CPU does, then a pointer on the CPU has the same meaning for the accelerator. 
Pointer based data structures, such as linked lists, trees and graphs, can be transferred between CPU and accelerator 
directly without data marshalling and unmarshalling. This is particularly attractive when the accelerator processes 
graphs, where nodes are linked together using pointers. The last reason is that data transfer to and from the accelerator
is typically slow. Letting the accelerator fetch data from the main memory, on the other hand, amortizes the overhead
of data movement, and hence enables finer grained batches being processed by the accelerator. 

When accelerators access the memory using virtual addresses, the IOMMU is responsible for translating virtual addresses
into physical addresses. IOMMU is a device connected to the system bus. All memory requests issued by the device must
be forwarded by the IOMMU in order to access memory. During system startup, the Operating System configures the IOMMU
by assigning I/O page tables to devices. Each device could have its own page table, the content of which may or may not be 
identical to the page table used by MMU. The IOMMU translates the address in the memory request issued by devices 
using the page table before forwading them to the memory controller. Similar to the case of CPU, in order to make 
translation faster, the IOMMU may also have a built-in TLB, which caches recently used translation entries. 

The IOMMU address translation can become a performance bottleneck when the accelerator runs memory intensive workloads. 
This paper addresses this problem using identity mapping. An identify mapping is a trivial way of performing address
translation: it directly outputs VA as the PA, which makes address translation unnecessary. Recall that the IOMMU also
checks permissions in addition to address translation. To facilitate this, the paper also proposes adding a new page 
table type: the Permissions Entry (PE). PE can replace a PTE on any level, assuming a multi-level page table. It contains
16 permission descriptors, which describes the access permissions of the memory range covered by an entry on that level. For
example, an L2 PE covers 2MB of memory, and hence each permission descriptor defines the permission of 128KB of consecutive 
memory; For L3 PE each descriptor defines the permission for 64MB of memory.