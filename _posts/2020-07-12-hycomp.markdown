---
layout: paper-summary
title:  "HyComp: A Hybrid Cache Compression Method for Selection of Data-Type-Specific Compression Methods"
date:   2020-07-12 00:54:00 -0500
categories: paper
paper_title: "HyComp: A Hybrid Cache Compression Method for Selection of Data-Type-Specific Compression Methods"
paper_link: https://dl.acm.org/doi/10.1145/2830772.2830823
paper_keyword: Cache; Compression; HyComp; Hybrid Compression
paper_year: MICRO 2015
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes HyComp, a cache compression framework with high compression ratio for all data types using multiple
compression algorithms. The paper is motivated by the observation that most compression algorithms are only capable of
compressing a certain with high compression ratio, while leaving data of incompatible types less compressed, introducing
a huge bias depending on the application and the data types used. The difference between compression capabilities is 
a consequence of different assumptions on sources of redundancy. For example, FPC is based on the assumption that most
redundancies are caused by small integers whose upper bits are all ones or all zeros. It works badly for pointers, which
typically contains addresses of user space data or stack segments. BDI, on the other hand, assumes that redundancies are
introduced by dynamic value locality of nearby values. In most cases, value locality is demonstrated by small integers,
or pointers of similar sized objects allocated from the heap. BDI, on the other hand, cannot process large integers, 
hetrogeneous data types or floating point values very well, since these values differ by a large amount in their numeric
literal. Huffman encoding, as used in SC2, only compresses values that frequently occur throughout the execution with 
shorter codewords. It cannot handle value locality in the form of nearby values, which is common for pointers.
Lastly, special optimization can be applied for zero blocks, i.e. a cache block with all-zero, which is common for 
initialized data structure and sparse matrix. Although all schemes described above can achieve a relative high compression
ratio for zero blocks, Zero Cache Augmentation (ZCA) compresses zero blocks at the highest compression ratio of one single
bit per block, which is ideal to zero-dominant workloads. None of the above compression algorithms can handle floating
point values well, since floating point values consists of three fields: sign, exponent, and mantissa, which satisfies
none of the compression assumptions above.

Instead of using one fixed compression algorithm for all kinds of possible data types that may occur, HyComp implements
four different compression algorithms, namely, SC2, BDI, ZCA and a floating point compression algorithm called FP-H.
Heuristics are employed to determine the data type within an uncompressed cache line. Once the data type is determined,
HyComp will compress the block using the most suitable algorithm for that type, achieving higher compression ratio than
any of the single algorithm for all workloads.

We next describe the operation of the cache architecture. HyComp is implemented for the LLC. Cache lines fetched from
the main memory or evicted by upper level caches should be compressed before installed into the LLC. Similarly, cache 
lines evicted or fetched by upper levels should be decompressed before they are sent over the network. The paper observes
that decompression is on the critical path of upper level data fetching, which can affect performance, while compression
is in the background, and it is unlikely that several more cycles added to the latency will negatively impact 
performance. In order to hold several compression blocks per physical slot, each data slot is equipped with two tags
which statically map addresses to the data slot. In addition to the conventional bits such as dirty, valid and coherence
bits per tag, an offset field, compressed size field, metadata field and compression type field are also added for placement 
and decompression of compressed blocks.
The offset field serves as an indirection pointer to locate the compressed block in the physical slot. The paper suggests
that word-aligned pointers or byte-aligned pointers are both fine, making a trade-off between external fragmentation
and static metadata cost.
The compression metadata is a three-bit field that stores algorithm-specific information for decoding the compressed block.
For BDI, it stores the three-bit compression type. For ZCA, it stores whether the block is a zero block (zero blocks
do not use data slots). For SC2 and FP-H, it stores the version since two instances of the codebook can be active at the 
same time.
The two-bit algorithm selection field selects the decompression logic via a multiplexer. The compressed block is sent
to the corresponding decompression circuit based on the value in this field.

The cache access protocol does not change much except that tag lookup and data slot access are serialized due to the 
extra level of indirection. When a dirty block is written back from high levels, the compression circuit first encodes
the block into a shorter form, and then compares the size between the old block and the new block. If the new block fits 
into the old block's storage (either smaller, or there is extra storage after the old block), the write back completes
without any eviction. If, on the other hand, the new block cannot fit into the old block's store, or there is no
free tag, at least one LRU tag, together with its data, must be evicted. 

The heuristics works as follows. When an uncompressed block is to be processed, the cache controller first guesses the 
data type of values in the block, and then selects the best compression algorithm based on type information.
Data type 
