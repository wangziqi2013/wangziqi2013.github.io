---
layout: paper-summary
title:  "HyComp: A Hybrid Cache Compression Method for Selection of Data-Type-Specific Compression Methods"
date:   2020-07-12 00:54:00 -0500
categories: paper
paper_title: "HyComp: A Hybrid Cache Compression Method for Selection of Data-Type-Specific Compression Methods"
paper_link: https://dl.acm.org/doi/10.1145/2830772.2830823
paper_keyword: Cache; Compression; HyComp; Hybrid Compression
paper_year: MICRO 2015
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes HyComp, a cache compression framework with high compression ratio for all data types using multiple
compression algorithms. The paper is motivated by the observation that most compression algorithms are only capable of
compressing a certain with high compression ratio, while leaving data of incompatible types less compressed, introducing
a huge bias depending on the application and the data types used. The difference between compression capabilities is 
a consequence of different assumptions on sources of redundancy. For example, FPC is based on the assumption that most
redundancies are caused by small integers whose upper bits are all ones or all zeros. It works badly for pointers, which
typically contains addresses of user space data or stack segments. BDI, on the other hand, assumes that redundancies are
introduced by dynamic value locality of nearby values. In most cases, value locality is demonstrated by small integers,
or pointers of similar sized objects allocated from the heap. BDI, on the other hand, cannot process large integers, 
hetrogeneous data types or floating point values very well, since these values differ by a large amount in their numeric
literal. Huffman encoding, as used in SC2, only compresses values that frequently occur throughout the execution with 
shorter codewords. It cannot handle value locality in the form of nearby values, which is common for pointers.
Lastly, special optimization can be applied for zero blocks, i.e. a cache block with all-zero, which is common for 
initialized data structure and sparse matrix. Although all schemes described above can achieve a relative high compression
ratio for zero blocks, Zero Cache Augmentation (ZCA) compresses zero blocks at the highest compression ratio of one single
bit per block, which is ideal to zero-dominant workloads. None of the above compression algorithms can handle floating
point values well, since floating point values consists of three fields: sign, exponent, and mantissa, which satisfies
none of the compression assumptions above.

Instead of using one fixed compression algorithm for all kinds of possible data types that may occur, HyComp implements
four different compression algorithms, namely, SC2, BDI, ZCA and a floating point compression algorithm called FP-H.
Heuristics are employed to determine the data type within an uncompressed cache line. Once the data type is determined,
HyComp will compress the block using the most suitable algorithm for that type, achieving higher compression ratio than
any of the single algorithm for all workloads.

