---
layout: paper-summary
title:  "Mallacc: Accelerating Memory Allocation"
date:   2019-01-05 02:18:00 -0500
categories: paper
paper_title: "Mallacc: Accelerating Memory Allocation"
paper_link: https://dl.acm.org/citation.cfm?doid=3037697.3037736
paper_keyword: malloc; Accelerator; Special Purpose Hardware
paper_year: ASPLOS 2017
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper proposes mallacc, a special purpose hardware design that accelerates the C heap memory allocation API: malloc().
Malloc, as one of the most important C library functions, are performance critical and must complete within only a few 
cycles. We briefly describe common designs of state-of-the-art malloc in this paragraph. Users call malloc() with the 
size of the request, which is the minimum amount of memory that the allocator should return. On receiving the request, the 
allocator aligns the requested size, and rounds it up to the nearest size available for allocation. Keeping the number of 
available sizes in a reasonable range achieves a balance between internal fragmentation (memory under-utilization because
not all allocated spaces are used by the user program) and the space taken to maintain metadata which can have a negative 
effect on performance if the metadata size is too large and cannot fit into the cache. Free memory blocks of each size class 
are usually maintained per size bucket in the form of linked lists. The allocator removes the head element from the list
and returns it to the user if the list is non-empty. To avoid synchronization overheads, each thread maintains their own 
size buckets and size classes. If a size bucket is empty, the allocator either tries to claim more memory blocks from a 
larger bucket, or from the higher level memory pool. The memory pool is a shared data structure, which itself can also 
be hierarchical and consists of multiple smaller pools. Accesses to the memory pool usually requires locking or other forms
of synchronizaton. During normal operation, the first level buckets will be accessed most frequently, which is the fast-path
of allocation. The paper suggests that the fast-path is already quite highly optimized, and only takes 20 to 30 cycles to 
complete. Free operations are treated almost the same way as allocation. When the bucket is calculated, the freed memory
block is inserted to the head of the linked list. Migrations of blocks between threads and between different levels of 
memory pools are also implemented to bound the total amount of memory usage. We do not concentrate on these details
because the implementation of block migration policies differ greately among implementations, and are not on the 
fast-path.

In the following sections, we use tcmalloc as a typical example to illustrate how mallacc improves the performance
of memory allocation. The hardware design, however, is totally general and can be equally applied to other malloc
implemetations (partially because the fast-path is performance critical, and hence different malloc implementations are 
largely the same or at least very similar to each other).

Regardless of the fact that the fast-path is already highly optimized, the paper still observes two major sources of 
slow down in the already-fast malloc fast-path, which is described as follows. First, during the first stage where the 
request size is rounded up to an existing size class, a lookup table is used to translate the aligned requested size 
into an actual allocation size. To accelerate the translation process, tcmalloc using the aligned requested size as an 
index to perform two table lookups. Both lookup tables are static read-only area of memory organized as arrays. The request 
size (after alignment) is directly used as the index to the first table to compute the size class. Then the size class is 
used to index the second lookup table, and the output is the index of the bucket where the free list is maintained. Both arrays 
are small enough such that there is non-negligible change that they will remain in the LLC or even L1. It is, however, still 
slow because the two load instructions are on the critical path of malloc, and hence cannot be overlapped or speculated. 
The second source of slow down is the overhead to maintain the linked list after the bucket has been identified. In order to
remove the first element from the linked list and return it as the allocated block, two loads and one store instruction
are needed: The first load instruction reads the pointer to the head node, and the following load reads the pointer 
to the next node (the pointer to the next node is stored inside the unallocated block itself). Then a store instruction
updates the bucket to point to the next block in the linked list using the result of the second load. These three memory 
instructions may suffer performance bottleneck from two aspects. First is that they are dependent on each other, which
makes speculation impossible. The second is that the linked structure may not have good cache hit ratio as in the 
case of array lookup from the previous stage, because modern cache hierarchy does not handle linked structure very well. 