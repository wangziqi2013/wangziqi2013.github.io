---
layout: paper-summary
title:  "An Empirical Guide to the Behavior and Use of Scalable Persistent Memory"
date:   2019-11-14 13:42:00 -0500
categories: paper
paper_title: "An Empirical Guide to the Behavior and Use of Scalable Persistent Memory"
paper_link: https://arxiv.org/abs/1908.03583v1
paper_keyword: NVM
paper_year: arXiv, Aug 9 2019
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This preprint draft paper presents several performance characteristics of the newly introduced commercial product of 
byte-addressable NVM, 3D XPoint. The observations made in this paper are based on actual physical persistent NVM hardware, 
which is most likely implemented with Phase-Change Memory technology. Byte-addressable NVM requires special support
from the integrated memory controller on the processor chip to ensure the correctness of stores once they reach
the write pending queue of the memory controller. The memory controller leverages Asynchronous DRAM Refresh capability
to guarantee that even on a power failure, dirty data in the write queue can still be flushed back to the NVM, and so does
dirty data within NVM's internal buffers and queues. Although the memory controller communicates with the NVM device
using 64 byte cache line sized blocks, the internal page size of the NVM device is 256 bytes due to density requirements
and physical space limites. As we will see later, this page size configuration will introduce write amplification
problem in some cases, as write operations into an NVM page will be internally performed as a read-modify-write sequence. 
To reduce the frequency of writes into the non-volatile components, the NVM also provides an internal buffer (called XPBuffer
in the paper) to combine writes. The buffer is also made persistent using residual powers on the device on a power failure. 
It is also suggested by the paper that there are 64 such buffers, 256 bytes each, in the device. **Both** reads and writes 
will allocate a buffer entry. 

The paper presents two commonly used methods of persisting data to the NVM device. The first method is to issue a cached
regular store, followed by a flush and memory fence. The memory fence will commit when the store reaches the write 
pending queue of the controller, after which the store is guarantee to persist. The second method is to use non-temporal
stores to circumvent the cache hierarchy. The non-temporal store will likely be write combined in the processor's write queue,
and sent to memory controller directly without write-allocate. Since non-temporal writes are not ordered against other
writes and even themselves, a memory fence must also be issued after the non-temporal store to ensure that later non-temorial
srores do not override the current one. The paper suggests that using the second method to persiste large amount of data 
is slightly faster than using the flush + fence sequence. This is because non-temproal stores do not allocate cache entry
for the write, and therefore, if the non-temproal store is able to fill an entire cache line after write combining,
the cache controller does not have to read the address before writing into it. This prevents the NVM devices from being
saturated prematurely by the extra read traffic, which is sometimes not intended (e.g. when we are writing log data
which is barely read again until crash recovery).

The paper makes the following observation regarding access latency. First, read latency is significantly higher than write
latency on NVM. This is not the case with DRAM, which has an almost symmetric read-write latency. This can be explained 
by the difference in the performance characteristics of underlying memory technologies, since reading from the PCM NVM
is just much slower than reading from the DRAM capacitor cells. Even more surprisingly, the paper also observed that 
the read latency of NVM is around 3x higher than writing into the NVM, and that the latency of DRAM write and NVM write
are alomost the same. This is because writing into the NVM only requires sending data to the memory controller, which buffers
the request in the write pending queue, while data being read from the NVM is sent by the device itself. The second observation
is that sequential reads are much faster than random read. This is the natural result of having internal buffers which
also allocate entries for reads. Sequential reads are likely to hit the buffer, and hence have lower average latency,
while random reads are likely to miss the buffer, which requires accessing the physical non-volatile component. The third
observation is on variance of latencies. In general, 3D XPoint features extremely low overall write latency variance. There 
are, however, outliers for writes which have 100x higher latency than normal writes. The paper suspected that this is either
casued by wear leveling, or as a result of thermal control.
