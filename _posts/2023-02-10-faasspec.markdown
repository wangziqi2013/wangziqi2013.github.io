---
layout: paper-summary
title:  "SpecFaaS: Accelerating Serverless Applications with Speculative Function Execution"
date:   2023-02-10 18:06:00 -0500
categories: paper
paper_title: "SpecFaaS: Accelerating Serverless Applications with Speculative Function Execution"
paper_link: N/A
paper_keyword: Serverless; Speculative Execution; SpecFaaS
paper_year: HPCA 2023
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes SpecFaaS, a function-as-a-service (FaaS) framework that enables the speculative execution of 
functions to speed up chained invocation. SpecFaaS is motivated by the high degree of both control and data 
determinism during chained invocation, i.e., with high probabilities, chained function execution will
follow the same path given the same invocation history, and the outputs of a single function will be identical
given the same inputs. To leverage such high degrees of determinism, SpecFaaS speculatively executes functions in
a call chain and buffers their outputs until all the predecessors of the functions complete non-speculatively.
Consequently, SpecFaaS can achieve much shorter overall execution latency in most cases as it effectively overlaps
the execution of dependent functions, which will be serialized without SpecFaaS.

SpecFaaS aims at optimizing chained execution of serverless functions in FaaS platforms. In FaaS, an application
is divided into basic execution units called "functions", and each function can be separately invoked either locally
or via RPC. The FaaS platform is responsible for managing the resource provisioning, scheduling, and the scaling
aspects of functions, liberating programmers from such tasks. FaaS platforms support chained function execution
in two fashions. In the first fashion, the control flow between functions is explicitly expressed using a procedure
annotation language which enables common control flow nodes, such as conditional branches and data dependencies to
be expressed and understood by the platform. In this case, the platform functions as a function scheduler, which
selects the next function to execute when the previous one completes according to the control flow graph.
In the second fashion, control flow information is expressed implicitly during runtime invocation, forming a 
hierarchical view of execution where a callee returns to the caller and resumes caller execution, rather than
giving control to the scheduler. 

To better understand the behavior of chained function execution, the paper conducted experimentation using three
serverless benchmarks that use function chaining. The first observation is that function execution only constitutes 
a minor fraction of the total invocation latency even on warm starts (it becomes much worse for cold starts), with the
major part of the overhead being the chained invocation (a.k.a. transfer function) and platform overhead. It is 
therefore more beneficial to optimize the entire process via overlapping rather than simply making execution faster
which can only yield marginal improvement. 
Second, both the control and data dependency pattern is highly deterministic, meaning that the control flow will likely
be the same when a branch occurs given the same execution history till the branching point, and that the output of a
function will also likely be the same given the same input. 
This observation indicates that history-based branch prediction and memorization would both work well.
Lastly, many functions do not frequently read or write global states, and even if they do, they typically
do not access the same location in the global state.
This observation suggests that the chained functions are unlikely to collide with each other via global state
access, and therefore, executing them out-of-order would be a feasible choice.
In addition, most functions do not have side effects that may prevent speculative execution. The most common types
of side effects are global state accesses, temporary file creation, and sending HTTP requests. 

Based on the above observations, SpecFaaS implements a speculative function execution mechanism where functions are
started before their predecessors are completed. In this novel paradigm, functions that are in the same invocation
chain can be executed out-of-order rather than being serially invoked by the platform, overlapping their execution
and hence significantly reducing the end-to-end latency. Similar to speculative execution on hardware, functions
must be validated after execution in order after they are completed in order to ensure that the control and 
data dependencies are not violated. If a function is validated successfully, it commits all global state modifications 
and exits.

SpecFaaS consists of three major components. The first component is the Sequence Table and Branch Predictor.
The sequence table stores the control flow of the function, which can be either compiled from the explicitly provided
function specification or implicitly inferred from the runtime execution trace. When the first function with a chain
is invoked, the platform's scheduler can speculatively invoke functions following the control flow graph. 
To deal with branches, the scheduler also actively predict the outcome of branches using the past execution history.
As stated earlier, branches in chained functions are highly biased given the execution history. Therefore, for 
each branching node in the control flow, the scheduler maintains the Branch Predictor metadata which is
a mapping between execution history and the frequencies of branch outcomes. Branch prediction is then performed 
by using the current dynamic execution history to query the map and then selecting the path with the highest 
probability. The branch predictor metadata is also updated when functions commit and become non-speculative.

The second component is the per-function Memoization Table which maps the function's input to the output. 
The Memoization Table addresses the data dependency problem between consecutive functions on the call chain.
When a function is speculatively invoked, if its input depends on the output of its predecessors, then the 
input will be provided by querying the Memoization Table with the input of the predecessor function, which
itself may be from the Memoization Table of another function.

