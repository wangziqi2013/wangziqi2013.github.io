---
layout: paper-summary
title:  "SpecFaaS: Accelerating Serverless Applications with Speculative Function Execution"
date:   2023-02-10 18:06:00 -0500
categories: paper
paper_title: "SpecFaaS: Accelerating Serverless Applications with Speculative Function Execution"
paper_link: N/A
paper_keyword: Serverless; Speculative Execution; SpecFaaS
paper_year: HPCA 2023
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes SpecFaaS, a function-as-a-service (FaaS) framework that enables the speculative execution of 
functions to speed up chained invocation. SpecFaaS is motivated by the high degree of both control and data 
determinism during chained invocation, i.e., with high probabilities, chained function execution will
follow the same path given the same invocation history, and the outputs of a single function will be identical
given the same inputs. To leverage such high degrees of determinism, SpecFaaS speculatively executes functions in
a call chain and buffers their outputs until all the predecessors of the functions complete non-speculatively.
Consequently, SpecFaaS can achieve much shorter overall execution latency in most cases as it effectively overlaps
the execution of dependent functions, which will be serialized without SpecFaaS.

SpecFaaS aims at optimizing chained execution of serverless functions in FaaS platforms. In FaaS, an application
is divided into basic execution units called "functions", and each function can be separately invoked either locally
or via RPC. The FaaS platform is responsible for managing the resource provisioning, scheduling, and the scaling
aspects of functions, liberating programmers from such tasks. FaaS platforms support chained function execution
in two fashions. In the first fashion, the control flow between functions is explicitly expressed using a procedure
annotation language which enables common control flow nodes, such as conditional branches and data dependencies to
be expressed and understood by the platform. In this case, the platform functions as a function scheduler, which
selects the next function to execute when the previous one completes according to the control flow graph.
In the second fashion, control flow information is expressed implicitly during runtime invocation, forming a 
hierarchical view of execution where a callee returns to the caller and resumes caller execution, rather than
giving control to the scheduler. 

To better understand the behavior of chained function execution, the paper conducted experimentation using three
serverless benchmarks that use function chaining. The first observation is that function execution only constitutes 
a minor fraction of the total invocation latency even on warm starts (it becomes much worse for cold starts), with the
major part of the overhead being the chained invocation (a.k.a. transfer function) and platform overhead. It is 
therefore more beneficial to optimize the entire process via overlapping rather than simply making execution faster
which can only yield marginal improvement. 
Second, both the control and data dependency pattern is highly deterministic, meaning that the control flow will likely
be the same when a branch occurs given the same execution history till the branching point, and that the output of a
function will also likely be the same given the same input. 
This observation indicates that history-based branch prediction and memorization would both work well.
Lastly, many functions do not frequently read or write global states, and even if they do, they typically
do not access the same location in the global state.
This observation suggests that the chained functions are unlikely to collide with each other via global state
access, and therefore, executing them out-of-order would be a feasible choice.
In addition, most functions do not have side effects that may prevent speculative execution. The most common types
of side effects are global state accesses, temporary file creation, and sending HTTP requests. 

Based on the above observations, SpecFaaS implements a speculative function execution mechanism where functions are
started before their predecessors are completed. In this novel paradigm, functions that are in the same invocation
chain can be executed out-of-order rather than being serially invoked by the platform, overlapping their execution
and hence significantly reducing the end-to-end latency.
