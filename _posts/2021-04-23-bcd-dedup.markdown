---
layout: paper-summary
title:  "BCD Deduplication: Effective Memory Compression Using Partial Cache-Line Deduplication"
date:   2021-04-23 21:08:00 -0500
categories: paper
paper_title: "BCD Deduplication: Effective Memory Compression Using Partial Cache-Line Deduplication"
paper_link: https://dl.acm.org/doi/10.1145/3445814.3446722
paper_keyword: Compression; Memory Compression; Deduplication; Inter-Block Compression
paper_year: ASPLOS 2021
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper presents BCD deduplication, a memory compression technique using block clustering inter-block compression.
The paper points out the limitations of previous deduplication and compression proposals as follows.
First, many previous schemes rely on in-memory mapping table for address remapping, due to blocks being 
variable sized after compression. These mapping tables inevitably incur more memory accesses during both reads
and writes, which will affect performance. 
Second, previous deduplication schemes, despite the fact that they can catch a wider range of block duplications
by running on the entire address space rather than a cached subset, still underperforms stat-of-the-art compression 
due to the granularity of deduplication being too coarse, which is on cache block level.
Lastly, most current compression algorithms either seek special word patterns within a block, such as BDI, FPC, 
and BPC, or rely on a dictionary to encode frequent words using less number of bits. In the former case, 
compression ratio is sub-optimal, despite low compression and decompression latency, since the algorithm only
exploits redundancy in a cache block without considering inter-block redundancy. In the latter case, statistics must
be generated in advance, and not changed frequently thereafter, such that correctness is guaranteed.

BCD deduplication, on the other hand, combines inter-block delta compression with deduplication. Its compression
algorithm consists of two steps. In the first step, cache blocks that are likely to contain similar contents are
clustered together into the same hash table bucket, and compared with each other. The bit-level delta is taken
in a specific form such that the delta is only three-fourth of the size of an uncompressed block.
In the second step, the bit-level delta is then compressed, and hashed into another hash table to perform deduplication. 
