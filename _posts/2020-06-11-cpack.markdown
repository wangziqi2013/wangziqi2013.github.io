---
layout: paper-summary
title:  "C-Pack: A High-Performance Microprocessor Cache Compression Algorithm"
date:   2020-06-11 23:26:00 -0500
categories: paper
paper_title: "C-Pack: A High-Performance Microprocessor Cache Compression Algorithm"
paper_link: https://dl.acm.org/doi/10.1109/TVLSI.2009.2020989
paper_keyword: Compression; Cache Tags; C-Pack
paper_year: TVLSI 2010
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes C-Pack, a cache compression architecture featuring realistic design assumptions and validiation steps
that actually fit into processor's design flow. The paper points out that existing cache compression algorithms either make 
unrealistic assumptions about the design complexity or compression/decompression latency, making the algorithm difficult 
to implement and deploy. In addition, some design algorithms are designed to work well with large blocks in order to
find sufficient number of patterns, which may not work well with 64 byte cache lines, and often requires an extra indirection
or caching level.

The paper consists of two parts. The first part discusses the hardware implemented compression algorithm, which is a 
hybrid of pattern-based encoding and dictionary-based compression (like LZW). The second part of the paper discusses
the compressed cache organization, which uses a restricted tag mapping scheme to achieve low cost cache management. 

The compression algorithm combines pattern matching and dictionary encoding, which will be discussed as follows.
Logically speaking, compression is performed on 32 bit words, which are processed one-by-one. The compression engine
maintains a dynamically generated dictionary to translate an incoming word into a smaller code. Thedictionary is implemented 
as a fixed size CAM array of 32 bit entries, with a comparator at each entry. 
The size of the dictionary affects several aspects of the design. First, a smaller dictionary may not be sufficient to
encode all patterns, which decreases the efficiency of compression, since some entries will be evicted before they 
can be used to encode an incoming word. Second, if the dictionary is fully associative, then dictionaries whose sizes are
larger than cache line capacity makes little sense, since in the worst case none of the dictionary entry serve as patterns
for encoding, resulting in the entire cache being inserted into the dictionary. If, however, fully-associative dictionary
is not an option, suggesting set-associative dictionary design, dictionaries larger than 64 bytes are useful, since code
words may not be evenly distributed into all sets, justifying some degrees of redundancy.

