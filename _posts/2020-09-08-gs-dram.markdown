---
layout: paper-summary
title:  "Gather-Scatter DRAM: In-DRAM Address Translation to Improve the Spatial Locality of Non-unit Strided Accesses"
date:   2020-09-08 23:56:00 -0500
categories: paper
paper_title: "Gather-Scatter DRAM: In-DRAM Address Translation to Improve the Spatial Locality of Non-unit Strided Accesses"
paper_link: https://dl.acm.org/doi/10.1145/2830772.2830820
paper_keyword: DRAM; GS-DRAM; 
paper_year: MICRO 2015
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Lowlight:**

1. The paper should mention that the step size cannot be too large (although in typical use cases it will not be
   very large), since the row buffer must hold DRAM rows from the same row ID. If the step size is too large,
   then it is possible that the line resides in a different row, requiring the chips in the rank to activate different
   rows (i.e., some lines are on row X, and some lines are on row X + 1), complicating the design of the row buffer.

2. The above problem is actually fatal, if some lines are at the end of one row, and some are at the next row. In this
   case, even if the step size is small, chips in a rank still have to activate different rows.

This paper proposes Gather-Scatter DRAM, a novel DRAM architecture that enables fast gather and scatter semantics.
The paper points out that current DRAM interface only supports cache line granularity access, which causes difficulties
when accessing data in a strided pattern. This prevents several common patterns from being implemented efficiently.
For example, in matrix manipulating code, accessing multiple matrix elements on the same column often requires strided 
reads where the stride length is the size of a row, if the matrix is stored in row-major order where elements in the 
same row are stored. Second, in HTAP database applications, tuples can be accessed either transactionally,
where only a few tuple is touched, or analytically, where certain fields of a large set of tuples are scanned. In a row 
store data layout, scanning one field of all tuples also rerquires strided accesses to memory, with the stride being the 
physical size of a tuple. 
The last example is SIMD, which supports data gathering and scattering with special instructions. These instructions,
however, can be inefficient if multiple cache lines need to be fetched from the DRAM.

The paper assumes the following DRAM architecture. The DRAM storage consists of potentially multiple channels,
each supporting several DIMMs. Channels have their own control and data signals. Multiple ranks exist within a channel.
All DRAM devices in a rank share the same command and data path, which operate as a basic unit of access.
This paper assumes that cache lines are always mapped into one rank for the simplicity of discussion, but other address
mappings can also be supported.
Each rank consists of several DRAM chips, and each chip stores part of a cache line. In this paper, it is assumed that
each chip provides 8 bytes of data on an access. The cache line width is determined by the number of chips on a single 
rank. For 64-byte cache lines, eight chips are present, in which chip 0 provides byte 0 - 7, chip 1 provides byte 8 - 15,
chip 7 provides byte 57 - 64, and so on. 
Although the internal organization of chips is not important to this paper, it is still briefly described that each
chip contains several arrays, where each array provides one bit to the 64-byte cache line.
On a chip read operation, the read command fetches an entire row of bits from each array, and cache them in a structure
called the row buffer. Although only one bit of the row is accessed per-array, the remaining bits from the row are latched
in the row buffer to serve future reads without precharging and activation. The offset of the bit that is accessed is 
called the column address, which represents the offset of the 64-bit cache line in the row buffer.

The above DRAM organization has two important properties that make strided accesses inefficient. 
First, the same bit of all cache lines mapped to a rank is always provided by a single chip. Second, all chips in a rank 
can only be accessed by one request at a time, since there is only one control and data path per rank. 
These two properties prevent strided accesses from accessing words on the same offset from two different cache lines,
since these two words will be mapped to the same chip.

This paper solves the above issue by mapping words into chips in a different manner. Conventionally, words on offset i
is always mapped to chip i. The paper proposes a novel data shuffling algorithm to ensure that, for cache line size 
of W words, any W adjacent cache lines will have their words on offset i mapped to different chips, enabling parallel
accesses to words on the same offset from adjacent cache lines, which is the most common strided access pattern.
