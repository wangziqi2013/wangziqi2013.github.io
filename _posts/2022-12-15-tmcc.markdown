---
layout: paper-summary
title:  "Translation-optimized Memory Compression for Capacity"
date:   2022-12-15 23:53:00 -0500
categories: paper
paper_title: "Translation-optimized Memory Compression for Capacity"
paper_link: https://ieeexplore.ieee.org/document/9923870
paper_keyword: Memory Compression; TMCC; Deflate
paper_year: MICRO 2022
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Comments:**

1. This paper lacks a high-level overview of the design while focusing too much on implementation details. 
As a result, the overall picture of how TMCC works is not clear.

This paper proposes TMCC, a hardware-based main memory compression technique based on the existing OS-supported 
memory compression framework. The paper focuses on two aspects of the design to make it efficient. First, the 
existing hardware designs introduce an extra level of indirection between the physical address and the storage location
of compressed pages. This extra level of indirection can incur great overhead on the access critical path and 
therefore should be reduced. Second, prior works on hardware dictionary compression proposed specialized ASIC 
compression and decompression engines. However, these engines are designed for general-purpose user scenarios and 
have an emphasis on compatibility, resulting in less efficient operations. The paper proposes a more specialized 
ASIC design that eliminates the overheads by tailoring the algorithm to fit into memory compression.

The work of this paper is based on a software-based memory compression approach implemented within the OS kernel 
(i.e., the OS-inspired approach in the paper). In the software-based approach, physical memory is managed at 
page granularity in two levels, namely memory level 1 (ML1) and memory level 2 (ML2). ML1 consists of physical pages
that are uncompressed, and the virtual-to-physical mapping is set up normally as in an uncompressed system. Meanwhile,
the ML2 consists of compressed physical pages, which are not directly mapped via the virtual memory system. 
Instead, virtual addresses that are mapped to ML2 pages rely on page faults to notify the OS kernel when they are 
accessed such that the OS will decompress them into a page frame and then set up the mapping, hence moving the 
page from ML2 to ML1.
Due to the high overhead of accessing ML2 pages, the OS kernel only moves a page from ML1 to ML2 when the page is 
deemed "cold", i.e., when the system is under memory pressure and the page is not accessed for a while. 
The detection of cold pages is performed using an LRU list called the "Recency List". The Recency List is updated
for a small fraction of accesses to approximate the actual access frequency (although the paper did not mention how
it can be achieved).

In the software approach, the OS maintains two free lists consisting of physical storage at the page or sub-page level.
The first list belongs to ML1 and always maintains storage at page granularity. This list can also be regarded as the 
OS physical page allocator. The second list belongs to ML2, and it is further divided into different size classes to 
accommodate compressed pages of different sizes. Physical storage can move around in both lists. When a page 
is moved from ML1 to ML2, it is broken down into smaller sub-pages of the desired size class. On the other hand, when
free ML2 sub-pages form a whole page, the page can also be optionally moved to ML1.

The paper identifies two issues when the above mechanism is implemented on hardware. First, in a hardware 
implementation, there is no OS kernel to explicitly manage the address mapping between virtual and physical address
space. Consequently, the hardware must manage the mapping instead, which inevitably adds another level of indirection
between the physical address space and the storage location of the page. 
Unfortunately, this mapping lies on the critical path of memory accesses as well as regular address translation, as 
it must be conducted on LLC misses after the physical address is available. 
Prior works attempted to alleviate such cost by adding an extra translation metadata cache at the memory controller
level. The metadata cache stores frequently used entries such that most translations can be performed with low latency.


