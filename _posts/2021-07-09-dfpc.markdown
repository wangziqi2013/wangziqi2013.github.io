---
layout: paper-summary
title:  "DPFC: A Dynamic Frequent Pattern Compression Scheme in NVM-Based Main Memory"
date:   2021-07-09 21:20:00 -0500
categories: paper
paper_title: "DPFC: A Dynamic Frequent Pattern Compression Scheme in NVM-Based Main Memory"
paper_link: https://ieeexplore.ieee.org/document/8342274/
paper_keyword: NVM; FPC; DFPC; Compression
paper_year: DATE 2018
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Comments:**

1. The pattern table, especially the dynamic one, should be saved on a context switch to allow multiple processes
   to use DFPC and each of them to have its own dynamic pattern. 
   Since not all processes use DFPC, one optimization is to only lazily swap it out when the process to be swapped in
   indicates in its PCB that it intends to use the pattern table. 
   This is similar to how the FPU states are saved in today's OS.

2. The author may need to polish and proof-read certain sections. Some terminologies are weird, e.g., a 4-bit unit
   is not a "character" which typically refers to 8-bit ASCII code or int8_t variables. 
   "Nibble" might be a batter option.

This paper proposes Dynamic Frequent Pattern Compression (DFPC), a data compression scheme for reducing NVM wear.
The paper is motivated by the fact that current NVM devices have limited number of write-erase cycles, and that 
excessive writes not only consume bandwidth, but also harm its lifespan.
Previous works attempt to address this issue with two techniques: Compression and Flip-and-Write (FNW).
Cache blocks are first compressed to reduce the number of bits to be written into the device, and compressed data is 
either as the original binary or as a bit complement of the original to minimize the number of bits flips.
In such an architecture, each cache block sized data on the NVM is tagged with 2-bit metadata, one bit for 
indicating whether the block is compressed, and the other bit to indicate whether the data is stored as flipped.

This paper, however, points out that static Frequent Pattern Compression (SPFC), which is used by previous work,
fails to compress certain patterns that occur frequently in the run time. 
The classical SFPC algorithm divides the input stream into 32-bit words, and only compares these words with certain
pattern masks (e.g., 00XX, where 0 means the byte has a literal value of zero, and X means do not care which will be
encoded in the output code word).
The paper observes that the static patterns used by SFPC do not capture many frequent patterns that are pervasive in 
some of the workloads, despite that the latter is quite consistent and compressible.
The fact that each application has its own set of frequent but "non-standard" patterns complicates the issue, since
we cannot simply improve SFPC by adding new static patterns.

To address this limitation, DFPC allows application-specific patterns to be trained dynamically using runtime data,
and employed by the FPC algorithm for higher compression ratio.
The training is based on the observation that the distribution of zero bits in 32-bit words is quite consistent 
over the execution, which can be both easily identified and utilized.
The paper, therefore, proposes that the patterns should be trained in the unit of 4-bit "nibbles", i.e., for any
32-bit words, the training logic monitors the distribution of zeros across the eight 4-bit nibbles, and selects
the ones that benefit from compression the most.
The selected patterns are then entered into a dynamic pattern matching table, which functions just like static patterns,
and are used to encode and decode data exchanged between the hierarchy and the NVM.

We next describe the operation of DFPC as follows. The DFPC framework allows each individual process to have its own
pattern matching table of size eight. Four of the patterns are statically determined and cannot be changed, which 
are simply just pattern from the classical SFPC algorithm. The remaining four patterns are trained dynamically
from the runtime, which will be entered during execution, and will never change once they are entered to avoid
expensive re-encoding.

