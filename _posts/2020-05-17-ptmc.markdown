---
layout: paper-summary
title:  "Enabling Transparent Memory Compression on Commodity Memory"
date:   2020-05-17 01:24:00 -0500
categories: paper
paper_title: "Enabling Transparent Memory Compression on Commodity Memory"
paper_link: https://ieeexplore.ieee.org/document/8675200
paper_keyword: Memory Compression
paper_year: HPCA 2019
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper presents Practical and Transparent Memory Compression (PTMC), an OS-transparent DRAM compression scheme running 
on commodity DRAM modules, which has low metadata overhead. The paper points out that prior DRAM compression proposals 
suffer from the following overhead or difficulties. First, in order to locate a cache line sized block in the DRAM module
given a physical address, the compression scheme needs to translate the physical address into the actual address that
stores the compressed line, since a compressed line may not be stored in its home location. Depending on the degree of 
associativity (i.e. the number of possible locations a block may be stored), the translation needs to be performed using 
a translation table of various complexity. The translation table inevitably incurrs two types of overheads. The first
type is bandwidth overhead, since each DRAM access needs to first access the table to determine the location of the line.
Such bandwidth overhead may offset the bandwidth benefit brought by compression, as pointed out by the paper.
The second type of overhead is storage overhead, especially when the compressed working set is large. The paper estimates 
that even if each line only needs 1 bit storage (the bare minimum, which is the case for 2-way associative placing schemes),
the total table size can still be as large as 32MB. These two types of overhead makes the translation scheme clumsy to
deploy and use.

The second challenge is the Operating System. Some designs do not use a dedicated translation table. Instead, they delegate
the task of mapping uncompressed physical addresses to DRAM addresses to the OS, and let the OS determine the mapping,
which is then written into the TLB or the page table. 
This way, The OS needs to be notified on every compression status change, and the TLB entries are modified. This introduces
several issues such as TLB coherence, OS overhead, and software compatibility.

The last challenge is to implemente a compression scheme suitable for commodity DRAM. Some proposals seek to change 
DRAM access protocol to allow smaller or larger blocks be bursted. This requires not only a re-design of the DRAM controller,
but may also introduce compatibility issues for commodity systems. 

The goal of PTMC is to reduce memory bandwidth consumption by taking advantage of free prefetching. If two cache lines
can be compressed and colocated in the same block, when either of them is read out using the normal access protocol,
the other one can also be fetched to the LLC for free, achieving the effect of near-by block prefetching.
The paper suggests that the prefetched block should only be inserted into the LLC to avoid disrupting locality in 
upper levels. Note that the DRAM compression scheme does not attempt to conserve memory by utilizing extra slots
due to compression. Marker values are stored in these unused slots in order for reads to redirect correctly, as we will 
see below.

Cache blocks in PTMC has limited associativity to reduce the complexity of address translation. Starting from address
zero, each four blocks form a group. There are three cases. In the first case, neither block 0, 1 can be stored in one 
block after compression, nor do block 2 and 3. In this case, all four blocks must be stored uncompressed in their home
locations, and no address mapping is required. In the second case, either block 0, 1, or block 2, 3, or both, can be stored
in a single slot after compression. In this case, we store compressed block 0, 1 in the home address of block 0, and/or 
compressed block 2, 3 in the home address of block 2. Other blocks, if any, are stored in uncompressed form in their 
home locations. In the last case, all four blocks can be stored in a single slot. In this case, we store all compressed 
blocks in the home address of block 0.

All blocks in the group except block 3 has at most two possible locations: Block 0 can only be stored in its home address.
Block 1 can be stored in either slot 0 (2:1 or 4:1 compression) or slot 1. Block 2 can be stored in either slot 0 
(4:1 compression) or slot 2. Block 3, however, has three possible locations. Despite its home location, if block 2 and 3 
are compressed together

PTMC solves the above challenge using the following techniques. First, each cache line 