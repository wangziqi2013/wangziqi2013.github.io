---
layout: paper-summary
title:  "MaaT: Effective and Scalable Coordination of Distributed Transactions in the Cloud"
date:   2018-12-19 00:01:00 -0500
categories: paper
paper_title: "MaaT: Effective and Scalable Coordination of Distributed Transactions in the Cloud"
paper_link: http://www.vldb.org/pvldb/vol7/p329-mahmoud.pdf
paper_keyword: MaaT; Concurrency Control; OCC; Interval-Based CC
paper_year: VLDB 2014
rw_set: Software
htm_cd: N/A
htm_cr: N/A
version_mgmt: N/A
---

This paper proposes a distributed concurrency control protocol that uses interval-based validation, called MaaT. 
Transactions are logically ordered by timestamps, which are integers whose value denote the serialization order of
transactions. Each transaction in the system has two timestamps: A lower bound (lb) which represents the lowest time the 
transaction could commit, and correspondingly a higher bound (ub) which represents the largest time the 
transaction could commit. As the transaction reads or writes data items, the CC manager adjusts both lb and ub, such
that the ordering between the current transaction and other transactions are consistent with read and write 
operations. At any time during the execution, if the transaction's lb and ub crosses, i.e. the lb is larger than
the ub, then an ordering conflict is detected, because there is no serial schedule that allows the current 
transaction to be properly ordered with other transactions based on the existing execution history. The current 
transaction must abort and roll back all changes it has made and retry. If at the end of the transaction, 
the interval [lb, ub) is still valid, the CC manager picks an arbitrary timestamp in this range as the commit 
timestamp (ct) of the transaction, and commits all changes. Data items are tagged with two timestamps: A
read timestamp (rts), indicating the largest committed transaction that read this item, and a write timestamp (wts),
indicating the largest committed transaction that wrote this item. Both timestamps are updated when a 
transaction commits.

Under distributed settings, the database is divided into multiple partitions. Each partition is hosted by a backend data 
server. User requests are forwarded to one of the front-end servers, which implement the concurrency control logic, and 
are responsible for issuing data requests to the backend. To reduce synchronization, each backend server only maintains 
states for its own partition. Global transaction states are exchanged via the link between servers. Compared with a 
non-distributed database, network connection and latency between servers pose a different set of problems and hence 
affects the overall design goal. For example, instead of trying to minimize data transfer between processing nodes 
as in a non-distributed architecture, in distributed designs we are more focused on reducing the number of 
data communication. This paper proposes five design goals that a distributed database CC scheme should generally meet:
High throughput, efficient CPU utilization, scalability, no thrashing, and liveness.

One of the important features of MaaT is the usage of soft locks. Similar to ordinary mutual exclusion locks, soft locks
can be acquired in two modes: read (shared) mode and write (exclusive) mode. Rather than blocking transactions from proceeding
when a conflicting lock mode is attempted, soft locks never block. Instead, a list of conflicting lock owners are returned, which 
serves as the basis for conflict resolution during validation. Soft locks are acquired in a two-phase manner as in S2PL. 
Once the transaction is committed or aborted, all soft locks will be released.

Another important feature is the transaction table. Every data server has a transaction table which records the 
status of transaction on its partition. The transaction table only maintains the transaction status that is consistent
with its local access on the server, which is not coherenct across servers. This way, we avoid expensive broadcast on 
the validation stage, which is a requirement in some earlier interval-based distributed protocols.

The details of the CC scheme is described as follows. On system initialization, all data items are initialized such that their
wts and rts are both zero. No centralized counter dispensing timestamps is required. On transaction begin, its lb and ub are 
initialized to 0 and +&infin; respectively. A new entry is also created in the transaction table. This only needs to be done at 
the server responsible for running CC for the transaction. The table entry is created on other servers lazily after the transaction 
is known to that server. 



