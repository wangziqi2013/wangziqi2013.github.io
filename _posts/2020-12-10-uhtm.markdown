---
layout: paper-summary
title:  "Unbounded Hardware Transactional Memory for a Hybrid DRAM/NVM Memory System"
date:   2020-12-10 05:18:00 -0500
categories: paper
paper_title: "Unbounded Hardware Transactional Memory for a Hybrid DRAM/NVM Memory System"
paper_link: https://www.microarch.org/micro53/papers/738300a525.pdf
paper_keyword: NVM; HTM; UHTM
paper_year: MICRO 2020
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Highlight:**

1. Divide on-chip speculative states and off-chip states into two parts. The on-chip part is maintained with LLC     
   directory and the off-chip part is maintained with bloom filters and overflow lists.
   This makes it easier to maintain on-chip states with regular coherence protocols, and off-chip states with 
   more advanced methods such as bloom filters and logs.
   UHTM further allows both sets be approximate, i.e., the off-chip states need not be accurate, so the maintenance
   cost is further lowered.

2. Use different logging methods for DRAM and NVM. For DRAM, the commit latency is the major concern, so we perform
   undo logging such that commit is simply a log truncation (although there are other log operations).
   For NVM, redo logging with DRAM buffer solves both the read indirection and log replay problem.
   Commit latency is also lower since the log need not be played immediately on commit point.

3. Although not proposed by this paper, the combination of DRAM cache and redo logging solves all problems of redo
   logging: (1) Read redirection is no longer needed as long as the access hits the DRAM cache; (2) Commit does not
   require replay of the log; (3) The working set is not restricted (canonical redo logging, if implemented in the
   cache, must lock dirty lines in the hierarchy to avoid polluting the consistent image).

**Lowlight:**

In general, the design is really complicated, and there are lots of corner cases and complicated race conditions
left unexplained. Although I do appreciate some aspects of the design, one should expect such a design to be only of 
theoretical interests, but very little practical value.

1. The paper lacks many details that are necessary to judge its feasibility. For example:
   (1) Where is the per-core bloom filter placed? LLC or L1? LLC would be a better place since a request only knows 
       that it misses the hierarchy at LLC. Also, inserting into the filter is easier on LLC evictions.
   (2) When a block is evicted from the LLC, how are bloom filters updated? Do we insert into read set for those
       that are in the sharers list, and insert into write set for the one that are in the owner list?
   (3) In the above scenario, what if one of the sharers or the owner is currently switched out (so its
       bloom filter is not loaded)? Do we invoke OS and let OS find its bloom filter?

2. This paper assumes inclusive LLC, such that cache lines not in the LLC will definitely not in upper level caches.
   This is may not be true for large multicores.
   And even if this assumption is valid, the author should explicitly put it in the text.

3. What if an address is first evicted into the LLC / DRAM, and then read back again? Do we delete it from the overflow
   list, or just let it stay in the list?

4. Are transactions required to be pinned on the same core until finish? It seems to me that this is not required
   since on page 9, "Context Switch" section, it is suggested that transaction abort should broadcast the aborted
   transaction ID to all cores. But in this case, how do you make speculative lines non-speculative, when a 
   transaction commits at a different core?

5. Speculative cache lines are not tagged with transaction ID to indicate the ownership. On commit or abort, how do you
   know which line belongs to which transaction? One way is to query the directory and overflow list, but this makes
   commit very slow since you still have to perform LLC search on L1 controller.

6. If the LLC directory maintains the list of speculative sharers and owner, the first access to a non-speculative 
   cache line should always be sent to the LLC such that the access is registered. Otherwise, if the line already
   exists in the private cache of the accessing core, then LLC will not be notified.
   This design makes it very similar to the coherence protocol design of OverlayTM, which is written by me, but
   the authors did not cite it.

7. How do you guarantee the atomicity of transaction commit and abort? What if another transaction wants to access a
   cache line that is currently being rolled back during an abort? Do you just lock the bus and serialize all commits
   and aborts?

This paper proposes Unboundes HTM (UHTM), a HTM design for hybrid DRAM/NVM architecture.
The paper points out that prior HTM prosals are unable to handle modern workloads on NVM-based architecture for 
several reasons.
First, due to the fact that NVM has higher storage density than DRAM, NVM devices can store much more data in the
same area, and therefore, applications running NVM workloads tend to have significantly larger memory footprint
than DRAM transactions. This puts a heavy burden on version management, which distinguishes speculative copies of
data from the committed image. Prior researches either assume a bounded HTM model, in which transactions will be 
aborted when speculative states overflow the last-level cache (LLC), or assumes unbounded model, and employ logging
or other heavy-weight techniques to maintain speculative versions of data.
Second, conflict detection between parallel transactions can severely hamper the usability of an HTM design.
Most proposals rely on cache coherence to provide read and write information. Some of them assume a fully-mapped
directory, in which one entry is reserved for each cache line sized block in physical memory. This is not applicable,
since NVM is typically much larger than DRAM, and the storage overhead of directory entries (whcih, for minimum 
latency, must be maintained in DRAM) would be huge compared with DRAM size. On the other hand, some other designs
use bloom filters or other address signatures to track the approximate read and write set, and rely on simple
bit operations between these signatures to detect conflict. These approaches suffer from very high false positive
rates, which can also render the design unusable when the size of the working set is large.

UHTM solves the above issues with a combination of different techniques. To deal with version management for both NVM 
and DRAM, the paper proposes that logging be used when speculative lines overflow from the LLC. For addresses mapped to
the DRAM, the DRAM controller performs undo logging before applying the update in-place, such that when a power loss
occurs, the speculative content of the DRAM can be rolled back automatically. In addition, commit operations on the DRAM
are fast, since the controller simply truncates the log.
For addresses mapped to the NVM, the paper proposes that speculative states are written in the form of redo logs, which 
are persisted to the NVM when dirty lines are evicted from the LLC.
Using redo logs, however, suffers from long log search latency when the most up-to-date values are to be accessed later.
To address this issue, the paper further proposes that an L4 DRAM cache be added between the LLC and the NVM.
Speculative states evicted from the LLC are not only written to the NVM as redo logs, but also update the DRAM cache,
such that future accesses are likely to hit the cache, saving both log traversal latency and NVM access bandwidth.

To deal with conflict detection overheads and false positives, the paper proposes a two-stage conflict detection 
protocol, where both the directory and address signatures are used. When the address to be accessed is in the cache
hierarchy, an access request is checked against the LLC's directory for conflicting accesses. This ensures that 
conflicts of cache lines in the hierarchy can be detected with normal access latency, which covers the majority of 
cases. The processor also maintains two per-core address signatures, one for read set and another for write set. 
Cache lines evicted from the LLC are inserted into the signatures for all accessing transactions. 
If the address to be accessed is not in the hierarchy, the requested address is then checked with all signatures,
and conflicts are signaled if one of the signature indicates that the address is present.
Since most conflicts are already resolved in the hierarchy with the directory, and that only overflowing addresses
are inserted into the signature, UHTM can achieve a lower false positive rates on the signature than prior proposals.

UHTM extends the cache hierarchy as follows. For each cache line, one speculative bit tracks whether the line
has been read or written by a transaction. This bit serves as an indicator to the LLC controller that the line
belongs to a transaction, has not yet been logged, and that special actions must be taken when it is evicted.
Second, for each core, two bloom filters are added to represent the working set currently not in the cache hierarchy,
one for read set, and another for write set. The LLC controller inserts evicted lines into the corresponding bloom
filters of cores that have read or written the line during a transaction. 
Note that the bloom filters are not updated when a line is fetched from the DRAM or NVM, at which time these lines
are treated as non-speculative lines. This does not affect correctness, since lines are logged for off-core commit
or abort the moment they were evicted.
The bloom filters are swapped in and out as part of the thread's context.

The third hardware change is that each LLC directory entry is extended with extra fields for tracking speculative
readers and writers. Each transaction is allocated a unique transaction ID at the beginning (and stored per-transaction
as part of the context), and LLC's directory
stores transaction IDs for transactions that have speculatively accessed the line. For each cache line, at most
one speculative writers, or multiple speculative readers are allowed, but not both.
The directory is updated when a speculative GETS or GETX is received by the LLC from an upper level cache, in which
case the transaction ID of the issuing transaction is added to the list, and when a transaction commits or aborts,
in which case the transaction's ID is removed from the corresponding list.
In addition, the LLC directory also uses the directory information to update bloom filters when a line is evicted, 
since a line may be in the read set of multiple transactions. 

The last hardware change is an overflow list, which is merely a pair of pointers (base, current) into the L4 DRAM cache.
Part of the L4 cache is allocated to store a per-transaction list of addresses that are evicted from the L1, such that 
they can be located quickly without having to scan lower level caches. The overflow list is also part of the thread 
context.
When a speculative block is evicted from the L1, its address is inserted into the overflow list. On the other hand,
addresses in the overflow list are never removed, hence making it possible to have false positives, i.e., an address 
may still be in the L1, while it is also present in the list, due to the fact that an evicted address can be re-fetched.

We next describe the operation of UHTM. In the cache hierarchy, all accessed lines are marked as speculative by setting
the special bit. The L1 controller should notify the LLC controller that an access is about to occur before it marks
a line that has not yet been marked. The LLC, on receiving the notifications, adds the transaction ID of the requesting
transaction into the directory's corresponding list, in addition to performing the necessary coherence actions.

When a speculative cache line is evicted from the L1 cache, the speculative bit is also carried to lower level caches.
In addition, the L1 controller adds the address into the per-transaction overflow list, by issuing write requests to the
L4 DRAM cache and incrementing the overflow list pointer.

In-cache conflict detection is performed by the LLC controller. Since all speculative accesses must inform the LLC,
conflict detection is performed eagerly when an upper level request is received. The LLC checks whether a conflicting
access is already present in the list. The paper suggests that RAW, WAR and WAW should all incur conflicts, in which 
case the requestor transaction wins, and the current holder of the line should abort. 
Detection between on-chip lines and off-chip lines is also performed by checking the request with all bloom filters.
If any of the filters indicate a positive, the requestor should abort. The difference between conflict resolution
policies of on-chip and off-chip lines is motivated by the fact that: (1) If a transaction has already overflowed, i.e.,
its bloom filter is non-empty, then the cycles wasted by this transaction is higher; and (2) Aborting
an overflowed transaction requires more work.

When a line is evicted from the LLC, if the address is mapped to DRAM, then the DRAM controller performs undo logging
by first copying the before image to the logging area, and then applying evicted data in-place. When the line is evicted
to the NVM, it is first inserted into the L4 DRAM cache, and meanwhile, a redo log entry is generated and written into
the NVM. As a result, fetch requests from the LLC that hit the DRAM cache can be fulfilled by the cache itself 
without accessing the NVM, but those that misses the DRAM cache still needs to first read the log in case there is 
a more up-to-date version in the log, after which the home address on the NVM is accessed.
In addition, data evicted from the DRAM cache is always discarded, regardless of whether it is dirty, since the 
NVM already has a copy.

On transaction commit, the L1 controller first flash-clears all lines accessed by the committing transaction that have 
the speculative bit set. This process consists of two stages. In the first stage, the L1 controller performs a tag walk
in the local L1 cache, and for each speculative line, it queries the LLC directory, and clears the speculative bit
if the line is accessed by the committing transaction. In the second stage, it iterates over the overflow list, and
for each line in the overflow list, it clears the speculative bit in a similar way in L2 and LLC. The reason that
UHTM does not perform tag walks on L2 and LLC is that this may introduce tag array access contention, resulting in
lower performance. The DRAM controller truncates the log by writing a commit mark with the transaction ID that just
committed. The NVM controller commits the transaction by atomically writing a commit mark. The redo log will be 
reeplayed to the home address, in the background, to free log buffer space and avoid excessive log redirection.

On transaction abort, speculative lines are invalidated using the same two-stage process, except that the valid
of the line is cleared.
The DRAM controller should replay the undo log entries by copying the pre-image back to their home addresses. 
The NVM controller simply truncates the redo log by writing an abort mark atomically. The logical point of abort is
the persistence of the abort mark.