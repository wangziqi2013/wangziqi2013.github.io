---
layout: paper-summary
title:  "Unbounded Hardware Transactional Memory for a Hybrid DRAM/NVM Memory System"
date:   2020-12-10 05:18:00 -0500
categories: paper
paper_title: "Unbounded Hardware Transactional Memory for a Hybrid DRAM/NVM Memory System"
paper_link: https://www.microarch.org/micro53/papers/738300a525.pdf
paper_keyword: NVM; HTM; UHTM
paper_year: MICRO 2020
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Unboundes HTM (UHTM), a HTM design for hybrid DRAM/NVM architecture.
The paper points out that prior HTM prosals are unable to handle modern workloads on NVM-based architecture for 
several reasons.
First, due to the fact that NVM has higher storage density than DRAM, NVM devices can store much more data in the
same area, and therefore, applications running NVM workloads tend to have significantly larger memory footprint
than DRAM transactions. This puts a heavy burden on version management, which distinguishes speculative copies of
data from the committed image. Prior researches either assume a bounded HTM model, in which transactions will be 
aborted when speculative states overflow the last-level cache (LLC), or assumes unbounded model, and employ logging
or other heavy-weight techniques to maintain speculative versions of data.
Second, conflict detection between parallel transactions can severely hamper the usability of an HTM design.
Most proposals rely on cache coherence to provide read and write information. Some of them assume a fully-mapped
directory, in which one entry is reserved for each cache line sized block in physical memory. This is not applicable,
since NVM is typically much larger than DRAM, and the storage overhead of directory entries (whcih, for minimum 
latency, must be maintained in DRAM) would be huge compared with DRAM size. On the other hand, some other designs
use bloom filters or other address signatures to track the approximate read and write set, and rely on simple
bit operations between these signatures to detect conflict. These approaches suffer from very high false positive
rates, which can also render the design unusable when the size of the working set is large.
