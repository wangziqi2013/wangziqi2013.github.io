---
layout: paper-summary
title:  "Transparent Dual Memory Compression Architecture"
date:   2020-07-15 21:36:00 -0500
categories: paper
paper_title: "Transparent Dual Memory Compression Architecture"
paper_link: https://ieeexplore.ieee.org/document/8675200
paper_keyword: Compression; Memory Compression; DCM; Dual Memory Compression
paper_year: 
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Highlight:**

1. Using two compression schemes to leverage access locality; Addresses with low access probablity are compressed with 
   more heavy-weight algorithms

2. Not using overflow space is an improvement to LCP. This optimization will result in lower compression ratio, but in
   this paper it is fine, because it is only used for a small fraction of recently accessed data



This paper proposes Transparent Dual Memory Compression (DMC), a compression main memory design using two separate compression
algorithms to optimize for space efficiency and access latency. The paper makes the observation that most cache and memory 
compression algorithms are designed with low decompression latency, since decompression is often on the critical access path.
These algorithms, however, produce compressed data with lower compression ratio when compared with classical streaming
compression algorithms designed for large files, such as LZ. 
These streaming compression algorithms, on the other hand, perform badly on small blocks. 
In addition, the paper also observes that the transparency of the compression scheme affects system design. For example,
in a system where the OS explicitly manages compressed address space, the address translation between VA and the compressed
address space must be intsalled by the OS, and performed by the MMU. This design has a few disadvantages as follows.
First, whenever the compression status of a page changes, e.g. when the page migrates to a different address, which is not
entirely uncommon due to the possibility of recompression or compaction, TLB shootdown must be used to keep the entry 
synchronized in all local TLBs. Second, since physical address is no longer linear, and the translation must be performed
by MMU, bus devices that do not have an MMU, such as DMA devices, are unable to access compressed memory due to the 
inability of translating uncompressed PA to compressed PA. Third, OS managed compressed pages conflict with other virtual
memory techniques, such as huge pages, the compression overhead of which is infeasible.

DMC is an OS-transparent, dual compression algorithm scheme for achieving both ease of deployment and high compression
ratio with low latency. Instead of applying a single algorithm to each individual block, DMC classifies memory blocks into
cold and hot blocks, based on access locality of the working set. Cold blocks are compressed with the slower but better
compression algorithm, LZ77, while hot blocks are compressed with faster but "worse" algorithm, LCP, to enable fast access
to individual cache blocks. Bus transactions, including OS's memory accesses, assume an uncompressed physical address space.
Translation between uncompressed and compressed physical address is performed by the memory controller and an in-memory
mapping table. A translation cache is also added to filter out most accesses to the translation table.

DMC works as follows. The mapping table resides in a static location (not specified, but preferably at address zero)
