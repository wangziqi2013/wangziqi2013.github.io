---
layout: paper-summary
title:  "Transparent Dual Memory Compression Architecture"
date:   2020-07-15 21:36:00 -0500
categories: paper
paper_title: "Transparent Dual Memory Compression Architecture"
paper_link: https://ieeexplore.ieee.org/document/8675200
paper_keyword: Compression; Memory Compression; DCM; Dual Memory Compression
paper_year: 
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Highlight:**

1. Using two compression schemes to leverage access locality; Addresses with low access probablity are compressed with 
   more heavy-weight algorithms

2. Not using overflow space is an improvement to LCP. This optimization will result in lower compression ratio, but in
   this paper it is fine, because it is only used for a small fraction of recently accessed data

**Lowlight:**

1. This paper is poorly written, both syntactically and grammartically. Terminologies are used before being introduced 
   (e.g. "slice controller" in Fig. 5).

2. The design can apply to conventional DRAM without any problem. Why using a separate section to describe HMC? I know
   Samsung published many research papers on HMC, but this paper is really not a good place to advertise.

3. The mapping table translates with 1KB pages, but DMC compresses with 32KB in LCP. The resulting table structure allows
   multiple entries being mapped to the same base address. How do you cache these duplicated entries (might
   waste space if they are all cached; or if only caching the first one, how do you know the size of the mapping
   before looking into the compression metadata)? Or there are two caches for different sizes?

This paper proposes Transparent Dual Memory Compression (DMC), a compression main memory design using two separate compression
algorithms to optimize for space efficiency and access latency. The paper makes the observation that most cache and memory 
compression algorithms are designed with low decompression latency, since decompression is often on the critical access path.
These algorithms, however, produce compressed data with lower compression ratio when compared with classical streaming
compression algorithms designed for large files, such as LZ. 
These streaming compression algorithms, on the other hand, perform badly on small blocks. 
In addition, the paper also observes that the transparency of the compression scheme affects system design. For example,
in a system where the OS explicitly manages compressed address space, the address translation between VA and the compressed
address space must be intsalled by the OS, and performed by the MMU. This design has a few disadvantages as follows.
First, whenever the compression status of a page changes, e.g. when the page migrates to a different address, which is not
entirely uncommon due to the possibility of recompression or compaction, TLB shootdown must be used to keep the entry 
synchronized in all local TLBs. Second, since physical address is no longer linear, and the translation must be performed
by MMU, bus devices that do not have an MMU, such as DMA devices, are unable to access compressed memory due to the 
inability of translating uncompressed PA to compressed PA. Third, OS managed compressed pages conflict with other virtual
memory techniques, such as huge pages, the compression overhead of which is infeasible.

DMC is an OS-transparent, dual compression algorithm scheme for achieving both ease of deployment and high compression
ratio with low latency. Instead of applying a single algorithm to each individual block, DMC classifies memory blocks into
cold and hot blocks, based on access locality of the working set. Cold blocks are compressed with the slower but better
compression algorithm, LZ77, while hot blocks are compressed with faster but "worse" algorithm, LCP, to enable fast access
to individual cache blocks. Bus transactions, including OS's memory accesses, assume an uncompressed physical address space.
Translation between uncompressed and compressed physical address is performed by the memory controller and an in-memory
mapping table. A translation cache is also added to filter out most accesses to the translation table.

DMC works as follows. The mapping table resides in a static location (not specified, but preferably at address zero) in
physical DRAM, which is not mapped by the memory controller to physical address space. The memory controller handles address
mapping and OS initialization on the memory subsystem. The paper suggests that a compression ratio can be assumed at system 
startup time. The amount of usable physical memory is then computed using the preliminary compression ratio and reported 
to the OS. During execution, if the monitored compression ratio deviates from the initial assumption, physical memory is 
recalculated, and then reported to the OS again. Modern OSs are equipped with corresponding modules for hot swap of DRAM 
modules. For example, to deal with shirnking physical address space, the OS could install a balloon driver which requests 
for physical memory on behalf of the process, causing inactive pages of the process to be paged out via normal virtual 
memory management. The released physical pages are then eliminated from the allocatable page pool.
On the other hand, if the address space is inflating, then the OS simply adds more usable address pages to the page pool. 
The mapping table translates physical addresses in 1KB granularity, although the physical address can be compressed in
a larger, but still aligned granularity. For example, DMC compresses with LZ77 in 1KB blocks, while with LCP it compresses 
in 32KB continuous ranges, which is larger than the original LCP proposal (4KB).
The mapping table is directly addressed with a bit slice from the requested address, but the paper does not elaborate
how the table supports two different translation granularities at the same time. 

To accelerate mapping table access, a metadata cache is added to the controller which is organized as a TLB-like conventional
cache. Extra bits are added per entry to store compression type and compression metadata.
On a memory request, the cache is first checked. If a miss occurs, the entry is fetched from the mapping table
after evicting an existing one, if necessary.
If the entry being hit is an LCP entry, the cache line is directed from the particular offset, and decompressed before 
being sent to the upper level.

One extra bit per translation entry is added to track recently accessed address ranges
