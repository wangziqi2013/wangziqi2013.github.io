---
layout: paper-summary
title:  "Write-Optimized Dynamic Hashing for Persistent Memory"
date:   2019-12-06 18:41:00 -0500
categories: paper
paper_title: "Write-Optimized Dynamic Hashing for Persistent Memory"
paper_link: https://dl.acm.org/citation.cfm?id=3323302
paper_keyword: NVM; Hash Table; CCEH; Extendible Hashing
paper_year: FAST 2019
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Highlight:**

1. Good application of extendible hashing to NVM

2. I appreciate the simple persistence protocol of updating from right-to-left. This reminds me of the common technique
   that many NVM malloc uses -- storing size information in the block header, and walk the block headers in the recovery
   process.

**Lowlight**

1. This paper is extremely badly written. Grammar and explanations are bad. Examples are difficult to understand. The auhthors
   may want to improve the explanation of segment split a little as well as the recovery process. Also the paper should 
   make clear where the "local depth" is stored - directory or segment or per bucket?

This paper proposes Cache Line Conscious Extendible Hashing (CCEH), a dynamic hashing scheme for byte-addressable persistent 
memory. The paper first identifies the importance of dynamic hashing as a foundation of general hash tables, which 
allows the hash table to expand and shrink according to the workload. In contrast, static hashing does not support dynamically
resizing the hash table, which limites its usage to only web caching (in which table entries can be evicted as conflict occurs)
or similar applications. The paper then proceed to identify that one of the major difficulties of designing a dynamic 
hashing scheme is to support efficient resize operation of the hash table. Hash tables are expanded when a conflict occurs
but cannot be resolved using the conflict resolution method (e.g. chaining, open addressing). Optionally, tables are shrinked
to save storage when the number of elements falls below a certain threshold. In a naive resizing scheme, a larger hash
table is built while blocking update operations (reads are not affected since they do not change the state of the table). 
All elements in the old table are rehashed to the new table, which is expensive in terms of both the number of stores
and the computation complexity. When running on NVM, this problem can only be exacerbated due to the lower write bandwidth
compared with DRAM.

CCEH is based on extendible hashing, which has been proposed to amortize the overhead of hash table resizing over potentially
many operations that are performed after the resize. Elements are rehashed lazily rather than eagerly as in non-extendible 
hashing schemes in which all elements are rehashed and inserted into the resized table. We now introduce the basic form
of extendible hashing as follows. The hash table consists of two parts: A directory array and a set of buckets mapped by
the directory. The directory is simply an array of pointers to buckets, which is addressed using bits from the hash value. 
Conventionally, each bucket is a page allocated from the page buffer pool, but we assume that buckets can be arbitrarily 
sized based on system parameters such as cache line size and the key-value size. Buckets can be shared by different directory 
entries, which means that keys whose hash values are mapped to the corresponding directory entries can be stored in the 
same bucket in a fully-associative manner. Read operation to a bucket must check all non-empty slots in order to decide 
whether the search key exists or not. Two runtime parameters decide the behavior of operations. The first parameter 
is the global depth G, which indicates the number of bits we use from the hash value to determine the directory. For 
example, if the hash value is 0x12345679, and G equals 4, then we take four bits from the value (assuming we use LSB),
which is nine, as the index of the directory entry we need to lookup. In addition, the the size of the directory array is 
also determined by the value of G, which is 2^G. In our example there will be 16 elements in the directory. When the 
hash table is expanded, we double the number of elements in the directory, and increment the value of G to make them
consistent. The second parameter is l, the local depth of a bucket. Each bucket stores the value of l in its header,
and this value encodes an important property of key-value pairs that can be stored in the current bucket. In order
for a key-value pair to be stored in buck with depth l, the first l bits of the directory index must equal the bits in
the key's hash value on the same bit position. In other words, instead of allocating one bucket to each directory entry
and only letting it store keys whose hash value is mapped to the directory entry pointing to the bucket, we allow multiple
directory entries to point to the same bucket, and hash values of keys mapped to the bucket have a common "prefix" whose 
length is exactly l. The remaining (G - l) bits of the hash index can be arbitrary value.

It is quite obvious that directory entries cannot be mapped to buckets arbitrarily. In order for entries E1, E2, ..., Ek
to map to the same bucket whose local depth is l, the indices of these directory entries must have a common l bit prefix.
When we split a bucket when it is full, the local depth of the bucket is incremented if it is less than G, and then we rehash
elements in the bucket as follows. First, a new bucket is allocated to hold rehashed key-value pairs. Next, for all active 
slots in the old bucket, we divide them into two groups: The first group consists of keys whose (l + 1)-th bit equals 0,
and the second group consists of the rest. We then move the second group to the new bucket. The directory entries E1, E2, ...,
Ek are updated, such that those whose indices has (l + 1)-th bit equalling zero still point to the old bucket (i.e. do not 
change since they already point to the old pre-split bucket), and the rest will be updated to point to the new bucket.
This way, the invariant between local depth of buckets and the directory entries that point to them are maintained. 

When inserting into the extendible hash table, we first compute the hash value of the key, and then extract the index
of directory entry using G. We then find the bucket by following the pointer stored in the directory entry. If an empty
slot can be found in the bucket, then we place the key-value pair in the bucket, and insertion succeeds. Otherwise, we 
check whether the value of l is smaller than G. If true, we split the bucket as described above, and retry the insertion
on one of the two buckets depending on the (l + 1)-th bit of the hash value. Note that in the most extreme case, the 
bucket to be inserted can still be full after insertion (i.e. there is no element whose (l + 1)-th bit is 0 or 1), the 
split process may therefore be recursively called until at least one element is copied during the split. If, on the other
hand, the local depth l of the full bucket equals G, then the bucket can no longer be splited since there is no sharer. 
In this case, we must resize the hash table by allocating a new directory array whose size is the double of the current one,
and initialize the array by copying the current array twice into the new directory array (i.e. copy the old array into both
the first half and the second half of the new array). Copying the current array twice essentially shares all buckets with
newly added directory entries at the second half of the new directory array. This way, no rehashing is required, since the 
newly added directory entries simply share the same bucket with their buddies in the first half. The global depth G is also 
incremented to indicate that all buckets are now shared. The actual cost of rehashing is postponed to the point where a 
bucket is full.