---
layout: paper-summary
title:  "Write-Optimized Dynamic Hashing for Persistent Memory"
date:   2019-12-06 18:41:00 -0500
categories: paper
paper_title: "Write-Optimized Dynamic Hashing for Persistent Memory"
paper_link: https://dl.acm.org/citation.cfm?id=3323302
paper_keyword: NVM; Hash Table; CCEH; Extendible Hashing
paper_year: FAST 2019
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Highlight:**

1. Good application of extendible hashing to NVM

2. I appreciate the simple persistence protocol of updating from right-to-left. This reminds me of the common technique
   that many NVM malloc uses -- storing size information in the block header, and walk the block headers in the recovery
   process.

**Lowlight**

1. This paper is extremely badly written. Grammar and explanations are bad. Examples are difficult to understand. The auhthors
   may want to improve the explanation of segment split a little as well as the recovery process. Also the paper should 
   make clear where the "local depth" is stored - directory or segment or per bucket?

This paper proposes Cache Line Conscious Extendible Hashing (CCEH), a dynamic hashing scheme for byte-addressable persistent 
memory. The paper first identifies the importance of dynamic hashing as a foundation of general hash tables, which 
allows the hash table to expand and shrink according to the workload. In contrast, static hashing does not support dynamically
resizing the hash table, which limites its usage to only web caching (in which table entries can be evicted as conflict occurs)
or similar applications. The paper then proceed to identify that one of the major difficulties of designing a dynamic 
hashing scheme is to support efficient resize operation of the hash table. Hash tables are expanded when a conflict occurs
but cannot be resolved using the conflict resolution method (e.g. chaining, open addressing). Optionally, tables are shrinked
to save storage when the number of elements falls below a certain threshold. In a naive resizing scheme, a larger hash
table is built while blocking update operations (reads are not affected since they do not change the state of the table). 
All elements in the old table are rehashed to the new table, which is expensive in terms of both the number of stores
and the computation complexity. When running on NVM, this problem can only be exacerbated due to the lower write bandwidth
compared with DRAM.

CCEH is based on extendible hashing, which has been proposed to amortize the overhead of hash table resizing over potentially
many operations that are performed after the resize. Elements are rehashed lazily rather than eagerly as in non-extendible 
hashing schemes in which all elements are rehashed and inserted into the resized table. We now introduce the basic form
of extendible hashing as follows. The hash table consists of two parts: A directory array and a set of buckets mapped by
the directory. The directory is simply an array of pointers to buckets, which is addressed using bits from the hash value. 
Conventionally, each bucket is a page allocated from the page buffer pool, but we assume that buckets can be arbitrarily 
sized based on system parameters such as cache line size and the key-value size. Buckets can be shared by different directory 
entries, which means that keys whose hash values are mapped to the corresponding directory entries can be stored in the 
same bucket in a fully-associative manner. Read operation to a bucket must check all non-empty slots in order to decide 
whether the search key exists or not. Two runtime parameters decide the behavior of operations. The first parameter 
is the global depth G, which indicates the number of bits we use from the hash value to determine the directory. For 
example, 