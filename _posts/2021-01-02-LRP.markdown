---
layout: paper-summary
title:  "Lazy Release Persistency"
date:   2021-01-02 14:06:00 -0500
categories: paper
paper_title: "Lazy Release Persistency"
paper_link: https://dl.acm.org/doi/10.1145/3373376.3378481
paper_keyword: NVM; Persistency Model; LRP; Release Persistency
paper_year: ASPLOS 2020
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Lazy Release Persistency (LRP) to optimize specifically the case for log-free data structures (LFD).
The paper starts by observing that current persistency models all face similar problems. First, most previous proposals
are overly restrictive when it comes to persistence ordering barriers in the form of cache flushes and memory fences.
These proposals either fail to exploit inherent parallelism in most NVM workloads by stalling the pipeline on a 
persistence operation, which fully couples memory consistency with persistence, or they only insert full barriers 
regardless of the actual semantics level requirments, even though persistence is conducted in the background,
which is still sub-optimal since not all legal reorderings are possible.
Second, some proposals are overly relaxed, failing to enforce certain orderings, which makes it non-trivial for LFDs
to perform crash recovery. These models, although still useful to other types of tasks, require extra software 
support in order for LFDs to work properly.

The paper gives detailed overviews of three possible persistence models. The first, epoch persistency, although not 
directly mentioned in the paper, serves as the foundation of the other two, and is the only one currently implemented
on mainstream x86 hardware. In epoch persistency, the dynamic instruction flow is divided into continuous pieces,
called epochs. The persistency model enforces the rule that write operations must be persisted in the order of epochs,
while allowing arbitrary ordering of writes within the same epoch.
Application programs start a new epoch by issuing cache flushes to all dirty cache lines that are to be written into
the NVM, followed by a store fence. The store fence operation will block the store buffer until all previous cache 
line flush backs have been completed, after which time later stores can proceed to the L1 cache and become visible.


