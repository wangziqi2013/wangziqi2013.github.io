---
layout: paper-summary
title:  "Lightweight Locking for Main Memory Database Systems"
date:   2018-06-29 00:35:00 -0500
categories: paper
paper_title: "Lightweight Locking for Main Memory Database Systems"
paper_link: https://dl.acm.org/citation.cfm?id=2448947
paper_keyword: Very Lighweight Locking; VLL
paper_year: VLDB 2012
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

Classical lock based transactional systems usually suffer from performance bottleneck because of the extra contention
at the lock manager. Locks are essential for old systems using locking protocols such as Two-Phase Locking (2PL) to
produce serializable schedules. In the paper it is claimed that the lock manager is implemented as a centralized 
hash table. Lock entries are hashed into one of the buckets of the table. For each lock entry, lists of transactions 
that are currently holding the lock as well as those who are blocked by the lock are maintained. To protect the consistency
of the lock table data structure itself, each bucket and lock entry has a latch, which is used to serialize insert, delete
and read operations on the lock table. On a main-memory database deployed on multicore platform, such a centralized 
lock manager is not efficient and not scalable. One or more linked list needs to be traversed in order to find the 
lock entry and the identity of threads that are related to the lock, which costs cycles. In addition, the linked structure
is not cache friendly, and is prone to incur high cache miss ratio. In terms of scalability, the hash table itself is 
a centralized structure, managed by lightweight latches. Worker threads need to acquire and release these latches everytime
the lock table is accessed. This may cause frequent cache line invalidation, and hence degrade performance.

This paper aims at solving the lock manager's efficiency and scalability problem by using per-tuple, metadata-less 
very lightweight locks (VLL). As the first step towards optimizing locks, VLL eliminates the lock manager entirely 
from the system. As an alternative, each data item, e.g. rows, tuples, etc, is extended with an extra metadata field 
that is invisible to users, which is used to maintain locks. This removes the need of traversing the linked list
for every lock request and also reduces the probabilities of expensive cache line misses. The next step is to eliminate
the presence of linked structures that record the current holder of the lock as well as threads waiting for the lock.
VLL uses two counters, one to record the number of threads currently holding the lock in shared mode, another to record
the number of threads holding the lock in exclusive mode. Note that even though exclusive mode lock can only be held by 
at most one threads at any moment, we need more than one bit, because as explained later, these two counters actually
count the number of waiting and active threads on the lock mode. In practice, the two counters could be implemented 
using a single 64 bit integer, split evenly. Threads requesting to acquire the lock in either shared or exclusive mode 
perform Compare-and-Swap (CAS) to atomically increment the corresponding counter, and read the value before the increment.
If the value before increment indicates that the lock can be acquired (i.e. no exclusive holder for shared request, and no
lock holder for exclusive request), then the thread proceeds without blocking. Otherwise, the thread returns failure.
We introduce in the next paragraph how VLL cooperate with the concurrency control layer to provide lightweight but yet
efficient locking service.

VLL requires that threads pre-declare their lock set before execution. This could be achieved by speculatively running the
transaction without concurrency control, and gather the set of locks under the speculative execution. If a lock not in
the lock set is requested during normal execution, the transaction has to abort and retry in a more conservative lock set.
We assume in the following text that transactions already know their lock sets. Before the transaction is submitted for 
execution, it first enters a critical section. In the critical section, the transaction enqueues itself into a global
transaction queue. The order that transactions are entered into the queue determine the order their locks should be 
granted and released. Since transactions only access the queue in the critical section, the global ordering is well-defined
and no race condition should occur. Within the critical section, the transaction acquire all locks in its lock set
as described in the previous paragraph. Should a lock request fail, the transaction exits the critical section, and blocks