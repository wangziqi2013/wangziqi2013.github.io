---
layout: paper-summary
title:  "Endurance Transient Inconsistency in Byte-Addressable Persistent B+Tree"
date:   2019-11-22 13:57:00 -0500
categories: paper
paper_title: "Endurance Transient Inconsistency in Byte-Addressable Persistent B+Tree"
paper_link: https://www.usenix.org/conference/fast18/presentation/hwang
paper_keyword: NVM; B+Tree; FAST; FAIR
paper_year: FAST 2018
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper presents FAST and FAIR, a B+Tree implementation designed for byte-addressable NVM. The paper identified two
major challenges of designing persistent B+Tree. The first challenge is to handle inserts and deletes efficiently without
exposing intermediate states, especially when keys are longer than 8 bytes, which is the largest unit of atomic update
on NVM mapped memory with regard to failures. Classical B+Tree maintains a sorted array of keys for both inner nodes
and leaf nodes, in order to perform binary search. Inserting elements into the sorted array, as a consequence, involves
right shifting some elements to higher addresses to make a new slot in the middle of the array. Without proper design, such
element shifting may introduce temporary inconsistent states such as lost keys or accessing wrong child nodes. If the 
system crashes at this point, such intermediate state may not be able to be resolved. The second challenge is structural
modification operations (SMO) initiated by threads attempting to split or merge a node. B+Tree SMOs consists of several
steps, each of which will bring the affected nodes into inconsistent intermediate states. 

Prior researches have proposed adapting logging concepts used in database systems into B+Tree implementations. For example,
in order to update a single node atomically, an extra level of indirection is added to each node, which maps logical locations
of elements into physical locations in the node. The node is updated by first appending new elements into the end of the 
storage area, and then updating the mapping atomically using either 8 byte atomic update, or advanced techniques such as
Hardware Transactional Memory (HTM). Since elements are not logically committed until the last atomic update step, even
if the system crashes at some point during the update, intermediate results cannot be seen. This approach achieves easy
node update, at the cost of limiting the node layout and node size, since the mapping field length cannot exceed a cache 
line, which is the maximum unit of atomic persistence on the NVM side. The mapping layer can even be eliminated by 
maintaining an in-node log and a single log tail pointer. Threads only update the tail pointer after they have appended 
data to the end of the in-node log. This way, elements are no longer sorted in the node, and threads need to recovery the 
current node content by scanning the log every time they traverse the node. As a compromise, some designs allow a node 
to be partially sorted: Part of the elements are sorted as a result of log consolitation, while new elements are still
appended in a los-structured fashion until the next consolidation. Although this seems promising, the paper identified 
that in-node logging requires an excessive number of cache line flush and memory fences, which are inserted after
data append and after adjusting the tail pointer (or the mapping field). Multi-step SMOs are usually handled by lightweight
logging or variants of that. Threads first create a log record describing the SMO, including the type of the operation
and arguments, and flush the log record, which logically commits the SMO. The thread then locks the affected nodes (or 
use non-blocking help-along protocol), and conducts the update in-place. The log record is removed only after all dirty 
items are flushed back to the NVM. On recovery, active log records are found and replayed by the recovery process.
As noted by the paper, this approach is universally useful not only to B+Trees but also to some other common types of 
data structures. It is, however, inefficient due to excessive usage of cache line flushes and memory fences.

FAST & FAIR differs from previous proposals in that it does not attempt to avoid exposing intermeidate inconsistent 
states to concurrent readers and to the NVM. Instead, node updates and SMOs follow certain carefully designed protocol,
such that the intermediate state is still consistent, from which the node image can be recovered. Compared with 
previous approaches, FAST & FAIR does not double write traffic to the NVM, does not limit the maximum node size, and 
no recovery time processing is needed. 