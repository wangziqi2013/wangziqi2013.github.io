---
layout: paper-summary
title:  "A Reliable and Highly Available Non-Volatile Memory System"
date:   2020-06-22 05:51:00 -0500
categories: paper
paper_title: "A Reliable and Highly Available Non-Volatile Memory System"
paper_link: https://dl.acm.org/doi/10.1145/2786763.2694370
paper_keyword: NVM; Mojim; Replication
paper_year: SIGARCH 2015
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Highlight:**

1. Redo logging could achieve atomic commit and replication together without any extra overhead

2. Having a mirrored copy overcomes the biggest issue with redo logging: Write ordering between dirty data and commit record,
   as the data transfer between the master and mirror is software controlled, unlike a cache.

3. Redo logging is also a good way of performing checkpointing at arbitrary boundaries selected by software. This is 
   similar to what we try to achieve in NVOverlay

**Lowlight:**

1. Write amplification of logging reduces the life time of NVM device by half

2. In m-sync scheme (most common one), if a mirror copy crashed, why flushing the cache of the master copy? I cannot 
   see how this affects recoverability. The mirror should be able to be rebuilt whether or not the master flushes the cache.

This paper proposes Mojim, an NVM-based replication scheme featuring both high performance and reliability. The paper begins
by identifying the problem with current replication schemes based on disks. These schemes often assume a disk access latency
that is order of magnitudes larger than DRAM. The resulting design considers less about the effect of performance impact 
with the presence of software overhead, network stack overhead, and network roundtrip latency. In the NVM-era, as the 
speed of NVM is comparable or even close to that of DRAM, the system becomes more sensitive to software and network
overhead, suffering performance degradation if not tuned specifically for NVM.

Mojim solves the above challenge using a combination of high-bandwidth network with RDMA interface, simpler replicating
protocols with redo logging, and different levels of consistency between the master working copy and the mirrored copy.
First, with high bandwidth interlink between the master and the mirror, the network latency can be reduced to a minimum,
resulting in less blocks of the application. In addition, by using RDMA interface, rather than TCP or any buffered network
interface, no extra software buffering is used for data transfer between the master and the mirror, reducing software 
buffering and data movement overhead to a minimum. The RDMA device could directly fetch cache-coherence data from the 
hardware cache, without enforcing a software write back and possibly blocking on the operation.
With simpler replication protocol, the data exchange between master and mirror copy is as simple as master sending the 
log entries to the mirror, and the mirror acknowledgment of tansmission completion. No complicated two-phase 
commit is used, since only one mirror copy is present.
At last, by providing several options with different consistency guarantees, applications can make trade-off between
the degree of consistency and performance. Switching between consistency levels are achieved by deciding whether the 
application should wait for mirror persistence, or whether dirty data should be flushed, etc.

We next introduce the system configuration of Mojim. Mojin is implemented as a kernel-level library that interacts 
with application programmers via system calls or other exposed interfaces such as memory allocation. The system consists
of a master copy with NVM installed, on which applications are executed, and a mirrored copy which is read-only.
Background tasks such as auditing can be scheduled on the mirrored copy as long as they are read-only.
A few optional backup copy can also be supported. Mojim guarantees that copies on the master and mirror are strongly
consistent (if it is configured so), while only weakly consistent between these two and the backup copy.
The kernel library works with a NVM file system which manages files on the NVM, and maps the file content using mmap()
to the calling process's virtual address space. Mojim provides two simple interfaces for the file system and application
programmers to initiate data replication. The first is msync(), which synchronizes a given address range between both
the master copy and the mirrored copy. The replication is guaranteed to be atomic in two aspects. First, all dirty
data within the address range will be persisted atomically with regard to crashes after the call returns (note that it 
is not atomic on the master itself). Second, the master and mirrored copy will be synchronized atomically, such that the 
mirrored copy always contain the most up-to-date content of the master after the call returns.
The second system call is gmsync(), which has the same semantics as msync(), but supports sending multiple address
ranges to be atomically replicated.

Mojim employs redo logging to perform both atomic commit and data replication. Recall that in redo logging, two types of
write ordering must be observed to ensure correctness. First, log entries should be persisted before the commit record.
This suggests that all log entries must be transferred to the mirrored copy before the commit mark is written, only
after which the system call could return. The second write ordering is between dirty data and the commit record. Dirty
data can only be written back after the commit record is persisted. The second writr ordering conveys different messages
to the master and the mirror. For master copies, there is no guarantee that the hardware cache will not write back dirty
data before the commit record, and therefore, the master copy by itself is not atomic at all. On the other hand, however,
if we think of the master copy as a large buffer, and the mirrored copy as actual data, both write orderings are observed,
since dirty data will never be directly transferred from the master to the mirror, which makes the second write ordering
trivially true.

Mojim divides falut-free program execution into sync points, at which moments msync() or gmsync() are called to request
for persistence. On crash recovery, the system image is guaranteed to be recovered to the most recent sync point given that
the function at that sync point has returned. How to correctly identify and place sync points are left to the application 
programmer.


