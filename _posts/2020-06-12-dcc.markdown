---
layout: paper-summary
title:  "Decoupled Compressed Cache: Exploiting Spatial Locality for Energy-Optimized Compressed Caching"
date:   2020-06-12 23:00:00 -0500
categories: paper
paper_title: "Decoupled Compressed Cache: Exploiting Spatial Locality for Energy-Optimized Compressed Caching"
paper_link: https://dl.acm.org/doi/10.1145/2540708.2540715
paper_keyword: Compression; Cache Tags; DCC
paper_year: MICRO 2013
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Decoupled Compressed Cache (DCC), a tag mapping framework for compressed LLC featuring low area cost
and high space utilization. The paper points out that previous segmented tag mapping schemes have three major issues. The 
first is internal fragmentation, which happens when compressed blocks could only be aligned to a certain segment boundary, 
as in segmented cache designs. Blocks could not take advantage of every byte of the physical segment allocated to it, 
wasting the rest storage of the segment. 
The second issue is external fragmentation, leading to suboptimal utilization of the segments if compressed blocks are
required to be stored in consecutive range of blocks. Even if the total number of bytes in free segments are sufficient,
these segments may not form a consecutive range that is large enough to hold the block, requiring compaction of existing 
blocks. On modern cache architecture, segments are usually implemented as data array banks. Compaction a set would require 
reading and writing all the banks, causing extra power consumption and head dissipation.
The last issue is tagging overhead. In conventional segmented cache schemes, the maximum number of compressed blocks
that can be stored within a set is bounded by the number of tags per-set. Over-provisioning of tags are required if 
more compressed blocks are expected to be stored in the set. More tags per-set, however, have a nagative impact on
power and access latency, since the larger the tag array, the more power and cycles it takes to access the array on
every access.

This paper takes advantage of two observations. First, LLC still preserves the spatial locality of access. Given a super
block whose size is four regular 64 byte blocks and is aligned to four-block boundary, in most cases, more than 1 regular
blocks within the super block are cached by the LLC. The second observation is that external fragmentation can be easily
solved by making the segment array fully-associative. As a result, a compressed block does not have to be stored in a 
consecutive range of segments. Instead, the block can reside in several non-continuous segments, which will be concatenated 
together by the block read circult on cache accesses.

We next describe DCC in details. Similar to segmented caches, the DCC allows arbitrary mapping between tags and segments
within a cache set. The physical storage of the set is divided into 16 byte segments. Tags are not over-privisioned.
For a n-way set-associative cache, there are still n address tags. These tags, however, now store addresses of super blocks,
rather than regular blocks. As discussed above, a super block consists of four regular blocks, which is also aligned to
the four-block boundary. The lower two bits of the block address is used as the block offset instead of forming the 
set index. Set index bits are extracted after the block offset.

Recall that DCC allows fully associative mapping between tags and segments. To this end, the tag no longer have a pointer
to the beginning segment in which a compressed block is stored. Instead, each segment is described by a back pointer field
consisting of two fields. The first field, tag index, indicates the super block tag the segment is allocated to. The second
field, block offset, indicates the offset of the regular block of which compressed data is stored in the segment.
Block data is still required to be stored in-order, i.e. if a compressed block is stored in more than one segments, the
cache controller must follow a certain order to combine these segments into a full block. The paper suggests that these
back pointers be stored in a separate bank, which could be access in parallel with super block tags during cache accesses.
Valid bits are maintained per-segment to aid segment allocation. Coherence states and compression states are maintained 
per-block within the super block tag. The paper assumes a super block size of four, and therefore, each super block tag
contains four copies of coherence states and compression type fields.

On cache accesses, the cache controller first computes the set index by extaring bits from the requested address, and
then read out all super block tags within the set. The requested address is then compared with tag addresses. In the meantime,
the block offset is also extracted from the requested address, and the back pointers of the same set are also accessed
in parallel with address tags. The cache controller also compares the block offset field with block offset from the 
address tag. A cache hit is signaled, if (1) one of the n super block tags match the requested address; (2) The extracted
block offset matches at least one block offset field in back pointers; (3) The tag ID of the matching back pointer entry 
in condition (2) is the same as the matching tag in condition (1). These three conditions can be checked using simple
combination logic after a comparator array, which does not add significant latency to the cache access logic.
On a cache hit,

