---
layout: paper-summary
title:  "CSALT: Context Switch Aware Large TLB"
date:   2019-08-18 02:43:00 -0500
categories: paper
paper_title: "CSALT: Context Switch Aware Large TLB"
paper_link: https://ieeexplore.ieee.org/document/8686492
paper_keyword: TLB; LRU; Cache Replacement
paper_year: MICRO 2018
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper proposes CSALT, a cache partitioning scheme for unified L3 TLB and data on lower level caches. The paper 
identifies that, on modern platform where virtualization is supported, running multiple virtual machines can overburden
the TLB with excessive misses, resulting in a massive number of page walks. A page walk on today's 2-D page table 
can take, in the worst case, 24 memory accesses (20 for accessing host page table for every guest physical address, 
and 4 for the standard 4-level guest translation), the latency of which can easily become a performance bottleneck
on a saturated system. 

Previous solutions focus on reducing the number of page walks required for a translation and reducing the latency of 
each page walk access. To reduce page walks which require a number of memory access on different levels of the page
table (typically four), previous researches propose using an in-memory L3 TLB which stores translation entries as 
a hardware TLB does in order to avoid a page walk. The L3 TLB can be as large as 16MB, a capacity that exceeds all 
hardware implementations. Some other researches also propose caching the intermediate page table entries in hardware
caches, such that the latency of accessing the entry can be reduced to as fast as a regular cache access. Using part
of the data cache to store TLB entries, however, can negatively affect performance, since the cache might be polluted
by extra TLB entries as a result of TLB misses. As we will see later, CSALT addresses the cache pollution problem 
by partitioning the cache into two parts, dedicated for data and TLB entries respectively.

In this paper, we assume that TLB entries are cached by the L2 and LLC, sharing the same storage with data accesses.
The paper also assumes that there is an underlying L3 TLB in the main memory, but also claims that this is not necessary,
and that the cache partitioning scheme works on all designs that cache page table entries in the data cache. Only translation 
entries (i.e. from VA to PA) are stored in the cache. Other intermediate entries might be cached by a dedicated cache
in the page walker.

To solve the cache pollution problem when data and TLB entries share the same cache, CSALT partitions the cache into two
parts, one for data and another for TLB entries. The partitioning parameter, N, dictates that in a way-associative cache, 
way 0 to way N - 1 of the cache is dedicated to data, while way N to the maximum way (W) is dedicated to TLB entries. 
When a memory access from upper levels misses the cache, the newly inserted entry must only be allocated from the 
data part, if the request is a regular load or store, or from the TLB entry part, if the request is to a memory region
allocated for L3 TLB (i.e. it is a hardware generated request to fetch L3 TLB entry). When a block is to be evicted,
the cache controller will prioritize eviction of blocks in the wrong partition, i.e. a data block in the TLB partition, 
or a TLB block in the data partition. Either case, the distribution of blocks will converge to the state where all
blocks in the data partition are data blocks, and so does the TLB partition. We will see from later sections that
a block can reside in the wrong partition when the parameter N is adjusted.

CSALT does not rely on a statically determined N to work correctly. Instead, the parameter N is determined dynamically 
based on profiling results. The profiling process compares the performance of the current partitioning with a potentially
better partitioning parameter N' within a time window, called an epoch. At the end of the epoch, if the latter is indeed 
better, then the parameter N will be updated to be N', and the hardare will converge to the new parameter. The profiling
is based on the concept of LRU stack. The LRU stack is an ordered list, in which a more recently accessed block is 
ordered before a less recently accessed block. The most recently used block is at the head of the list, while the least
recently used block is at the tail (when LRU evicts a block from the cache, the block at the tail is selected). Each way 
in a single set has a position in the LRU stack, and therefore, the stack size is identical to the number of ways.