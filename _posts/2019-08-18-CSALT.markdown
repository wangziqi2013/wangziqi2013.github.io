---
layout: paper-summary
title:  "CSALT: Context Switch Aware Large TLB"
date:   2019-08-18 02:43:00 -0500
categories: paper
paper_title: "CSALT: Context Switch Aware Large TLB"
paper_link: https://ieeexplore.ieee.org/document/8686492
paper_keyword: TLB; LRU; Cache Replacement
paper_year: MICRO 2018
rw_set: 
htm_cd: 
htm_cr: 
version_mgmt: 
---

This paper proposes CSALT, a cache partitioning scheme for unified L3 TLB and data on lower level caches. The paper 
identifies that, on modern platform where virtualization is supported, running multiple virtual machines can overburden
the TLB with excessive misses, resulting in a massive number of page walks. A page walk on today's 2-D page table 
can take, in the worst case, 24 memory accesses (20 for accessing host page table for every guest physical address, 
and 4 for the standard 4-level guest translation), the latency of which can easily become a performance bottleneck
on a saturated system. 

Previous solutions focus on reducing the number of page walks required for a translation and reducing the latency of 
each page walk access. To reduce page walks which require a number of memory access on different levels of the page
table (typically four), previous researches propose using an in-memory L3 TLB which stores translation entries as 
a hardware TLB does in order to avoid a page walk. The L3 TLB can be as large as 16MB, a capacity that exceeds all 
hardware implementations. Some other researches also propose caching the intermediate page table entries in hardware
caches, such that the latency of accessing the entry can be reduced to as fast as a regular cache access. Using part
of the data cache to store TLB entries, however, can negatively affect performance, since the cache might be polluted
by extra TLB entries as a result of TLB misses. As we will see later, CSALT addresses the cache pollution problem 
by partitioning the cache into two parts, dedicated for data and TLB entries respectively.

In this paper, we assume that TLB entries are cached by the L2 and LLC, sharing the same storage with data accesses.
The paper also assumes that there is an underlying L3 TLB in the main memory, but also claims that this is not necessary,
and that the cache partitioning scheme works on all designs that cache page table entries in the data cache.

