---
layout: paper-summary
title:  "SAND: Towards High-Performance Serverless Computing"
date:   2021-09-24 01:53:00 -0500
categories: paper
paper_title: "SAND: Towards High-Performance Serverless Computing"
paper_link: https://www.usenix.org/conference/atc18/presentation/akkus
paper_keyword: Container; Serverless; SAND
paper_year: USENIX ATC 2018
rw_set:
htm_cd:
htm_cr:
version_mgmt:
--- 

This paper proposes SAND, a serverless framework with reduces cold start latency, and is specifically optimized for 
function chaining. The paper is motived by the fact that existing serverless
platforms suffer from cold start latency, and existing solutions of using warm-up instances will significantly increase
the resource consumption. Besides, function calls from the internals of an application, namely, chained function calls,
are handled no different from external requests, which incurs unnecessary performance overhead.
This paper addresses these two issues in SAND by using application-level sandboxes and local message bus, respectively.

The paper observes that existing service providers use either containers or virtual machines to isolate function 
instances. In the simplest case, each function instance is mapped to one instance of virtualized process, which executes
to the end, and then the instance will be destroyed. 
This naive approach often incurs huge cold start latency, mainly because of the initialization cost of the 
virtualization platforms, and the cost of setting up the language environment such as installing and importing
libraries. It is suggested in the paper that the overhead can be as large as a few seconds, or even tens of seconds.

To optimize the cold start latency, many service providers, on their worker nodes, maintain a pool of already 
initialized instances. These instances are scheduled to serve incoming requests, and when the execution completes,
they are returned to the pool instead of being destroyed. This way, the instance will be kept in a warm state,
with all the system components and libraries already loaded, which eliminates the overhead of initializing them.
The paper points out, however, that these idle instances will still consume system resource such as memory, and 
therefore, the improved cold start latency still come at a cost.

The other issue faced by today's serverless framework is to support efficient function chaining. 
In the model assumed by this paper, an application consists of a few types of functions, each of them being called a 
"grain". Grains may serve a request solely by their own, or more generally, several grains may collaborate together to 
finish a complex task, which is called "function chaining". 
In function chaining, when a grain has finished execution, it will invoke one or more successor grains, and pass the 
output of the current execution to them. 
The invocation relation between grains for a particular request may be static or dynamically generated. In either case, 
it is assumed that the current grain always has full information about the execution path, such that execution could 
resume and complete even if a host crashes, and the function invocation is scheduled elsewhere for re-execution.
This property, although not pointed out explicitly in the paper, is critical for crash recovery, which is what the
paper focuses on.

Existing solutions to function chaining is just to treat them as regular requests, and forward chained functions to
the frontend API gate as if these requests came from the external world. This approach, obviously, is sub-optimal,
since it takes a full round-trip time from the worker node to the frontend node, plus the processing time of the 
API gateway just to get a function invoked. In theory, the function could be invoked locally, if one is deployed on
the same machine, saving all the above latencies except the minimum overhead of a few local IPC messages. 

SAND addresses the first issue, that is, the seemingly inevitable trade-off between cold start latency and resource 
consumption of idle instances, by lowering the degree of isolation within an application.