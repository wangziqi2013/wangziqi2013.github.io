---
layout: paper-summary
title:  "Lukewarm serverless functions: characterization and optimization"
date:   2022-07-19 01:40:00 -0500
categories: paper
paper_title: "Lukewarm serverless functions: characterization and optimization"
paper_link: https://dl.acm.org/doi/10.1145/3470496.3527390
paper_keyword: Serverless; Prefetching; Jukebox; Function Keep-Alive; Instruction Cache
paper_year: ISCA 2022
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Jukebox, an instruction cache prefetching mechanism designed to reduce instruction
cache misses for short-lived serverless functions. 
The paper is motivated by the three distinctive properties of serverless functions, which are not observed on
other types of cloud services.
First, serverless functions generally have short execution time, in the unit of milliseconds, and small
memory footprint, which is usually within a few hundreds of megabytes, due to the fact that most functions are
deployed for web services, and they only perform simple tasks such as HTML rendering.
Second, different serverless functions often co-reside on the same physical machine and share the hardware
resource. This is natural outcome in today's cloud computing paradigm as service providers adopt virtualization
technologies which make it possible to subscribe the same physical machine to many unrelated service instances.
Lastly, intervals between function invocations can be much longer than the function's lifetime.
This is especially true when function's lifetime is short.

The three observations, combined together, brings about an issue known that as cold start problem. When a function
is short and frequently invoked, the overhead of initializing and tearing down the function container 
cannot be well amortized over the function's execution period unlike the case of traditional long-running services.
To address the issue, function service providers have already adopted a technique known as function keep-alive,
which maintains "warm" instances of the function containers in a process pool. 
Function invocations are serviced by an already initialized process in the pool without paying the initialization cost,
and when a function completes execution, the container process is returned to the pool without tearing it down, thus
avoiding paying the extra cost that container initialization and teardown.
A container process is destroyed when the function is not invoked for a while in order to free up the resource 
it consumes.

This paper observes that, however, even with function keep-alive, the instances executing on a shared 
platform with high degrees of interleaving still suffer from performance degradation, compared with executions
conducted back-to-back without any interleaving. 
The paper calls this phenomenon "lukewarm execution", and attributes the performance issue to cold processor
states as a result of over-subscribing the processor with many different instances between two invocations. 

The paper further conducted experiments comparing the execution's CPI between interleaved and back-to-back,
and makes the following conclusions.
First, the instruction footprint of functions are typically several hundreds of KBs, showing low variance
between different functions. 
Second, most of the instruction footprint are common on repeated invocations of functions, expect a few outliers.
Third, performance counters suggest that the main bottleneck during execution is frontend instruction fetch latency.
Further investigation into cache misses reveals that the cache hit rate for instruction fetches in all levels of the 
hierarchy are uniformly low, when executions are interleaved.
Meanwhile, when executions are back-to-back, L1 and L2 still demonstrate high miss rate due to the working set
size exceeding maximum cache size, but the LLC has lower misses.
These two combined together indicate that last-level cache thrashing on instruction data is a major source of 
bottleneck that causes slowdowns known as lukewarm execution.
Besides, since processors are over-subscribed to many functions, the total instruction footprint of which 
is much larger than the LLC, attempting to keep all instruction data in the LLC will not work, necessitating 
a different mechanism.

The paper therefore proposes Jukebox as a record-and-replay instruction prefetching mechanism 
to alleviate the negative effect of LLC thrashing on instruction data.
Jukebox works as a two-stage process. In the record stage, the instruction access miss history is logged
by a special piece of hardware into the main memory, thus saving the access pattern for costly instructions.
Then in the replay stage, the instruction access log is loaded by the prefetching hardware, and instruction blocks
are fetched into the L2 cache. The paper suggests that it is better to prefetch into the L2 cache, instead of 
prefetching into the L1, as the typical instruction footprint is too large to be stored entirely in the L1.
We next describe the two stages in more details.

