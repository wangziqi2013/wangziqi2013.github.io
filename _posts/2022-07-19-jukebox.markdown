---
layout: paper-summary
title:  "Lukewarm serverless functions: characterization and optimization"
date:   2022-07-19 01:40:00 -0500
categories: paper
paper_title: "Lukewarm serverless functions: characterization and optimization"
paper_link: https://dl.acm.org/doi/10.1145/3470496.3527390
paper_keyword: Serverless; Prefetching; Jukebox; Function Keep-Alive; Instruction Cache
paper_year: ISCA 2022
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Jukebox, an instruction cache prefetching mechanism designed to reduce instruction
cache misses for short-lived serverless functions. 
The paper is motivated by the three distinctive properties of serverless functions, which are not observed on
other types of cloud services.
First, serverless functions generally have short execution time, in the unit of milliseconds, and small
memory footprint, which is usually within a few hundreds of megabytes, due to the fact that most functions are
deployed for web services, and they only perform simple tasks such as HTML rendering.
Second, different serverless functions often co-reside on the same physical machine and share the hardware
resource. This is natural outcome in today's cloud computing paradigm as service providers adopt virtualization
technologies which make it possible to subscribe the same physical machine to many unrelated service instances.
Lastly, intervals between function invocations can be much longer than the function's lifetime.
This is especially true when function's lifetime is short.

The three observations, combined together, brings about an issue known that as cold start problem. When a function
is short and frequently invoked, the overhead of initializing and tearing down the function container 
cannot be well amortized over the function's execution period unlike the case of traditional long-running services.
To address the issue, function service providers have already adopted a technique known as function keep-alive,
which maintains "warm" instances of the function containers in a process pool. 
Function invocations are serviced by an already initialized process in the pool without paying the initialization cost,
and when a function completes execution, the container process is returned to the pool without tearing it down, thus
avoiding paying the extra cost that container initialization and teardown.
A container process is destroyed when the function is not invoked for a while in order to free up the resource 
it consumes.

This paper observes that, however, even with function keep-alive, the instances executing on a shared 
platform with high degrees of interleaving still suffer from performance degradation, compared with executions
conducted back-to-back without any interleaving. 
The paper calls this phenomenon "lukewarm execution", and attributes the performance issue to cold processor
states as a result of over-subscribing the processor with many different instances between two invocations. 

The paper further conducted experiments comparing the execution's CPI between interleaved and back-to-back,
and makes the following conclusions.
First, the instruction footprint of functions are typically several hundreds of KBs, showing low variance
between different functions. 
Second, most of the instruction footprint are common on repeated invocations of functions, expect a few outliers.
Third, performance counters suggest that the main bottleneck during execution is frontend instruction fetch latency.
Further investigation into cache misses reveals that the cache hit rate for instruction fetches in all levels of the 
hierarchy are uniformly low, when executions are interleaved.
