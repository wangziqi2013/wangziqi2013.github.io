---
layout: paper-summary
title:  "SpZip: Architectural Support for Effective Data Compression in Irregular Applications"
date:   2021-06-16 16:48:00 -0500
categories: paper
paper_title: "SpZip: Architectural Support for Effective Data Compression in Irregular Applications"
paper_link: https://conferences.computer.org/iscapub/pdfs/ISCA2021-4ghucdBnCWYB7ES2Pe4YdT/333300b069/333300b069.pdf
paper_keyword: SpZip; Compression; Data Flow Execution
paper_year: ISCA 2021
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Comments:**

1. I can be wrong, but it is not obvious how the hardware accelerated data flow architecture can support other commonly
   used data structures in addition to sparse matrix encoded in Compressed Sparse Row (CSR) format. 
   While I agree that CSR is general enough to encode sparse graphs, and is hence very useful when
   graph algorithms are being implemented, when it comes to something like a linked list or a hash table, could 
   SpZip handle them as well? My best guess is no, because there is no way to express conditions in the data flow
   architecture.
   Imagine the following scenario: I have an array of keys, and I want to fetch the values stored in the hash table
   for each key.
   On the CPU side, I can compute the hash values of several keys using SIMD instructions. 
   It would be great, if these hash values can be entered into the queues of the data flow machine, and let it
   issue memory requests to traverse the linear probing hash table and grab the value.

2. Figure 9 does not show L1 cache, which can be misleading because it may seem that SpZip entirely gets rid of L1
   (which is not a good design choice as it cripples other applications not using it).
   It turns out that L1 is still being used in the simulation, just not drawn in Figure 9.

This paper proposes SpZip, a data-flow memory access architecture with compression for better cache and memory 
efficiency on irregular data structures and algorithms.
SpZip is motivated by two important observations.
First, many real world applications use sparse data structures in which most of the elements are of value zero,
and the data structure itself is stored in a compact format. As a result, these applications demonstrate irregular
access patterns over the address space. Pure software solutions that use manually crafted, ad-hoc routines to traverse 
these structure usually suffer from high cache miss rates due to software's inability to communicate the access pattern
to hardware. The core pipeline is hence frequently stalled by data dependencies, which prevents it from issuing
memory accesses on the critical path, causing under-utilization of memory bandwidth.
Second, conventional compressed cache architecture is agnostic of the layouts of underlying data structures, and most
of them just perform compression in a granularity that is consistent with cache hierarchy's block interface. 
This does not work well for irregular accesses, since these accesses are typically scattered across the address space
due to indirections, making it difficult to group accesses that are nearby in the address space for compression 
or decompression.

The design of SpZip seems to primarily focus on a general encoding for sparse matrix, the Compressed Sparse Row (CSR) 
format, which can be used to represent the adjacency matrix of a graph.
CSR encodes a sparse matrix by only storing non-zero entries for each row using a (column id, value) tuple.
The matrix is still stored in row-major format, meaning that non-zero values on the same row are stored adjacent
to each other, and the column ids are sorted (at least in all the examples given in the paper, while in practice,
as the paper also explicitly points out, sorting columns on the same row based on some other keys other than the 
column id will not change the graph it encodes, but it may bring some locality benefit and result in performance 
improvement). 
Random access to individual rows of a CSR matrix is supported by adding another "offset" array which serves as 
the index into the body of the matrix. The offset array contains the starting offset of each row in the body, and
there is exactly one element for each row. 
Random accesses on columns are not supported. In order to read the value of a column given a row, the entire row
must be traversed, and if an entry with the column id exists, the value of the entry is read. Otherwise, the column
contains value zero and is simply not stored.

The paper then presents two of the most common access patterns that graph algorithms are likely to perform. The
first pattern, most notably seen in PageRank (push model), enumerates all nodes in the graph as the source node, 
and for each source node, enumerates its adjacent nodes as destination. The destination score is then updated 
by computing a "contrib" function on the source node.
Both the destination score and contrib of the source are stored in separate arrays for fast access.

The data flow model of the above pattern is described as follows. First, a ranged scan operation reads out values
in the offset array. This can be done in parallel since there is no data dependency. Then, for each value read from 
the offset array, an indirection operation is performed that uses the value as an index into the body of the CSR
matrix. The value in the CSR matrix is also read out using another ranged scan, the starting address of which is 
the address computed by the indirection operation, and the size of the scan is given by taking the diff between
the current offset value and the next value (or the size of the offset array, if the value is the last one in
offset array).
The output of the second scan is then sent to the CPU core for value update.
In the meantime, values in score array is also read out by scan operations in a similar manner.
Values in contrib arrays, however, cannot be read by scans, since it is indexed by destination node ids.
The paper shows that it can be accessed using a single indirection operation, the input of which is the column
ids output from the second range scan operation that reads the matrix body.
