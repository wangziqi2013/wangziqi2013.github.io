---
layout: paper-summary
title:  "Enhancing Address Translations in Throughput Processors via Compression"
date:   2023-01-08 20:30:00 -0500
categories: paper
paper_title: "Enhancing Address Translations in Throughput Processors via Compression"
paper_link: https://dl.acm.org/doi/10.1145/3410463.3414633
paper_keyword: GPGPU; TLB Compression; Compression
paper_year: PACT 2020
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes a technique for compressing GPGPU TLB entries to accommodate the increasingly irregular
access pattern of modern GPU workloads. The paper is motivated by the observation that modern GPGPU workloads 
suffer a high L2 TLB miss ratio which can become a performance bottleneck under Unified Virtual Memory (UVM).
The paper proposes to compress the TLB translation entry by extracting the common high bits of both virtual
and physical addresses and storing compressed translation entries in the hardware structure for reduced misses.

The paper assumes Unified Virtual Memory (UVM) architecture where the GPU device shares the same virtual address
space as the process running on the CPU. Compared with prior memory architectures where the GPU and CPU do not 
share any address space, UVM has two obvious advantages. First, programmers no longer need to manually transfer
data from and into the GPU memory, which greatly simplifies the development process. Secondly, UVM enables 
the GPU device to access a working set that is larger than its onboard memory size, as data can be dynamically
paged in and out of GPU memory.

To support UVM, GPU devices must be able to translate virtual addresses issued by programs into the physical address
of the main memory, before the address can be used for memory accesses. Current GPU implementations either add 
dedicated TLB structures to the GPU device or rely on the existing IOMMU to perform address translation.

