---
layout: paper-summary
title:  "Enhancing Address Translations in Throughput Processors via Compression"
date:   2023-01-08 20:30:00 -0500
categories: paper
paper_title: "Enhancing Address Translations in Throughput Processors via Compression"
paper_link: https://dl.acm.org/doi/10.1145/3410463.3414633
paper_keyword: GPGPU; TLB Compression; Compression
paper_year: PACT 2020
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes a technique for compressing GPGPU TLB entries to accommodate the increasingly irregular
access pattern of modern GPU workloads. The paper is motivated by the observation that modern GPGPU workloads 
suffer a high L2 TLB miss ratio which can become a performance bottleneck under Unified Virtual Memory (UVM).
The paper proposes to compress the TLB translation entry by extracting the common high bits of both virtual
and physical addresses and storing compressed translation entries in the hardware structure for reduced misses.

The paper assumes Unified Virtual Memory (UVM) architecture where the GPU device shares the same virtual address
space as the process running on the CPU. Compared with prior memory architectures where the GPU and CPU do not 
share any address space, UVM has two obvious advantages. First, programmers no longer need to manually transfer
data from and into the GPU memory, which greatly simplifies the development process. Secondly, UVM enables 
the GPU device to access a working set that is larger than its onboard memory size, as data can be dynamically
paged in and out of GPU memory.

To support UVM, GPU devices must be able to translate virtual addresses issued by programs into the physical address
of the main memory, before the address can be used for memory accesses. Current GPU implementations either add 
dedicated TLB structures to the GPU device or rely on the existing IOMMU to perform address translation.
In both cases, the address translation is not free, since TLB misses will trigger page walks to fetch the 
translation entry from the in-memory page table, the latency of which is usually non-negligible and is 
on the critical path.

To study the properties of address translation on GPU devices, the paper conducted experiments on a GPU simulator
with 22 workloads. The simulator models a two-level TLB hierarchy, with the first level being private and at a 
per-SM basis and the second level being a shared TLB structure. 
The paper presents several observations. First, different workloads demonstrate different TLB miss ratios, but in 
general, the miss ratios are significant, indicating that TLB misses have become a major source of bottleneck
in these executions. In particular, if we quantitatively classify the degree of locality using reuse distance, 
i.e., the number of other accesses between two consecutive accesses to the same page, we can observe that 
most pages demonstrate larger reuse distances, which implies bad locality.
Second, by replacing the realistic L1 cache with an ideal L1 that does not incur any access 
overhead (but can still miss), the overall performance improvement is very limited. 
On the contrary, by replacing the existing L2 TLB with an ideal one, the paper observes significant performance 
improvement, which is a good indication that optimizing the L2 TLB can bring more benefits than optimizing the L1 TLB.
Lastly, the paper also observes that accesses exhibit a clustered pattern, meaning that most accesses are clustered 
around one or more regions during a time window, and in different time windows, the regions where accesses are 
clustered may vary. However, the paper also noted that most accesses do not demonstrate any stride pattern, i.e.,
there is no consistent stride value such that these accesses can be described.
The observed access pattern is a strong hint that most addresses in the TLB will share the same higher bits as they
are close to each other in the address space.

Based on the above observations, the paper proposes a TLB compression scheme where the L2 TLB entries are compressed

