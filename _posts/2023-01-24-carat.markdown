---
layout: paper-summary
title:  "CARAT CAKE: Replacing Paging via Compiler/Kernel Cooperation"
date:   2023-01-24 07:17:00 -0500
categories: paper
paper_title: "CARAT CAKE: Replacing Paging via Compiler/Kernel Cooperation"
paper_link: https://dl.acm.org/doi/10.1145/3503222.3507771
paper_keyword: Carat; Compiler; TLB; OS
paper_year: ASPLOS 2020
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

**Comments:**

1. My biggest concern about the design is its usage scenario. If Carat is to be deployed for servers running cloud 
workloads, how does it solve the virtualization problem (which is addressed with 2D page table walks)? 
This design seems a perfect fit for micro VMs that run within a lightweight supervisor. It enables the micro VM 
to start multiple processes as service handlers.

This paper proposes Carat Cake, a compiler-OS co-design that eliminates the need for the hardware memory management 
unit (MMU) and TLB while providing the same level of address mapping and protection. Carat Cake is motivated by the 
high performance and power overhead of today's address translation infrastructure and aims at eliminating the overhead
completely thus allowing applications to be directly executed on the physical address space. Carat Cake achieves its 
design goal via software instrumentations that are inserted by the compiler to check access rights in the runtime.
It also requires a user-space software runtime as well as Operating System coordination such that address mapping 
and memory compaction can be conducted transparently to the application.

Conventionally, address translation occurs for every memory access instruction issued by the CPU. Address 
translation is performed by the hardware MMU consisting of a TLB for caching recently used translation entries
and a page table walker that fetches an entry from the main memory when an access misses the TLB.
While providing the flexibility of a per-process virtual address space, the benefit comes at a cost.
The paper identifies four issues with the MMU.
First, the MMU hardware consumes real estate on the chip and increases energy consumption as it is a significant
piece of hardware that is accessed for every memory instruction. Besides, MMU hardware is difficult to get right
and requires non-trivial design and verification efforts, which elongates the development process. 
Second, the L1 TLB limits the size of the L1 cache because the L1 cache is virtually indexed. As a result, the 
number of bits that must remain unchanged before and after the translation should be at least the number of 
bits used to index the L1 cache. For example, on the current architecture, a 4KB page guarantees that the lower 12 bits 
will not change, thus enabling a maximum number of (12 - 6) = 6 bits for generating the index, limiting the L1
size to 64 sets.
Third, address translation also incurs extra latency on the memory access critical path, especially when the translation
misses the TLB and a page walk has to be started. The extra latency has become an issue with today's big data workload
whose working set size far exceeds the coverage of any realistic TLB hardware.
Lastly, security attacks that exploit hardware-controlled access permissions, such as Spectre, can be difficult to 
mitigate with software patches, as the MMU hardware is generally not programmable.

In order to address all these issues with hardware address translation, this paper proposes to eliminate
the hardware MMU, and instead, use software approaches to perform address translation and to enforce access rights.
The paper identifies three design goals that must be achieved in order to provide the same level of abstraction as 
the existing virtual memory mechanism. First, the design should be able to enforce access rights at user-defined 
memory regions. This feature corresponds to the per-page access permission bits of the conventional virtual memory 
system. Second, the design should only allow pre-defined entry points to request kernel functions or high-privileged 
functions while restricting access to arbitrary code in the kernel and another process's address space. This 
feature corresponds to the system call feature on the existing architecture that exposes supervisor entry points. 
Lastly, the design should also support transparent page migration which enables physical pages to move around or 
be swapped out without disrupting user-space execution. In conventional systems, this feature is achieved by 
updating the virtual-to-physical mapping and then performing a TLB shootdown.

In order to move the responsibility of MMU to the software, Carat leverages compile-time instrumentation to 
add stubs before certain critical operations. At a high level, Carat uses a customized compiler toolchain with 
special passes after generating the IR to insert the stubs called "guards". When an application is compiled, 
the compiler scans the IR, identifies instructions that may require extra operations, and inserts the guards. 
The same process is repeated for every dependency. The executable binary is generated by statically linking all
pieces together and signing it with the provenance of the compiler used.
The Carat runtime is also linked into the application. The runtime is responsible for communicating with the OS and,
as we will see later, performing various tasks on behalf of the OS that would have been done by the hardware MMU on
today's hardware.



