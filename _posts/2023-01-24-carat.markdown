---
layout: paper-summary
title:  "CARAT CAKE: Replacing Paging via Compiler/Kernel Cooperation"
date:   2023-01-24 07:17:00 -0500
categories: paper
paper_title: "CARAT CAKE: Replacing Paging via Compiler/Kernel Cooperation"
paper_link: https://dl.acm.org/doi/10.1145/3503222.3507771
paper_keyword: Carat; Compiler; TLB; OS
paper_year: ASPLOS 2020
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This paper proposes Carat Cake, a compiler-OS co-design that eliminates the need for the hardware memory management 
unit (MMU) and TLB while providing the same level of address mapping and protection. Carat Cake is motivated by the 
high performance and power overhead of today's address translation infrastructure and aims at eliminating the overhead
completely thus allowing applications to be directly executed on the physical address space. Carat Cake achieves its 
design goal via software instrumentations that are inserted by the compiler to check access rights in the runtime.
It also requires a user-space software runtime as well as Operating System coordination such that address mapping 
and memory compaction can be conducted transparently to the application.

Conventionally, address translation occurs for every memory access instruction issued by the CPU. Address 
translation is performed by the hardware MMU consisting of a TLB for caching recently used translation entries
and a page table walker that fetches an entry from the main memory when an access misses the TLB.
While providing the flexibility of a per-process virtual address space, the benefit comes at a cost.
The paper identifies four issues with the MMU.
First, the MMU hardware consumes real estate on the chip and increases energy consumption as it is a significant
piece of hardware that is accessed for every memory instruction. Besides, MMU hardware is difficult to get right
and requires non-trivial design and verification efforts, which elongates the development process. 
Second, the L1 TLB limits the size of the L1 cache because the L1 cache is virtually indexed. As a result, the 
number of bits that must remain unchanged before and after the translation should be at least the number of 
bits used to index the L1 cache. For example, on the current architecture, a 4KB page guarantees that the lower 12 bits 
will not change, thus enabling a maximum number of (12 - 6) = 6 bits for generating the index, limiting the L1
size to 64 sets.
Third, address translation also incurs extra latency on the memory access critical path, especially when the translation
misses the TLB and a page walk has to be started. The extra latency has become an issue with today's big data workload
whose working set size far exceeds the coverage of any realistic TLB hardware.
Lastly, security attacks that exploit hardware-controlled access permissions, such as Spectre, can be difficult to 
mitigate with software patches, as the MMU hardware is generally not programmable.