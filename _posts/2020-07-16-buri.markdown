---
layout: paper-summary
title:  "Buri: Scaling Big-Memory Computing with Hardware-Based Memory Expansion"
date:   2020-07-16 18:59:00 -0500
categories: paper
paper_title: "Buri: Scaling Big-Memory Computing with Hardware-Based Memory Expansion"
paper_link: https://dl.acm.org/doi/10.1145/2808233
paper_keyword: Compression; Memory Compression; Buri
paper_year: TACO 2015
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This journal article proposes Buri, a hardware main memory compression scheme designed for big-data workloads. 
The paper observes that due to the high memory demand of big memory workloads, simple scaling up the number of 
cores and memory modules is no longer possible or finacially feasible to extend the amount of memory in a system.
Hardware memory compression, on the other hand, reduces the amount of physical storage required for these workloads 
without installing more components. 

The paper then identifies a few difficulties of designing a hardware memory compression scheme. First, compression adds 
one extra level of indirection on the address tralsnation path, namely from uncompressed address space to compressed 
memory space. To accommodate the extra indirection level, there are two design options. The first is to perform both 
translations via the MMU, and let the OS set up an explicit mapping from the uncompressed to compressed address space. 
This option introduces considerable changes throughout the hardware and software stack, which is incompatible with
existing hardware abstractions and may cause some to fail (e.g. DMA, huge pages). 
The second option is to explicitly maintain the three-level translation scheme, and isolates address translation from
uncompressed to compressed address space to memory components such as the memory controller or the DRAM module.
This way, existing components can work correctly without any modification, which makes commercial adoption much easier.

The second difficulty is more complicated memory management with compressed blocks. In uncompressed address space,
physical memory is managed by the OS in the unit of fixed sized pages, while in compressed address space, pages are 
no longer uniformly sized after compression. In order to conserve memory, the memory allocation strategy should dynamically
adjust the number of physical blocks allocated to a logical page according to the runtime compression ratio.
The memory allocator should therefore be able to allocate free memory either incrementally, or in variably sized blocks.

The third difficulty is modification of existing hardware components, such as the TLB and cache. As has been stated in 
the above discussion, if the design combines VA and uncompressed PA, both the cache tagging and TLB need to be changed.
The cache tags no longer use uncompressed PA, since the address generation unit outputs PA in the compressed space.
Meanwhile, the TLB should directly translate from VA to compressed PA, being burdurned with the extra responsibility of
maintaining translation information for compression.
The above discussion, in fact, further confims that separating uncompressed and compressed PA explicitly help isolating
hardware changes and reducing both hardware and software migration cost.

Buri addresses the above challenges with the following high-level design ideas. First, Buri adopts the three-level address
mapping scheme, explicitly acknowledging an intermediate, uncompressed PA between VA and compressed PA, called the 
"shadow address space". Upper level components, such as OS, cache, and TLB, always use shadow address space for space
management and address translation, as in a conventional system without compression. Translation is performed by
memory controllers using a dedicated mapping table in the unmapped part of physical memory, the content of which is also
entirely managed by the hardware.
Second, Buri allocates memory in fixed sized chunks incrementally. Only one size class is maintained to avoid the complexity
of splitting and merging size classes to fulfill allocation requests. The physical storage of a 4KB compressed page is 
then represented by four pointers to non-continuous blocks. Note that the page size of storage allocation is orthogonal 
to the translation page size, which is defined by the page table structure. The memory controller always compress data
in the shadow address space in the granularity of 4KB pages, regardless of the OS's paging policy.
