---
layout: paper-summary
title:  "Buri: Scaling Big-Memory Computing with Hardware-Based Memory Expansion"
date:   2020-07-16 18:59:00 -0500
categories: paper
paper_title: "Buri: Scaling Big-Memory Computing with Hardware-Based Memory Expansion"
paper_link: https://dl.acm.org/doi/10.1145/2808233
paper_keyword: Compression; Memory Compression; Buri
paper_year: TACO 2015
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This journal article proposes Buri, a hardware main memory compression scheme designed for big-data workloads. 
The paper observes that due to the high memory demand of big memory workloads, simple scaling up the number of 
cores and memory modules is no longer possible or finacially feasible to extend the amount of memory in a system.
Hardware memory compression, on the other hand, reduces the amount of physical storage required for these workloads 
without installing more components. 

The paper then identifies a few difficulties of designing a hardware memory compression scheme. First, compression adds 
one extra level of indirection on the address tralsnation path, namely from uncompressed address space to compressed 
memory space. To accommodate the extra indirection level, there are two design options. The first is to perform both 
translations via the MMU, and let the OS set up an explicit mapping from the uncompressed to compressed address space. 
This option introduces considerable changes throughout the hardware and software stack, which is incompatible with
existing hardware abstractions and may cause some to fail (e.g. DMA, huge pages). 
The second option is to explicitly maintain the three-level translation scheme, and isolates address translation from
uncompressed to compressed address space to memory components such as the memory controller or the DRAM module.
This way, existing components can work correctly without any modification, which makes commercial adoption much easier.

The second difficulty is more complicated memory management with compressed blocks. In uncompressed address space,
physical memory is managed by the OS in the unit of fixed sized pages, while in compressed address space, pages are 
no longer uniformly sized after compression. In order to conserve memory, the memory allocation strategy should dynamically
adjust the number of physical blocks allocated to a logical page according to the runtime compression ratio.
The memory allocator should therefore be able to allocate free memory either incrementally, or in variably sized blocks.


