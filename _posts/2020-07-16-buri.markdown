---
layout: paper-summary
title:  "Buri: Scaling Big-Memory Computing with Hardware-Based Memory Expansion"
date:   2020-07-16 18:59:00 -0500
categories: paper
paper_title: "Buri: Scaling Big-Memory Computing with Hardware-Based Memory Expansion"
paper_link: https://dl.acm.org/doi/10.1145/2808233
paper_keyword: Compression; Memory Compression; Buri
paper_year: TACO 2015
rw_set:
htm_cd:
htm_cr:
version_mgmt:
---

This journal article proposes Buri, a hardware main memory compression scheme designed for big-data workloads. 
The paper observes that due to the high memory demand of big memory workloads, simple scaling up the number of 
cores and memory modules is no longer possible or finacially feasible to extend the amount of memory in a system.
Hardware memory compression, on the other hand, reduces the amount of physical storage required for these workloads 
without installing more components. 

The paper then identifies a few difficulties of designing a hardware memory compression scheme. First, compression adds 
one extra level of indirection on the address tralsnation path, namely from uncompressed address space to compressed 
memory space. To accommodate the extra indirection level, there are two design options. The first is to perform both 
translations via the MMU, and let the OS set up an explicit mapping from the uncompressed to compressed address space. 
This option introduces considerable changes throughout the hardware and software stack, which is incompatible with
existing hardware abstractions and may cause some to fail (e.g. DMA, huge pages). 
The second option is to explicitly maintain the three-level translation scheme, and isolates address translation from
uncompressed to compressed address space to memory components such as the memory controller or the DRAM module.
This way, existing components can work correctly without any modification, which makes commercial adoption much easier.

The second difficulty is more complicated memory management with compressed blocks. In uncompressed address space,
physical memory is managed by the OS in the unit of fixed sized pages, while in compressed address space, pages are 
no longer uniformly sized after compression. In order to conserve memory, the memory allocation strategy should dynamically
adjust the number of physical blocks allocated to a logical page according to the runtime compression ratio.
The memory allocator should therefore be able to allocate free memory either incrementally, or in variably sized blocks.

The third difficulty is modification of existing hardware components, such as the TLB and cache. As has been stated in 
the above discussion, if the design combines VA and uncompressed PA, both the cache tagging and TLB need to be changed.
The cache tags no longer use uncompressed PA, since the address generation unit outputs PA in the compressed space.
Meanwhile, the TLB should directly translate from VA to compressed PA, being burdurned with the extra responsibility of
maintaining translation information for compression.
The above discussion, in fact, further confims that separating uncompressed and compressed PA explicitly help isolating
hardware changes and reducing both hardware and software migration cost.

Buri addresses the above challenges with the following high-level design ideas. First, Buri adopts the three-level address
mapping scheme, explicitly acknowledging an intermediate, uncompressed PA between VA and compressed PA, called the 
"shadow address space". Upper level components, such as OS, cache, and TLB, always use shadow address space for space
management and address translation, as in a conventional system without compression. Translation is performed by
memory controllers using a dedicated mapping table in the unmapped part of physical memory, the content of which is also
entirely managed by the hardware.
Second, Buri allocates memory in fixed sized chunks incrementally. Only one size class is maintained to avoid the complexity
of splitting and merging size classes to fulfill allocation requests. The physical storage of a 4KB compressed page is 
then represented by four pointers to non-continuous blocks. Note that the page size of storage allocation is orthogonal 
to the translation page size, which is defined by the page table structure. The memory controller always compress data
in the shadow address space in the granularity of 4KB pages, regardless of the OS's paging policy.
Lastly, Buri isolates hardware changes to the memory controller and extra components added to the controller. All upper
level components use the abstraction of the shadow address space without any special customization, except that the OS
should support memory module hot-plug to handle dynamic compression ratio changes.

We next describe Buri's data and metadata layout. From a high level, every aligned 4KB page is compressed with a variant 
of LCP, and stored in the unit of 1KB blocks. In LCP, the compression ratio of a 4KB page is pre-determined before any
cache line is written. Compressed lines are then mapped into the page linearlly, the offset of which is calculated using
the compression ratio. If a compressed line could not fit into its regular slot, the line will be appended at the end of 
the current page, with an extra level of indirection indicating the offset of the overflowed line.
In the original LCP design, each page also contains a metadata header for storing pointers to these overflowed lines.
A cache line lookup must first check this header to see if the line resides in the conventional slot or the overflow 
area, necessitating an extra DRAM read. In this paper, the indirection pointers are stored in the metadata entry, which
can fit into a 64 byte DRAM read unit, which further optimizes performance.
This paper suggests that a static compression of 4:1 be used globally. Cache lines that are not compressibile to one 
fourth of its original size should always be stored in the overflow area. 
This design decision not only achieves a balance between design complexity and effectiveness of compression, but also 
eliminates recompression, during which cache lines will be copied around.
In future designs, this compression ratio may also be dynamically adjusted based on runtime profiling results.

As is the case for many previously proposed compression schemes, Buri
uses a direct mapped translation table at the beginning of the physical DRAM to perform shadow to physical translation
in the granularity of 1KB blocks. Each 1KB block has an entry in the mapping table, which contains the allocation
status, 
